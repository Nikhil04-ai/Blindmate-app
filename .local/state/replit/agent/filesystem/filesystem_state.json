{"file_contents":{"README.md":{"content":"# BlindMate - AI Assistant for Visually Impaired Users\n\nBlindMate is a comprehensive web application designed specifically for visually impaired users. It provides real-time object detection, voice-activated navigation, and multilingual AI assistance to help users navigate their environment safely and independently.\n\n## üåü Features\n\n### üîç Real-time Object Detection\n- **TensorFlow.js COCO-SSD Model**: Detects objects like people, chairs, vehicles, stairs, and more\n- **Live Webcam Feed**: Continuous monitoring of the user's environment\n- **Visual Feedback**: Bounding boxes and labels overlaid on detected objects\n- **Audio Alerts**: Text-to-speech announcements with distance estimation\n- **Smart Throttling**: Prevents audio overload with intelligent announcement timing\n\n### üó£Ô∏è Voice-Activated AI Assistant\n- **Gemini Pro Integration**: Advanced natural language processing for command interpretation\n- **Continuous Listening**: Web Speech API for hands-free operation\n- **Smart Commands**: \n  - \"Start detection\" / \"Stop detection\"\n  - \"Take me to [location]\"\n  - \"Enable location\"\n  - \"Change language to [language]\"\n- **Structured Responses**: JSON-based command processing for reliable action execution\n\n### üåç Multilingual Support\n- **7 Indian Languages**: English, Hindi, Tamil, Telugu, Bengali, Marathi, Gujarati\n- **Voice Recognition**: Speech-to-text in user's preferred language\n- **Localized Responses**: AI responses and system messages in selected language\n- **Easy Language Switching**: Voice commands or manual selection\n\n### üß≠ Smart Navigation\n- **Google Maps Integration**: Turn-by-turn directions using Google Maps API\n- **Voice-Guided Directions**: Spoken navigation instructions\n- **Walking-Optimized Routes**: Pedestrian-friendly path calculation\n- **Location-Based Services**: Automatic current location detection\n\n### ‚ôø Accessibility-First Design\n- **Large Fonts & High Contrast**: Optimized for users with partial vision\n- **Keyboard Navigation**: Full keyboard accessibility with focus indicators\n- **Screen Reader Compatible**: Semantic HTML and ARIA labels\n- **Mobile-First Responsive**: Works seamlessly on smartphones and tablets\n- **Voice-First Interface**: Minimal visual interaction required\n\n## üõ†Ô∏è Technology Stack\n\n### Frontend\n- **HTML5**: Semantic markup for accessibility\n- **CSS3**: Responsive design with Bootstrap framework\n- **Vanilla JavaScript**: ES6+ with modern web APIs\n- **TensorFlow.js**: Machine learning in the browser\n- **Web Speech API**: Voice recognition and synthesis\n- **Google Maps API**: Navigation and directions\n\n### Backend\n- **Flask**: Lightweight Python web framework\n- **Google Generative AI**: Gemini Pro for command processing\n- **Flask-CORS**: Cross-origin resource sharing\n- **Python 3.8+**: Modern Python features\n\n## üöÄ Quick Start\n\n### Prerequisites\n- Python 3.8 or higher\n- Modern web browser with camera and microphone access\n- Internet connection for AI services\n\n### Environment Setup\n\n1. **Get the required API key**:\n   - Visit [Google AI Studio](https://aistudio.google.com)\n   - Sign in with your Google account\n   - Create a new API key (free)\n   - Keep this key safe - you'll need it in step 4\n\n2. **Set up the project in Replit**:\n   - The project is already configured and ready to run\n   - All dependencies are pre-installed\n\n3. **Add your API key**:\n   - Go to the Secrets tab in your Replit project\n   - Add a new secret with key: `GEMINI_API_KEY`\n   - Paste your API key as the value\n\n4. **Run the application**:\n   ```bash\n   python main.py\n   ```\n   \n5. **Access the application**:\n   - Open your browser and go to `http://localhost:5000`\n   - Allow camera and microphone permissions when prompted\n   - The app will automatically initialize and start the voice interaction\n\n## üéØ Usage Guide\n\n### First Time Setup\n1. When you first open BlindMate, it will ask: \"Should I start detection, Sir?\"\n2. Say \"Yes\" to enable object detection\n3. Grant camera permission when prompted\n4. The app will then ask: \"Would you like to enable location?\"\n5. Say \"Yes\" to enable navigation features\n6. Grant location permission when prompted\n\n### Voice Commands\nBlindMate understands natural language commands in multiple languages:\n\n**Object Detection:**\n- \"Start detection\" / \"Begin scanning\"\n- \"Stop detection\" / \"Pause scanning\"\n\n**Navigation:**\n- \"Take me to the library\"\n- \"Navigate to Central Park\"\n- \"Go to the nearest coffee shop\"\n\n**Language Control:**\n- \"Change language to Hindi\"\n- \"Switch to Tamil\"\n\n**System Control:**\n- \"Enable location\"\n- \"Stop\" (stops current activity)\n\n### Keyboard Shortcuts\nFor accessibility, these keyboard shortcuts are available:\n- `Ctrl + S`: Start/Stop object detection\n- `Ctrl + V`: Activate voice command\n- `Ctrl + L`: Enable location services\n\n### Supported Languages\n- **English (India)**: en-IN\n- **Hindi**: hi-IN (‡§π‡§ø‡§Ç‡§¶‡•Ä)\n- **Tamil**: ta-IN (‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç)\n- **Telugu**: te-IN (‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å)\n- **Bengali**: bn-IN (‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ)\n- **Marathi**: mr-IN (‡§Æ‡§∞‡§æ‡§†‡•Ä)\n- **Gujarati**: gu-IN (‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä)\n\n## üîß Features in Detail\n\n### Object Detection\n- Uses COCO-SSD model to detect 80+ object classes\n- Real-time processing at 30 FPS\n- Distance estimation based on object size\n- Audio announcements every 3 seconds to prevent spam\n- Visual overlay with bounding boxes and confidence scores\n\n### AI Voice Assistant\n- Powered by Google's Gemini Pro model\n- Processes natural language in 7 Indian languages\n- Fallback logic when AI service is unavailable\n- Structured JSON responses for reliable action execution\n\n### Navigation System\n- Opens directions in user's default maps application\n- Supports Google Maps, Apple Maps, and generic map services\n- Provides walking directions optimized for pedestrians\n- Voice guidance integration with object detection\n\n### Accessibility Features\n- WCAG 2.1 AA compliant design\n- Screen reader compatible with ARIA labels\n- High contrast colors and large fonts\n- Keyboard navigation support\n- Voice-first interaction model\n\n## üö® Browser Compatibility\n\n**Fully Supported:**\n- Chrome 80+ (recommended)\n- Edge 80+\n- Firefox 75+\n- Safari 13+\n\n**Required Permissions:**\n- Camera access for object detection\n- Microphone access for voice commands\n- Location access for navigation (optional)\n\n## üõü Troubleshooting\n\n### Common Issues\n\n**\"Camera not working\":**\n- Ensure camera permissions are granted\n- Check if another application is using the camera\n- Try refreshing the page\n\n**\"Voice commands not responding\":**\n- Check microphone permissions\n- Ensure you're speaking clearly\n- Try switching to English if using another language\n\n**\"AI responses seem incorrect\":**\n- Verify GEMINI_API_KEY is set correctly\n- Check internet connection\n- The app has fallback logic for basic commands\n\n**\"Navigation not working\":**\n- Enable location permissions\n- Check if popup blocker is preventing navigation window\n- Try manually opening Google Maps with the destination\n\n### Performance Tips\n- Use Chrome for best performance\n- Ensure good lighting for object detection\n- Speak clearly and at normal pace for voice commands\n- Close other camera-using applications\n\n## ü§ù Contributing\n\nBlindMate is designed to be accessible and helpful. If you have suggestions for improvements:\n\n1. Focus on accessibility enhancements\n2. Consider multilingual user needs\n3. Test with actual assistive technology users\n4. Ensure changes don't break voice-first interaction\n\n## üìÑ License\n\nThis project is designed for educational and assistive technology purposes. Please use responsibly and consider the privacy and safety of visually impaired users.\n\n## üÜò Support\n\nFor technical support or feature requests, please consider:\n- Testing in different browsers\n- Checking browser console for error messages\n- Verifying all permissions are granted\n- Ensuring stable internet connection for AI features\n","size_bytes":7898},"UPGRADE_NOTES.md":{"content":"# BlindMate Upgrade Notes\n\n## Recent Improvements (July 30, 2025)\n\n### üîß Fixed Issues\n\n#### ‚úÖ Visual Bounding Boxes for Object Detection\n- **Problem**: Objects were detected but no visual feedback was shown\n- **Solution**: Enhanced canvas overlay with color-coded bounding boxes\n  - Red boxes for people (highest priority)\n  - Orange boxes for vehicles \n  - Teal boxes for furniture\n  - Green boxes for other objects\n- **Features**: Shows object name, confidence percentage, and distance estimate\n\n#### ‚úÖ Voice-Guided Permission Flow\n- **Problem**: App didn't ask for permissions through voice\n- **Solution**: Complete voice-guided setup process\n  - Greets user on load: \"Should I start detection, Sir?\"\n  - Requests camera permission after \"yes\" response\n  - Asks for location: \"Would you like to enable location for navigation?\"\n  - Multilingual support for yes/no responses\n\n#### ‚úÖ Continuous Wake Word Detection\n- **Problem**: Had to manually trigger voice commands\n- **Solution**: Always-listening wake word system\n  - Wake phrases: \"Hey BlindMate\", \"Hey Blind Mate\", \"BlindMate\"\n  - Continuous background listening\n  - Automatic restart on errors\n  - Example: \"Hey BlindMate, start detection\"\n\n#### ‚úÖ Fixed Gemini API Encoding Issues\n- **Problem**: ASCII encoding errors with non-English characters\n- **Solution**: Proper UTF-8 encoding for all Gemini API calls\n- **Result**: Bengali, Hindi, Tamil and other languages now work properly\n\n#### ‚úÖ Speech Synthesis Overlap Prevention\n- **Problem**: Multiple overlapping speech outputs causing errors\n- **Solution**: Smart speech queue management\n  - Priority system (high priority for user responses)\n  - 2-second cooldown between announcements\n  - 5-second interval for object detection announcements\n  - Speech queue for non-priority messages\n\n#### ‚úÖ Enhanced Object Detection Announcements\n- **Problem**: Too much noise from detecting every object\n- **Solution**: Priority-based intelligent announcements\n  - Only announces most important objects (people, vehicles, obstacles)\n  - Contextual positioning: \"person on your left, 1 meter away\"\n  - Maximum 2 objects per announcement\n  - Prioritizes closer and more relevant objects\n\n#### ‚úÖ Improved Navigation Integration\n- **Problem**: Google Maps integration wasn't working properly\n- **Solution**: Simplified navigation approach\n  - Opens navigation in user's default maps app\n  - Supports Google Maps, Apple Maps, and generic maps\n  - Provides backup voice guidance\n  - Works without API key requirement\n\n### üöÄ New Features Added\n\n#### üéØ Smart Object Priority System\n- People and vehicles get highest priority\n- Furniture and obstacles get medium priority\n- Other objects get low priority (only announced if very close)\n\n#### üåç Enhanced Multilingual Support\n- Fixed voice recognition in all 7 supported languages\n- Proper fallback command processing when Gemini is unavailable\n- Language-specific yes/no response detection\n\n#### üé® Visual Improvements\n- Color-coded bounding boxes for different object types\n- Real-time detection indicator with pulsing animation\n- Better canvas overlay positioning\n- Improved typography and contrast\n\n#### üîä Better Voice Experience\n- Natural language announcements: \"person on your right, very close\"\n- Reduced speech interruptions\n- Queue-based speech management\n- Improved voice synthesis error handling\n\n## Technical Improvements\n\n### Code Quality\n- Separated speech queue management\n- Better error handling for speech recognition\n- Modular voice permission flow\n- Enhanced object detection algorithms\n\n### Performance\n- Reduced CPU usage with smart announcement throttling\n- Optimized canvas rendering\n- Better memory management for speech synthesis\n- Improved continuous listening efficiency\n\n### Accessibility\n- Enhanced keyboard navigation\n- Better screen reader compatibility\n- Improved focus indicators\n- High contrast mode support\n\n## Usage Examples\n\n### Voice Commands Now Working:\n1. **\"Hey BlindMate, start detection\"** - Begins object detection\n2. **\"Hey BlindMate, take me to the library\"** - Opens navigation\n3. **\"Hey BlindMate, stop detection\"** - Stops object detection\n4. **\"Hey BlindMate, change language to Hindi\"** - Switches interface language\n\n### What You'll Experience:\n1. **On page load**: Voice greeting asks for detection permission\n2. **During detection**: Visual bounding boxes + voice announcements\n3. **Navigation**: Opens in your preferred maps app with voice guidance\n4. **Continuous**: Always listening for \"Hey BlindMate\" commands\n\n## Browser Compatibility\n\n### Fully Tested:\n- Chrome 90+ (recommended)\n- Edge 90+\n- Firefox 85+\n\n### Required Permissions:\n- Microphone (for voice commands)\n- Camera (for object detection)\n- Location (for navigation, optional)\n\n## Known Limitations\n\n1. **Google Maps API Key**: Navigation opens external maps app (no API key needed)\n2. **Internet Required**: For Gemini AI voice processing\n3. **Modern Browser**: Requires Web Speech API support\n4. **Lighting**: Object detection works best in good lighting\n\n## Next Potential Improvements\n\n1. **Offline Mode**: Basic voice commands without Gemini\n2. **Custom Training**: Let users train on specific objects\n3. **Smart Home Integration**: Control IoT devices via voice\n4. **Advanced Navigation**: Turn-by-turn voice directions\n5. **Emergency Features**: Quick emergency contact voice commands","size_bytes":5382},"app.js":{"content":"/**\n * BlindMate - AI Assistant for Visually Impaired Users\n * Main Application JavaScript\n */\n\nclass BlindMate {\n    constructor() {\n        // Core properties\n        this.video = document.getElementById('webcam');\n        this.canvas = document.getElementById('canvas');\n        this.ctx = this.canvas.getContext('2d');\n        this.model = null;\n        this.isDetecting = false;\n        this.stream = null;\n        this.currentLanguage = 'en-IN';\n        this.currentTone = 'friendly';\n        this.userLocation = null;\n        \n        // Voice synthesis and recognition\n        this.synth = window.speechSynthesis;\n        this.recognition = null;\n        this.isListening = false;\n        \n        // UI elements\n        this.elements = {\n            startBtn: document.getElementById('startDetectionBtn'),\n            stopBtn: document.getElementById('stopDetectionBtn'),\n            voiceBtn: document.getElementById('voiceCommandBtn'),\n            locationBtn: document.getElementById('locationBtn'),\n            languageSelect: document.getElementById('languageSelect'),\n            toneSelect: document.getElementById('toneSelect'),\n            systemStatus: document.getElementById('systemStatus'),\n            detectionStatus: document.getElementById('detectionStatus'),\n            voiceStatus: document.getElementById('voiceStatus'),\n            loadingOverlay: document.getElementById('loadingOverlay'),\n            detectionIndicator: document.getElementById('detectionIndicator')\n        };\n        \n        // Language configurations\n        this.languages = {\n            'en-IN': { name: 'English', voice: 'en-IN', greeting: 'Hello! Should I start detection, Sir?' },\n            'hi-IN': { name: 'Hindi', voice: 'hi-IN', greeting: '‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§ï‡•ç‡§Ø‡§æ ‡§Æ‡•à‡§Ç ‡§°‡§ø‡§ü‡•á‡§ï‡•ç‡§∂‡§® ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡•Ç‡§Ç, ‡§∏‡§∞?' },\n            'ta-IN': { name: 'Tamil', voice: 'ta-IN', greeting: '‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç! ‡Æ®‡Ææ‡Æ©‡Øç ‡Æï‡Æ£‡Øç‡Æü‡Æ±‡Æø‡Æ§‡Æ≤‡Øà‡Æ§‡Øç ‡Æ§‡Øä‡Æü‡Æô‡Øç‡Æï ‡Æµ‡Øá‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Ææ, ‡Æê‡ÆØ‡Ææ?' },\n            'te-IN': { name: 'Telugu', voice: 'te-IN', greeting: '‡∞®‡∞Æ‡∞∏‡±ç‡∞ï‡∞æ‡∞∞‡∞Ç! ‡∞®‡±á‡∞®‡±Å ‡∞ó‡±Å‡∞∞‡±ç‡∞§‡∞ø‡∞Ç‡∞™‡±Å‡∞®‡±Å ‡∞™‡±ç‡∞∞‡∞æ‡∞∞‡∞Ç‡∞≠‡∞ø‡∞Ç‡∞ö‡∞æ‡∞≤‡∞æ, ‡∞∏‡∞æ‡∞∞‡±ç?' },\n            'bn-IN': { name: 'Bengali', voice: 'bn-IN', greeting: '‡¶®‡¶Æ‡¶∏‡ßç‡¶ï‡¶æ‡¶∞! ‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶ø ‡¶∏‡¶®‡¶æ‡¶ï‡ßç‡¶§‡¶ï‡¶∞‡¶£ ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡¶¨, ‡¶∏‡ßç‡¶Ø‡¶æ‡¶∞?' },\n            'mr-IN': { name: 'Marathi', voice: 'mr-IN', greeting: '‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞! ‡§Æ‡•Ä ‡§ì‡§≥‡§ñ ‡§∏‡•Å‡§∞‡•Ç ‡§ï‡§∞‡§æ‡§µ‡•Ä ‡§ï‡§æ, ‡§∏‡§∞?' },\n            'gu-IN': { name: 'Gujarati', voice: 'gu-IN', greeting: '‡™®‡™Æ‡™∏‡´ç‡™§‡´á! ‡™∂‡´Å‡™Ç ‡™Æ‡™æ‡™∞‡´á ‡™°‡™ø‡™ü‡´á‡™ï‡´ç‡™∂‡™® ‡™∂‡™∞‡´Ç ‡™ï‡™∞‡™µ‡´Å‡™Ç ‡™ú‡´ã‡™à‡™è, ‡™∏‡™∞?' }\n        };\n        \n        // Detection settings\n        this.detectionThreshold = 0.5;\n        this.lastDetections = [];\n        this.lastAnnouncement = 0;\n        this.announcementInterval = 5000; // 5 seconds between announcements\n        \n        // Smart object announcement tracking system\n        this.objectAnnouncementCount = new Map(); // Track how many times each object was announced\n        this.objectLastSeen = new Map(); // Track when each object was last seen\n        this.objectDisappearanceTime = new Map(); // Track when object disappeared\n        this.maxAnnouncements = 3; // Maximum announcements per object\n        this.cooldownPeriod = 7000; // 7 seconds cooldown after object disappears\n        this.lastSpeechTime = 0;\n        this.speechCooldown = 2000; // 2 seconds cooldown between speech\n        this.isSpeaking = false;\n        this.speechQueue = [];\n        \n        // Enhanced speech delay configuration for object announcements\n        this.speechDelayTimer = null; // Timer for delaying speech\n        this.minObjectAnnouncementDelay = 1500; // 1.5 second minimum delay between object announcements\n        this.pendingAnnouncement = null; // Store pending announcement\n        this.isAnnouncementDelayed = false; // Flag to track if announcement is delayed\n        \n        // Navigation settings\n        this.isNavigating = false;\n        this.currentRoute = null;\n        this.currentStepIndex = 0;\n        this.locationWatcher = null;\n        this.routeDeviationThreshold = 15; // meters\n        \n        // Wake word detection\n        this.isListeningForWakeWord = true;\n        this.wakeWords = ['hey blindmate', 'hey blind mate', 'blindmate'];\n        this.continuousRecognition = null;\n        \n        // Volume key detection\n        this.volumeUpPressed = false;\n        this.volumeKeyTimeout = null;\n        \n        // Mobile double-tap gesture detection\n        this.lastTapTime = 0;\n        this.tapTimeout = null;\n        this.doubleTapDelay = 400; // milliseconds between taps (increased for better detection)\n        this.isMobileDevice = this.detectMobileDevice();\n        console.log('Mobile device detected:', this.isMobileDevice, {\n            userAgent: navigator.userAgent,\n            ontouchstart: 'ontouchstart' in window,\n            maxTouchPoints: navigator.maxTouchPoints\n        });\n        \n        this.init();\n    }\n\n\n\n    /**\n     * Get current position with error handling\n     */\n    getCurrentPosition() {\n        return new Promise((resolve, reject) => {\n            if (!navigator.geolocation) {\n                reject(new Error('Geolocation is not supported'));\n                return;\n            }\n            \n            navigator.geolocation.getCurrentPosition(\n                (position) => {\n                    resolve({\n                        lat: position.coords.latitude,\n                        lng: position.coords.longitude\n                    });\n                },\n                (error) => {\n                    let errorMessage = 'Location access failed. ';\n                    switch (error.code) {\n                        case error.PERMISSION_DENIED:\n                            errorMessage += 'Please enable GPS in your browser settings.';\n                            break;\n                        case error.POSITION_UNAVAILABLE:\n                            errorMessage += 'Location information is unavailable.';\n                            break;\n                        case error.TIMEOUT:\n                            errorMessage += 'Location request timed out.';\n                            break;\n                        default:\n                            errorMessage += 'An unknown error occurred.';\n                            break;\n                    }\n                    this.showError(errorMessage);\n                    this.speak('Location access is required. Please enable GPS in your browser settings.');\n                    reject(error);\n                },\n                {\n                    enableHighAccuracy: true,\n                    timeout: 10000,\n                    maximumAge: 60000\n                }\n            );\n        });\n    }\n\n    /**\n     * Update action status display\n     */\n    updateActionStatus(message, type = 'info') {\n        if (this.elements && this.elements.status && this.elements.statusText) {\n            this.elements.statusText.textContent = message;\n            this.elements.status.style.display = 'block';\n            this.elements.status.className = `alert alert-${type} mt-2`;\n            \n            // Auto-hide after 5 seconds for non-critical messages\n            if (type !== 'danger') {\n                setTimeout(() => {\n                    if (this.elements.status && this.elements.statusText.textContent === message) {\n                        this.elements.status.style.display = 'none';\n                    }\n                }, 5000);\n            }\n        }\n    }\n\n    /**\n     * Show error message\n     */\n    showError(message) {\n        if (this.elements && this.elements.errorMessage && this.elements.errorText) {\n            this.elements.errorText.textContent = message;\n            this.elements.errorMessage.style.display = 'block';\n            \n            // Auto-hide after 8 seconds\n            setTimeout(() => {\n                if (this.elements.errorMessage && this.elements.errorText.textContent === message) {\n                    this.elements.errorMessage.style.display = 'none';\n                }\n            }, 8000);\n        }\n    }\n\n    /**\n     * Monitor user position for route deviation\n     */\n    monitorPosition(expectedPath) {\n        if (this.locationWatcher) {\n            navigator.geolocation.clearWatch(this.locationWatcher);\n        }\n        \n        this.locationWatcher = navigator.geolocation.watchPosition(\n            (position) => {\n                const currentPos = {\n                    lat: position.coords.latitude,\n                    lng: position.coords.longitude\n                };\n                \n                // Check if user has deviated from route\n                if (this.isNavigating && this.currentRoute && this.currentRoute.legs) {\n                    const currentStep = this.getCurrentRouteStep();\n                    if (currentStep) {\n                        const distance = this.calculateDistance(\n                            currentPos,\n                            {\n                                lat: currentStep.end_location.lat(),\n                                lng: currentStep.end_location.lng()\n                            }\n                        );\n                        \n                        // If user is more than threshold distance away, re-route\n                        if (distance > this.routeDeviationThreshold) {\n                            this.handleRouteDeviation(currentPos);\n                        }\n                    }\n                }\n            },\n            (error) => {\n                console.warn('Position monitoring error:', error);\n                this.showError('GPS monitoring failed. Navigation accuracy may be reduced.');\n            },\n            {\n                enableHighAccuracy: true,\n                timeout: 5000,\n                maximumAge: 10000\n            }\n        );\n    }\n\n    /**\n     * Handle route deviation and re-calculate route\n     */\n    async handleRouteDeviation(currentPosition) {\n        try {\n            this.speak('You have moved off the route, recalculating...', true);\n            this.updateActionStatus('Re-routing...', 'warning');\n            \n            // Get the destination from current route\n            const destination = this.currentDestination;\n            if (!destination) {\n                this.showError('Cannot re-route: destination unknown');\n                return;\n            }\n            \n            // Re-calculate route from current position\n            await this.getDirections(currentPosition, destination);\n            \n            this.updateActionStatus('Route recalculated', 'success');\n            this.speak('New route calculated. Continuing navigation.');\n            \n        } catch (error) {\n            console.error('Re-routing failed:', error);\n            this.showError('Failed to recalculate route');\n            this.speak('Route recalculation failed. Please navigate manually.');\n        }\n    }\n\n    /**\n     * Get current route step\n     */\n    getCurrentRouteStep() {\n        if (!this.currentRoute || !this.currentRoute.legs || !this.currentRoute.legs[0]) {\n            return null;\n        }\n        \n        const steps = this.currentRoute.legs[0].steps;\n        if (this.currentStepIndex < steps.length) {\n            return steps[this.currentStepIndex];\n        }\n        \n        return null;\n    }\n\n    /**\n     * Calculate distance between two coordinates (Haversine formula)\n     */\n    calculateDistance(pos1, pos2) {\n        const R = 6371e3; // Earth's radius in meters\n        const œÜ1 = pos1.lat * Math.PI / 180;\n        const œÜ2 = pos2.lat * Math.PI / 180;\n        const ŒîœÜ = (pos2.lat - pos1.lat) * Math.PI / 180;\n        const ŒîŒª = (pos2.lng - pos1.lng) * Math.PI / 180;\n\n        const a = Math.sin(ŒîœÜ/2) * Math.sin(ŒîœÜ/2) +\n                Math.cos(œÜ1) * Math.cos(œÜ2) *\n                Math.sin(ŒîŒª/2) * Math.sin(ŒîŒª/2);\n        const c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1-a));\n\n        return R * c; // Distance in meters\n    }\n\n    /**\n     * Get location coordinates (supports both hardcoded and saved locations)\n     */\n    getLocationCoordinates(destinationName) {\n        // This function is deprecated - all destinations now go through Google Geocoding API\n        // Return null to force use of the enhanced navigation system\n        return null;\n    }\n\n    /**\n     * Simple stop navigation function\n     */\n    stopNavigationSimple() {\n        console.log('Stopping navigation');\n        \n        this.isNavigating = false;\n        this.currentRoute = null;\n        this.currentStepIndex = 0;\n        this.currentDestination = null;\n        \n        // Stop position monitoring\n        if (this.locationWatcher) {\n            navigator.geolocation.clearWatch(this.locationWatcher);\n            this.locationWatcher = null;\n        }\n        \n        this.updateActionStatus('Navigation stopped', 'warning');\n        this.speak('Navigation has been stopped', true);\n    }\n\n    /**\n     * Initialize the application\n     */\n    async init() {\n        try {\n            this.updateStatus('Initializing BlindMate...', 'info');\n            \n            // Load user preferences and check if this is a first-time user\n            await this.loadServerPreferences();\n            this.checkFirstTimeUser();\n            \n            // Initialize DOM elements first\n            this.initDOMElements();\n            \n            // Setup event listeners\n            this.setupEventListeners();\n            \n            // Initialize speech recognition\n            this.initSpeechRecognition();\n            \n            // Load TensorFlow model (optional - app works without it)\n            await this.loadModel();\n            \n            // Start voice interaction\n            this.startVoiceInteraction();\n            \n        } catch (error) {\n            console.error('Initialization error:', error);\n            this.updateStatus('Failed to initialize. Please refresh the page.', 'danger');\n            this.speak('Sorry, there was an error initializing the application. Please refresh the page.');\n        }\n    }\n    \n    /**\n     * Initialize DOM elements with fallback for missing elements\n     */\n    initDOMElements() {\n        this.elements = {\n            video: document.getElementById('webcam'),\n            canvas: document.getElementById('canvas'),\n            startBtn: document.getElementById('startDetectionBtn'),\n            stopBtn: document.getElementById('stopDetectionBtn'),\n            voiceBtn: document.getElementById('voiceCommandBtn'),\n            locationBtn: document.getElementById('locationBtn'),\n            languageSelect: document.getElementById('languageSelect'),\n            toneSelect: document.getElementById('toneSelect'),\n            detectionStatus: document.getElementById('detectionStatus'),\n            voiceStatus: document.getElementById('voiceStatus'),\n            systemStatus: document.getElementById('systemStatus'),\n            status: document.getElementById('status'),\n            statusText: document.getElementById('statusText'),\n            errorMessage: document.getElementById('error-message'),\n            errorText: document.getElementById('errorText'),\n            navigationStatus: document.getElementById('navigationStatus') || this.createNavigationStatus(),\n            loadingOverlay: document.getElementById('loadingOverlay'),\n            detectionIndicator: document.getElementById('detectionIndicator')\n        };\n\n        // Ensure all critical elements exist\n        this.validateElements();\n    }\n\n    /**\n     * Validate that essential elements exist and create fallbacks if needed\n     */\n    validateElements() {\n        const requiredElements = ['video', 'canvas', 'startBtn', 'stopBtn', 'voiceBtn', 'locationBtn', 'languageSelect', 'toneSelect', 'systemStatus'];\n        \n        for (const elementKey of requiredElements) {\n            if (!this.elements[elementKey]) {\n                console.warn(`Missing element: ${elementKey}`);\n                \n                // Create fallback element to prevent crashes\n                if (elementKey === 'systemStatus') {\n                    this.elements[elementKey] = this.createStatusElement();\n                }\n            }\n        }\n    }\n\n    /**\n     * Create fallback status element\n     */\n    createStatusElement() {\n        const statusDiv = document.createElement('div');\n        statusDiv.className = 'alert alert-info';\n        statusDiv.textContent = 'System ready';\n        return statusDiv;\n    }\n\n    /**\n     * Detect if device is mobile\n     */\n    detectMobileDevice() {\n        return /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent) || \n               ('ontouchstart' in window) || \n               (navigator.maxTouchPoints > 0);\n    }\n\n    /**\n     * Setup mobile double-tap gesture detection\n     */\n    setupMobileDoubleTap() {\n        let firstTapTime = 0;\n        let tapCount = 0;\n        let tapTimeout = null;\n        \n        console.log('Setting up mobile double-tap gesture detection...');\n        \n        // Add touch event listener to entire document for full-screen double-tap\n        document.addEventListener('touchend', (e) => {\n            const currentTime = Date.now();\n            \n            // Clear existing timeout\n            if (tapTimeout) {\n                clearTimeout(tapTimeout);\n                tapTimeout = null;\n            }\n            \n            // Prevent interference with UI elements that need single taps\n            const target = e.target;\n            const isUIElement = target.tagName === 'BUTTON' || \n                              target.tagName === 'SELECT' || \n                              target.tagName === 'INPUT' ||\n                              target.closest('button') || \n                              target.closest('select') ||\n                              target.closest('input') ||\n                              target.closest('.btn') ||\n                              target.id.includes('Btn') ||\n                              target.className.includes('btn') ||\n                              target.classList.contains('form-control') ||\n                              target.classList.contains('form-select');\n            \n            // Skip double-tap detection on UI elements\n            if (isUIElement) {\n                console.log('Tap on UI element ignored:', target.tagName, target.id || target.className);\n                return;\n            }\n            \n            tapCount++;\n            \n            if (tapCount === 1) {\n                // First tap\n                firstTapTime = currentTime;\n                \n                // Set timeout to reset tap count if no second tap\n                tapTimeout = setTimeout(() => {\n                    tapCount = 0;\n                    firstTapTime = 0;\n                    console.log('Double-tap timeout - single tap detected');\n                }, this.doubleTapDelay);\n                \n                console.log('First tap detected, waiting for second tap...');\n                \n            } else if (tapCount === 2) {\n                // Second tap - check if within double-tap delay\n                const timeDiff = currentTime - firstTapTime;\n                \n                if (timeDiff <= this.doubleTapDelay) {\n                    // Double-tap detected!\n                    e.preventDefault(); // Prevent default zoom behavior\n                    e.stopPropagation(); // Stop event bubbling\n                    \n                    console.log(`Double-tap detected! Time difference: ${timeDiff}ms`);\n                    \n                    // Provide immediate feedback\n                    navigator.vibrate && navigator.vibrate(50); // Short vibration if available\n                    this.speak('Listening started');\n                    \n                    // Call the same function as voice command button\n                    setTimeout(() => {\n                        this.startVoiceCommand();\n                    }, 100); // Small delay to ensure speech starts first\n                    \n                    // Reset counters\n                    tapCount = 0;\n                    firstTapTime = 0;\n                } else {\n                    // Too slow, treat as new first tap\n                    tapCount = 1;\n                    firstTapTime = currentTime;\n                    \n                    tapTimeout = setTimeout(() => {\n                        tapCount = 0;\n                        firstTapTime = 0;\n                    }, this.doubleTapDelay);\n                    \n                    console.log('Second tap too slow, treating as new first tap');\n                }\n            }\n        }, { passive: false });\n        \n        // Also add touchstart to prevent default behaviors during double-tap\n        document.addEventListener('touchstart', (e) => {\n            // Only prevent default on non-UI elements during potential double-tap\n            const target = e.target;\n            const isUIElement = target.tagName === 'BUTTON' || \n                              target.tagName === 'SELECT' || \n                              target.tagName === 'INPUT' ||\n                              target.closest('button') || \n                              target.closest('select') ||\n                              target.closest('input') ||\n                              target.closest('.btn') ||\n                              target.id.includes('Btn') ||\n                              target.className.includes('btn');\n            \n            if (!isUIElement && tapCount === 1) {\n                // During potential double-tap sequence, prevent default behaviors\n                e.preventDefault();\n            }\n        }, { passive: false });\n        \n        console.log('Mobile double-tap gesture enabled for voice commands with improved detection');\n        \n        // Add visual hint for mobile users\n        this.addMobileHint();\n    }\n\n    /**\n     * Add visual hint for mobile double-tap feature\n     */\n    addMobileHint() {\n        // Create hint element\n        const hintElement = document.createElement('div');\n        hintElement.id = 'mobileHint';\n        hintElement.className = 'alert alert-info mobile-hint';\n        hintElement.style.cssText = `\n            position: fixed;\n            bottom: 20px;\n            left: 50%;\n            transform: translateX(-50%);\n            z-index: 1000;\n            background: rgba(0, 123, 255, 0.9);\n            color: white;\n            padding: 10px 20px;\n            border-radius: 25px;\n            font-size: 14px;\n            text-align: center;\n            animation: fadeInOut 4s ease-in-out;\n            pointer-events: none;\n        `;\n        hintElement.innerHTML = 'üí° Double-tap anywhere to start voice commands';\n        \n        // Add CSS animation\n        const style = document.createElement('style');\n        style.textContent = `\n            @keyframes fadeInOut {\n                0% { opacity: 0; transform: translateX(-50%) translateY(20px); }\n                15% { opacity: 1; transform: translateX(-50%) translateY(0); }\n                85% { opacity: 1; transform: translateX(-50%) translateY(0); }\n                100% { opacity: 0; transform: translateX(-50%) translateY(-20px); }\n            }\n        `;\n        document.head.appendChild(style);\n        \n        // Add to page\n        document.body.appendChild(hintElement);\n        \n        // Remove after animation\n        setTimeout(() => {\n            if (hintElement.parentNode) {\n                hintElement.parentNode.removeChild(hintElement);\n            }\n        }, 4000);\n    }\n    \n    /**\n     * Create navigation status element if it doesn't exist\n     */\n    createNavigationStatus() {\n        const navStatus = document.createElement('span');\n        navStatus.id = 'navigationStatus';\n        navStatus.className = 'badge bg-secondary';\n        navStatus.textContent = 'Ready';\n        return navStatus;\n    }\n\n    /**\n     * Setup all event listeners\n     */\n    setupEventListeners() {\n        // Add event listeners with null checks\n        if (this.elements.startBtn) {\n            this.elements.startBtn.addEventListener('click', () => this.startDetection());\n        }\n        if (this.elements.stopBtn) {\n            this.elements.stopBtn.addEventListener('click', () => this.stopDetection());\n        }\n        if (this.elements.voiceBtn) {\n            this.elements.voiceBtn.addEventListener('click', () => this.startVoiceCommand());\n        }\n        if (this.elements.locationBtn) {\n            this.elements.locationBtn.addEventListener('click', () => this.requestLocation());\n        }\n        if (this.elements.languageSelect) {\n            this.elements.languageSelect.addEventListener('change', (e) => this.changeLanguage(e.target.value));\n        }\n        if (this.elements.toneSelect) {\n            this.elements.toneSelect.addEventListener('change', (e) => this.changeTone(e.target.value));\n        }\n        \n        // Mobile double-tap gesture for voice commands\n        console.log('Checking mobile device for double-tap setup:', this.isMobileDevice);\n        if (this.isMobileDevice) {\n            this.setupMobileDoubleTap();\n        } else {\n            console.log('Desktop device - double-tap not enabled');\n        }\n        \n        // Keyboard shortcuts for accessibility and volume key detection\n        document.addEventListener('keydown', (e) => {\n            // Volume Up key detection (multiple key codes for different devices)\n            if (e.key === 'VolumeUp' || e.keyCode === 175 || e.keyCode === 174 || \n                e.code === 'VolumeUp' || e.code === 'AudioVolumeUp') {\n                e.preventDefault();\n                this.handleVolumeUpPress();\n                return;\n            }\n            \n            // Ctrl + key shortcuts\n            if (e.ctrlKey) {\n                switch(e.key) {\n                    case 's':\n                        e.preventDefault();\n                        if (this.isDetecting) {\n                            this.stopDetection();\n                        } else {\n                            this.startDetection();\n                        }\n                        break;\n                    case 'v':\n                        e.preventDefault();\n                        this.startVoiceCommand();\n                        break;\n                    case 'l':\n                        e.preventDefault();\n                        this.requestLocation();\n                        break;\n                }\n            }\n        });\n    }\n\n    /**\n     * Initialize speech recognition for voice commands\n     */\n    initSpeechRecognition() {\n        if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {\n            console.warn('Speech recognition not supported');\n            this.elements.voiceBtn.disabled = true;\n            this.updateStatus('Voice commands not supported. Use text input instead.', 'warning');\n            this.showTextFallback();\n            return;\n        }\n\n        // Initialize speech recognition\n        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n        \n        // Create recognition instance for voice commands\n        this.commandRecognition = new SpeechRecognition();\n        this.commandRecognition.continuous = false;\n        this.commandRecognition.interimResults = false;\n        this.commandRecognition.lang = this.currentLanguage;\n        \n        // Command recognition event handlers\n        this.commandRecognition.onstart = () => {\n            this.isListening = true;\n            this.updateStatus('üé§ Listening... Speak your command now', 'primary');\n            this.elements.voiceStatus.textContent = 'Listening';\n            this.elements.voiceStatus.className = 'badge bg-primary';\n            this.elements.voiceBtn.innerHTML = '<i class=\"fas fa-stop\"></i> Stop Listening';\n            this.speak('Speak your command now', true);\n        };\n        \n        this.commandRecognition.onresult = (event) => {\n            const command = event.results[0][0].transcript.trim();\n            const confidence = event.results[0][0].confidence;\n            console.log('Voice command received:', command, 'Confidence:', confidence);\n            \n            // Show command in UI\n            this.showRecognizedCommand(command);\n            \n            // Process the command via Gemini\n            this.processVoiceCommand(command);\n        };\n        \n        this.commandRecognition.onerror = (event) => {\n            console.error('Command recognition error:', event.error);\n            this.isListening = false;\n            this.elements.voiceStatus.textContent = 'Error';\n            this.elements.voiceStatus.className = 'badge bg-danger';\n            this.elements.voiceBtn.innerHTML = '<i class=\"fas fa-microphone\"></i> Voice Command';\n            \n            let errorMessage = 'Voice recognition error';\n            if (event.error === 'not-allowed') {\n                errorMessage = 'Microphone access denied. Please allow microphone access and try again.';\n                this.showTextFallback();\n            } else if (event.error === 'no-speech') {\n                errorMessage = 'No speech detected. Please try again.';\n            }\n            \n            this.updateStatus(errorMessage, 'danger');\n            this.speak('Voice command failed. Try again or use the buttons.', true);\n        };\n        \n        this.commandRecognition.onend = () => {\n            this.isListening = false;\n            this.elements.voiceStatus.textContent = 'Ready';\n            this.elements.voiceStatus.className = 'badge bg-secondary';\n            this.elements.voiceBtn.innerHTML = '<i class=\"fas fa-microphone\"></i> Voice Command';\n            this.updateStatus('Voice command completed. Say \"Hey BlindMate\" or press Volume Up for next command.', 'success');\n            \n            // Restart continuous listening for wake words\n            setTimeout(() => {\n                this.startContinuousListening();\n            }, 1000);\n        };\n\n        // Add click handler for voice button\n        this.elements.voiceBtn.addEventListener('click', () => {\n            if (this.isListening) {\n                this.stopVoiceCommand();\n            } else {\n                this.startVoiceCommand();\n            }\n        });\n        \n        // Initialize continuous recognition for wake words separately\n        this.initContinuousListening();\n    }\n    \n    /**\n     * Initialize continuous listening for wake words\n     */\n    initContinuousListening() {\n        if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {\n            return;\n        }\n\n        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n        this.continuousRecognition = new SpeechRecognition();\n        this.continuousRecognition.continuous = true;\n        this.continuousRecognition.interimResults = true;\n        this.continuousRecognition.lang = this.currentLanguage;\n        \n        this.continuousRecognition.onresult = (event) => {\n            for (let i = event.resultIndex; i < event.results.length; i++) {\n                const result = event.results[i];\n                const command = result[0].transcript.toLowerCase().trim();\n                \n                console.log('Continuous listening heard:', command);\n                \n                // Check for wake words with better matching\n                if (this.wakeWords.some(wake => command.includes(wake))) {\n                    console.log('Wake word detected:', command);\n                    this.handleWakeWordDetected();\n                    break;\n                }\n            }\n        };\n        \n        this.continuousRecognition.onerror = (event) => {\n            console.log('Continuous recognition error:', event.error);\n            if (event.error !== 'aborted') {\n                // Restart continuous listening after a short delay\n                setTimeout(() => {\n                    if (this.isListeningForWakeWord) {\n                        this.startContinuousListening();\n                    }\n                }, 1000);\n            }\n        };\n        \n        this.continuousRecognition.onend = () => {\n            // Restart continuous listening if it should be active\n            if (this.isListeningForWakeWord && !this.isListening) {\n                setTimeout(() => {\n                    this.startContinuousListening();\n                }, 500);\n            }\n        };\n    }\n    \n    /**\n     * Start voice command\n     */\n    startVoiceCommand() {\n        if (!this.commandRecognition) {\n            this.updateStatus('Voice recognition not available.', 'danger');\n            this.showTextFallback();\n            return;\n        }\n\n        if (this.isListening) {\n            this.stopVoiceCommand();\n            return;\n        }\n\n        try {\n            // Stop continuous listening temporarily\n            this.stopContinuousListening();\n            \n            // Start command recognition\n            this.commandRecognition.lang = this.currentLanguage;\n            this.commandRecognition.start();\n        } catch (error) {\n            console.error('Error starting voice recognition:', error);\n            this.updateStatus('Could not start voice recognition. Try again.', 'danger');\n            \n            // Restart continuous listening\n            this.startContinuousListening();\n        }\n    }\n    \n    /**\n     * Stop voice command\n     */\n    stopVoiceCommand() {\n        if (this.commandRecognition && this.isListening) {\n            this.commandRecognition.stop();\n        }\n    }\n    \n    /**\n     * Start continuous listening for wake words\n     */\n    startContinuousListening() {\n        if (this.continuousRecognition && this.isListeningForWakeWord && !this.isListening) {\n            try {\n                this.continuousRecognition.lang = this.currentLanguage;\n                this.continuousRecognition.start();\n            } catch (error) {\n                console.log('Continuous listening start error:', error.message);\n            }\n        }\n    }\n    \n    /**\n     * Stop continuous listening\n     */\n    stopContinuousListening() {\n        if (this.continuousRecognition) {\n            try {\n                this.continuousRecognition.stop();\n            } catch (error) {\n                console.log('Continuous listening stop error:', error.message);\n            }\n        }\n    }\n    \n    /**\n     * Handle wake word detection\n     */\n    handleWakeWordDetected() {\n        console.log('Wake word \"Hey BlindMate\" detected!');\n        this.updateStatus('üé§ Wake word detected! Listening for command...', 'success');\n        \n        // Stop continuous listening temporarily\n        this.stopContinuousListening();\n        \n        // Give audio feedback\n        this.speak('Yes, how can I help you?', true);\n        \n        // Start command listening after response\n        setTimeout(() => {\n            this.startVoiceCommand();\n        }, 1500);\n    }\n    \n    /**\n     * Handle volume up key press for voice activation\n     */\n    handleVolumeUpPress() {\n        console.log('Volume Up key pressed for voice activation');\n        \n        // Prevent multiple rapid presses\n        if (this.volumeKeyTimeout) {\n            clearTimeout(this.volumeKeyTimeout);\n        }\n        \n        // If already listening, stop\n        if (this.isListening) {\n            this.stopVoiceCommand();\n            this.speak('Voice command stopped', true);\n            return;\n        }\n        \n        // Start voice command\n        this.updateStatus('üé§ Volume Up pressed - Starting voice command...', 'info');\n        this.speak('Voice command activated. Speak now.', true);\n        \n        // Small delay to let the speech finish\n        this.volumeKeyTimeout = setTimeout(() => {\n            this.startVoiceCommand();\n        }, 1000);\n    }\n    \n    /**\n     * Show text fallback input for when voice is not available\n     */\n    showTextFallback() {\n        if (document.getElementById('textCommandInput')) return; // Already shown\n        \n        const fallbackHtml = `\n            <div class=\"mt-3 p-3 border rounded bg-light\">\n                <h6>Voice not available? Use text instead:</h6>\n                <div class=\"input-group\">\n                    <input type=\"text\" id=\"textCommandInput\" class=\"form-control\" \n                           placeholder=\"Type your command (e.g., 'start detection', 'take me to library')\">\n                    <button class=\"btn btn-primary\" id=\"textCommandBtn\">\n                        <i class=\"fas fa-paper-plane\"></i> Send\n                    </button>\n                </div>\n                <small class=\"text-muted\">Commands: start detection, stop, where am i, take me to [place], enable location</small>\n            </div>\n        `;\n        \n        const controlsSection = document.querySelector('.col-md-6:last-child .card-body');\n        if (controlsSection) {\n            controlsSection.insertAdjacentHTML('beforeend', fallbackHtml);\n            \n            const textInput = document.getElementById('textCommandInput');\n            const textBtn = document.getElementById('textCommandBtn');\n            \n            const processTextCommand = () => {\n                const command = textInput.value.trim();\n                if (command) {\n                    this.showRecognizedCommand(command);\n                    this.processVoiceCommand(command);\n                    textInput.value = '';\n                }\n            };\n            \n            textBtn.addEventListener('click', processTextCommand);\n            textInput.addEventListener('keypress', (e) => {\n                if (e.key === 'Enter') {\n                    processTextCommand();\n                }\n            });\n        }\n    }\n    \n    /**\n     * Show the recognized command in UI\n     */\n    showRecognizedCommand(command) {\n        // Update the system status to show the command\n        this.updateStatus(`Command received: \"${command}\"`, 'info');\n        \n        // Show in dedicated command display\n        let commandDisplay = document.getElementById('lastCommand');\n        if (!commandDisplay) {\n            const statusArea = document.getElementById('systemStatus').parentElement;\n            statusArea.insertAdjacentHTML('afterend', `\n                <div class=\"alert alert-info mt-2\" id=\"lastCommand\" style=\"display: none;\">\n                    <strong>Last Command:</strong> <span id=\"commandText\"></span>\n                </div>\n            `);\n            commandDisplay = document.getElementById('lastCommand');\n        }\n        \n        document.getElementById('commandText').textContent = command;\n        commandDisplay.style.display = 'block';\n        \n        // Hide after 5 seconds\n        setTimeout(() => {\n            commandDisplay.style.display = 'none';\n        }, 5000);\n    }\n\n    /**\n     * Load TensorFlow.js Coco SSD model\n     */\n    async loadModel() {\n        try {\n            this.updateStatus('Loading AI detection model...', 'warning');\n            \n            // Check if TensorFlow.js is available\n            if (typeof tf === 'undefined') {\n                throw new Error('TensorFlow.js not loaded');\n            }\n            \n            // Set backend to CPU if WebGL is not available\n            if (!tf.ENV.getBool('WEBGL_VERSION')) {\n                console.warn('WebGL not available, falling back to CPU backend');\n                await tf.setBackend('cpu');\n            }\n            \n            // Ensure TensorFlow.js is ready\n            await tf.ready();\n            \n            // Check if COCO-SSD is available\n            if (typeof cocoSsd === 'undefined') {\n                throw new Error('COCO-SSD model not loaded');\n            }\n            \n            // Load COCO-SSD model\n            this.model = await cocoSsd.load();\n            \n            this.updateStatus('AI model loaded successfully!', 'success');\n            if (this.elements.loadingOverlay) {\n                this.elements.loadingOverlay.style.display = 'none';\n            }\n            \n            console.log('COCO-SSD model loaded successfully');\n            \n        } catch (error) {\n            console.error('Error loading model:', error);\n            this.updateStatus('Object detection disabled. Voice commands and navigation still available.', 'warning');\n            \n            // Hide loading overlay even on error\n            if (this.elements.loadingOverlay) {\n                this.elements.loadingOverlay.style.display = 'none';\n            }\n            \n            // Don't throw error - allow app to continue without object detection\n            console.log('Continuing without object detection...');\n        }\n    }\n\n    /**\n     * Start voice interaction flow\n     */\n    startVoiceInteraction() {\n        const greeting = 'Hello! I am BlindMate, your AI assistant. Say \"Hey BlindMate\" or press Volume Up anytime to give me voice commands.';\n        this.speak(greeting, true); // High priority\n        \n        // Start continuous listening for wake word after greeting\n        setTimeout(() => {\n            this.startContinuousListening();\n            this.updateStatus('üëÇ Always listening for \"Hey BlindMate\" or Volume Up key', 'info');\n        }, 4000);\n    }\n    \n    /**\n     * Setup voice-guided permission flow\n     */\n    setupVoicePermissionFlow() {\n        if (this.recognition && !this.isListening) {\n            this.recognition.continuous = false; // Short responses for permissions\n            this.recognition.interimResults = false;\n            \n            this.recognition.onresult = (event) => {\n                const command = event.results[event.results.length - 1][0].transcript.toLowerCase().trim();\n                console.log('Permission flow - heard:', command);\n                \n                if (command.includes('yes') || command.includes('‡§π‡§æ‡§Å') || command.includes('‡¶ì‡¶Ø‡¶º‡¶æ‡¶á') || command.includes('‡ÆÜ‡ÆÆ‡Øç')) {\n                    this.handlePermissionYes();\n                } else if (command.includes('no') || command.includes('‡§®‡§π‡•Ä‡§Ç') || command.includes('‡¶®‡¶æ') || command.includes('‡Æá‡¶≤‡Øç‡Æ≤‡Øà')) {\n                    this.handlePermissionNo();\n                }\n            };\n            \n            this.recognition.onerror = (event) => {\n                console.log('Permission recognition error:', event.error);\n                this.isListening = false;\n            };\n            \n            this.recognition.onend = () => {\n                this.isListening = false;\n            };\n            \n            try {\n                this.recognition.start();\n                this.isListening = true;\n            } catch (error) {\n                console.log('Could not start permission recognition:', error);\n            }\n        }\n    }\n    \n    /**\n     * Handle \"yes\" response during permission flow\n     */\n    async handlePermissionYes() {\n        if (!this.stream) {\n            // First \"yes\" - start detection\n            this.speak('Starting camera detection now.', true);\n            await this.startDetection();\n            \n            // Ask for location\n            setTimeout(() => {\n                this.speak('Would you like to enable location for navigation?', true);\n            }, 2000);\n        } else if (!this.userLocation) {\n            // Second \"yes\" - enable location\n            this.speak('Enabling location services.', true);\n            await this.requestLocation();\n            this.finalizeSetup();\n        }\n    }\n    \n    /**\n     * Handle \"no\" response during permission flow\n     */\n    handlePermissionNo() {\n        this.speak('Okay, you can enable features later using voice commands or buttons.', true);\n        this.finalizeSetup();\n    }\n    \n    /**\n     * Finalize setup and start continuous listening\n     */\n    finalizeSetup() {\n        setTimeout(() => {\n            this.speak('Setup complete. Say \"Hey BlindMate\" followed by your command to interact with me.', true);\n            this.startContinuousListening();\n        }, 2000);\n    }\n    \n    /**\n     * Start continuous listening for wake word\n     */\n    startContinuousListening() {\n        if (!this.recognition || this.continuousRecognition) return;\n        \n        try {\n            this.continuousRecognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();\n            this.continuousRecognition.continuous = true;\n            this.continuousRecognition.interimResults = false;\n            this.continuousRecognition.lang = this.currentLanguage;\n            \n            this.continuousRecognition.onresult = (event) => {\n                const command = event.results[event.results.length - 1][0].transcript.toLowerCase().trim();\n                console.log('Continuous listening heard:', command);\n                \n                // Check for wake word\n                const hasWakeWord = this.wakeWords.some(wake => command.includes(wake));\n                \n                if (hasWakeWord) {\n                    // Extract command after wake word\n                    const commandAfterWake = command.split(/hey\\s*blind\\s*mate\\s*/i)[1]?.trim();\n                    if (commandAfterWake) {\n                        this.speak('Yes, how can I help?', true);\n                        this.processVoiceCommand(commandAfterWake);\n                    } else {\n                        this.speak('Yes, I am listening. What can I do for you?', true);\n                    }\n                }\n            };\n            \n            this.continuousRecognition.onerror = (event) => {\n                console.log('Continuous recognition error:', event.error);\n                // Only restart if it's not already running\n                if (event.error !== 'aborted') {\n                    setTimeout(() => {\n                        if (this.continuousRecognition && !this.isListening) {\n                            try {\n                                this.continuousRecognition.start();\n                            } catch (e) {\n                                console.log('Could not restart continuous recognition:', e);\n                            }\n                        }\n                    }, 2000);\n                }\n            };\n            \n            this.continuousRecognition.onend = () => {\n                // Only restart if we should be listening\n                if (this.continuousRecognition && !this.isListening) {\n                    setTimeout(() => {\n                        try {\n                            this.continuousRecognition.start();\n                        } catch (e) {\n                            console.log('Could not restart continuous recognition:', e);\n                        }\n                    }, 1000);\n                }\n            };\n            \n            this.continuousRecognition.start();\n            \n        } catch (error) {\n            console.log('Could not start continuous listening:', error);\n        }\n    }\n\n    /**\n     * Start object detection\n     */\n    async startDetection() {\n        try {\n            if (!this.model) {\n                this.speak('AI model is not ready. Please wait.');\n                return;\n            }\n\n            this.updateStatus('Starting camera...', 'warning');\n            \n            // Request camera access\n            this.stream = await navigator.mediaDevices.getUserMedia({\n                video: { \n                    width: { ideal: 640 }, \n                    height: { ideal: 480 },\n                    facingMode: 'environment' // Use back camera on mobile\n                }\n            });\n            \n            this.video.srcObject = this.stream;\n            \n            // Wait for video to be ready\n            await new Promise((resolve) => {\n                this.video.onloadedmetadata = () => {\n                    this.video.play();\n                    resolve();\n                };\n            });\n            \n            // Setup canvas dimensions\n            this.canvas.width = this.video.videoWidth;\n            this.canvas.height = this.video.videoHeight;\n            \n            this.isDetecting = true;\n            this.updateStatus('Detection active - Scanning for objects...', 'success');\n            this.elements.detectionStatus.textContent = 'Active';\n            this.elements.detectionStatus.className = 'badge bg-success';\n            \n            // Show detection indicator\n            this.elements.detectionIndicator.style.display = 'block';\n            this.elements.detectionIndicator.classList.add('active');\n            \n            this.elements.startBtn.disabled = true;\n            this.elements.stopBtn.disabled = false;\n            \n            this.speak('Object detection started. I will alert you about any obstacles or objects I detect.');\n            \n            // Start detection loop\n            this.detectObjects();\n            \n        } catch (error) {\n            console.error('Error starting detection:', error);\n            this.updateStatus('Camera unavailable. Voice commands and navigation are still active.', 'warning');\n            this.speak('Camera is not available, but voice commands and navigation are ready to use.');\n        }\n    }\n\n    /**\n     * Stop object detection\n     */\n    stopDetection() {\n        this.isDetecting = false;\n        \n        // Clean up speech delay timer\n        if (this.speechDelayTimer) {\n            clearTimeout(this.speechDelayTimer);\n            this.speechDelayTimer = null;\n        }\n        this.pendingAnnouncement = null;\n        this.isAnnouncementDelayed = false;\n        \n        if (this.stream) {\n            this.stream.getTracks().forEach(track => track.stop());\n            this.stream = null;\n        }\n        \n        this.video.srcObject = null;\n        this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);\n        \n        this.updateStatus('Detection stopped.', 'secondary');\n        this.elements.detectionStatus.textContent = 'Inactive';\n        this.elements.detectionStatus.className = 'badge bg-secondary';\n        \n        // Hide detection indicator\n        this.elements.detectionIndicator.style.display = 'none';\n        this.elements.detectionIndicator.classList.remove('active');\n        \n        this.elements.startBtn.disabled = false;\n        this.elements.stopBtn.disabled = true;\n        \n        this.speak('Object detection stopped.');\n    }\n\n    /**\n     * Main object detection loop\n     */\n    async detectObjects() {\n        if (!this.isDetecting || !this.model) {\n            return;\n        }\n\n        try {\n            // Perform detection\n            const predictions = await this.model.detect(this.video);\n            \n            // Clear previous drawings\n            this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);\n            \n            // Filter predictions by confidence threshold\n            const validPredictions = predictions.filter(prediction => \n                prediction.score >= this.detectionThreshold\n            );\n            \n            if (validPredictions.length > 0) {\n                this.drawPredictions(validPredictions);\n                \n                // Update object tracking and announce with smart system\n                this.updateObjectTracking(validPredictions);\n                this.announceDetectionsSmart(validPredictions);\n            } else {\n                // No objects detected, update tracking for disappearances\n                this.updateObjectTracking([]);\n            }\n            \n            // Continue detection loop\n            requestAnimationFrame(() => this.detectObjects());\n            \n        } catch (error) {\n            console.error('Detection error:', error);\n            // Continue detection even if one frame fails\n            setTimeout(() => this.detectObjects(), 100);\n        }\n    }\n\n    /**\n     * Draw bounding boxes and labels on canvas with improved styling\n     */\n    drawPredictions(predictions) {\n        // Clear previous drawings\n        this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);\n        \n        predictions.forEach((prediction, index) => {\n            const [x, y, width, height] = prediction.bbox;\n            const confidence = Math.round(prediction.score * 100);\n            const label = `${prediction.class} ${confidence}%`;\n            \n            // Color coding for different object types\n            let boxColor = '#00ff00'; // Default green\n            if (prediction.class === 'person') boxColor = '#ff6b6b'; // Red for people\n            else if (prediction.class.includes('vehicle') || prediction.class === 'car' || prediction.class === 'truck') boxColor = '#ffa500'; // Orange for vehicles\n            else if (prediction.class === 'chair' || prediction.class === 'couch') boxColor = '#4ecdc4'; // Teal for furniture\n            \n            // Draw bounding box with shadow for better visibility\n            this.ctx.shadowColor = 'rgba(0, 0, 0, 0.8)';\n            this.ctx.shadowBlur = 3;\n            this.ctx.strokeStyle = boxColor;\n            this.ctx.lineWidth = 3;\n            this.ctx.strokeRect(x, y, width, height);\n            \n            // Reset shadow for text\n            this.ctx.shadowBlur = 0;\n            \n            // Measure text to create proper background\n            this.ctx.font = 'bold 16px Arial';\n            const textMetrics = this.ctx.measureText(label);\n            const textWidth = textMetrics.width + 10;\n            const textHeight = 25;\n            \n            // Draw label background with some padding\n            this.ctx.fillStyle = boxColor;\n            this.ctx.fillRect(x, y - textHeight, textWidth, textHeight);\n            \n            // Draw label text\n            this.ctx.fillStyle = '#000000';\n            this.ctx.fillText(label, x + 5, y - 7);\n            \n            // Add distance indicator\n            const distance = this.estimateDistance(prediction.bbox);\n            this.ctx.font = 'bold 12px Arial';\n            this.ctx.fillStyle = '#ffffff';\n            this.ctx.fillText(distance, x + 5, y + height - 5);\n        });\n    }\n\n    /**\n     * Announce detected objects via speech with priority system\n     */\n    /**\n     * Update object tracking system for smart announcements\n     */\n    updateObjectTracking(predictions) {\n        const now = Date.now();\n        const currentDetectedObjects = new Set();\n        \n        // Extract object names from current predictions\n        predictions.forEach(prediction => {\n            currentDetectedObjects.add(prediction.class);\n        });\n        \n        // Update last seen time for currently detected objects\n        for (const objectName of currentDetectedObjects) {\n            this.objectLastSeen.set(objectName, now);\n            \n            // Remove from disappearance tracking if it reappeared\n            if (this.objectDisappearanceTime.has(objectName)) {\n                this.objectDisappearanceTime.delete(objectName);\n            }\n        }\n        \n        // Check for disappeared objects and mark their disappearance time\n        for (const [objectName, lastSeenTime] of this.objectLastSeen.entries()) {\n            if (!currentDetectedObjects.has(objectName) && !this.objectDisappearanceTime.has(objectName)) {\n                // Object just disappeared, mark the time\n                this.objectDisappearanceTime.set(objectName, now);\n            }\n        }\n        \n        // Clean up objects that have been gone for longer than cooldown period\n        for (const [objectName, disappearanceTime] of this.objectDisappearanceTime.entries()) {\n            if (now - disappearanceTime > this.cooldownPeriod) {\n                // Reset announcement count for objects that have been gone long enough\n                this.objectAnnouncementCount.delete(objectName);\n                this.objectLastSeen.delete(objectName);\n                this.objectDisappearanceTime.delete(objectName);\n            }\n        }\n    }\n\n    /**\n     * Smart announcement system with 3-announcement limit and cooldown\n     */\n    announceDetectionsSmart(predictions) {\n        const now = Date.now();\n        \n        // Respect global announcement cooldown\n        if (now - this.lastAnnouncement < this.announcementInterval) {\n            return;\n        }\n        \n        // Priority objects (most important for navigation)\n        const priorityObjects = ['person', 'chair', 'car', 'truck', 'bus', 'bicycle', 'motorcycle'];\n        \n        // Filter predictions that can be announced based on smart tracking\n        const announcablePredictions = predictions.filter(prediction => {\n            const announcementCount = this.objectAnnouncementCount.get(prediction.class) || 0;\n            \n            if (announcementCount >= this.maxAnnouncements) {\n                return false; // Already announced 3 times\n            }\n            \n            // Check if object was missing and came back (reset scenario)\n            const disappearanceTime = this.objectDisappearanceTime.get(prediction.class);\n            if (disappearanceTime && (now - disappearanceTime) < this.cooldownPeriod) {\n                return false; // Object reappeared too quickly, don't announce\n            }\n            \n            return true;\n        });\n        \n        if (announcablePredictions.length === 0) {\n            // Debug: Show why objects weren't announced\n            predictions.forEach(prediction => {\n                const count = this.objectAnnouncementCount.get(prediction.class) || 0;\n                const disappearanceTime = this.objectDisappearanceTime.get(prediction.class);\n                const timeSinceDisappearance = disappearanceTime ? (now - disappearanceTime) : null;\n                \n                if (count >= this.maxAnnouncements) {\n                    console.log(`${prediction.class}: Max announcements reached (${count}/${this.maxAnnouncements})`);\n                } else if (timeSinceDisappearance !== null && timeSinceDisappearance < this.cooldownPeriod) {\n                    console.log(`${prediction.class}: In cooldown (${Math.round(timeSinceDisappearance/1000)}s/${Math.round(this.cooldownPeriod/1000)}s)`);\n                }\n            });\n            return; // No objects to announce\n        }\n        \n        // Sort predictions by priority and distance\n        const sortedPredictions = announcablePredictions.sort((a, b) => {\n            const aPriority = priorityObjects.includes(a.class) ? 1 : 0;\n            const bPriority = priorityObjects.includes(b.class) ? 1 : 0;\n            \n            if (aPriority !== bPriority) {\n                return bPriority - aPriority; // Higher priority first\n            }\n            \n            // If same priority, sort by size (closer objects are larger)\n            const aSize = a.bbox[2] * a.bbox[3];\n            const bSize = b.bbox[2] * b.bbox[3];\n            return bSize - aSize;\n        });\n        \n        // Take only the most important objects (max 2)\n        const importantObjects = sortedPredictions.slice(0, 2);\n        \n        if (importantObjects.length > 0) {\n            // Increment announcement count for announced objects\n            importantObjects.forEach(prediction => {\n                const currentCount = this.objectAnnouncementCount.get(prediction.class) || 0;\n                this.objectAnnouncementCount.set(prediction.class, currentCount + 1);\n                \n                // Debug logging for smart announcement system\n                console.log(`Smart Announcement: ${prediction.class} (count: ${currentCount + 1}/${this.maxAnnouncements})`);\n            });\n            \n            const objectsWithDistance = importantObjects.map(prediction => {\n                const distance = this.estimateDistance(prediction.bbox);\n                const position = this.getRelativePosition(prediction.bbox);\n                return { \n                    name: prediction.class, \n                    distance: distance,\n                    position: position,\n                    confidence: Math.round(prediction.score * 100)\n                };\n            });\n            \n            // Create contextual announcement\n            let announcement = '';\n            objectsWithDistance.forEach((obj, index) => {\n                if (index > 0) announcement += '. Also, ';\n                \n                // More natural language\n                if (obj.name === 'person') {\n                    announcement += `person ${obj.position}, ${obj.distance}`;\n                } else {\n                    announcement += `${obj.name} ${obj.position}, ${obj.distance}`;\n                }\n            });\n            \n            this.speak(announcement, false, true); // Mark as object announcement for special delay handling\n            this.lastAnnouncement = now;\n        }\n    }\n\n    /**\n     * Legacy announcement method for backwards compatibility\n     */\n    announceDetections(predictions) {\n        // Redirect to smart announcement system\n        this.announceDetectionsSmart(predictions);\n    }\n    \n    /**\n     * Get relative position of object (left, center, right)\n     */\n    getRelativePosition(bbox) {\n        const [x, y, width, height] = bbox;\n        const centerX = x + width / 2;\n        const canvasCenter = this.canvas.width / 2;\n        const threshold = this.canvas.width * 0.25; // 25% threshold\n        \n        if (centerX < canvasCenter - threshold) {\n            return 'on your left';\n        } else if (centerX > canvasCenter + threshold) {\n            return 'on your right';\n        } else {\n            return 'ahead of you';\n        }\n    }\n\n    /**\n     * Estimate distance based on bounding box size (simplified)\n     */\n    estimateDistance(bbox) {\n        const [x, y, width, height] = bbox;\n        const area = width * height;\n        const videoArea = this.video.videoWidth * this.video.videoHeight;\n        const relativeSize = area / videoArea;\n        \n        if (relativeSize > 0.3) return 'very close';\n        if (relativeSize > 0.15) return '1 meter away';\n        if (relativeSize > 0.05) return '2 meters away';\n        return 'far away';\n    }\n\n    /**\n     * Process voice commands via Gemini API\n     */\n    async processVoiceCommand(command) {\n        console.log('Processing voice command:', command);\n        \n        try {\n            this.updateStatus('Processing your command...', 'primary');\n            \n            // Send command to Gemini API for processing\n            const response = await fetch('/api/process-command', {\n                method: 'POST',\n                headers: {\n                    'Content-Type': 'application/json',\n                },\n                body: JSON.stringify({\n                    command: command,\n                    language: this.currentLanguage,\n                    tone: this.currentTone\n                })\n            });\n            \n            if (!response.ok) {\n                throw new Error(`HTTP error! status: ${response.status}`);\n            }\n            \n            const result = await response.json();\n            console.log('Gemini response:', result);\n            \n            // Execute the action based on Gemini's response\n            await this.executeAction(result);\n            \n        } catch (error) {\n            console.error('Error processing command:', error);\n            \n            // Fallback to basic command processing\n            this.speak('Let me try to process that command locally.', true);\n            await this.fallbackCommandProcessing(command);\n        }\n    }\n    \n    /**\n     * Fallback command processing when Gemini is unavailable\n     */\n    async fallbackCommandProcessing(command) {\n        const cmd = command.toLowerCase();\n        \n        if (cmd.includes('start') && cmd.includes('detection')) {\n            this.speak('Starting object detection', true);\n            this.startDetection();\n        } else if (cmd.includes('stop')) {\n            this.speak('Stopping detection', true);\n            this.stopDetection();\n        } else if (cmd.includes('location') || cmd.includes('where am i')) {\n            this.speak('Enabling location services', true);\n            this.requestLocation();\n        } else if (cmd.includes('take me') || cmd.includes('navigate') || cmd.includes('go to')) {\n            // Extract destination\n            let destination = cmd.replace(/take me to|navigate to|go to/g, '').trim();\n            \n            // Handle common phrase variations\n            if (cmd.includes('take me to the')) {\n                destination = cmd.replace(/take me to the/g, '').trim();\n            }\n            \n            if (destination) {\n                console.log('Navigation command detected:', cmd, 'Destination:', destination);\n                this.speak(`Navigating to ${destination}`, true);\n                await this.navigateToLocation(destination);\n            } else {\n                this.speak('Where would you like to go? Please say the name of any place, landmark, or address.', true);\n            }\n        } else if (cmd.includes('preview') || cmd.includes('route to')) {\n            // Extract destination for route preview\n            let destination = cmd.replace(/preview route to|route to|preview/g, '').trim();\n            if (destination) {\n                console.log('Preview command detected:', cmd, 'Destination:', destination);\n                await this.previewRoute(destination);\n            } else {\n                this.speak('Which location would you like to preview?', true);\n            }\n        } else if (cmd.includes('stop navigation') || cmd.includes('cancel navigation')) {\n            this.stopNavigation();\n        } else if (cmd.includes('language') && cmd.includes('hindi')) {\n            this.changeLanguage('hi-IN');\n        } else if (cmd.includes('language') && cmd.includes('english')) {\n            this.changeLanguage('en-IN');\n        } else if (cmd.includes('tutorial') || cmd.includes('help') || cmd.includes('guide') || cmd.includes('learn')) {\n            this.speak('Starting BlindMate tutorial. This will help you learn all the features.', true);\n            setTimeout(() => {\n                window.location.href = '/tutorial';\n            }, 2000);\n        } else {\n            this.speak('I did not understand that command. Try saying start detection, stop, take me to a location, or start tutorial for help.', true);\n        }\n    }\n\n    /**\n     * Execute actions based on Gemini response\n     */\n    async executeAction(result) {\n        console.log('Executing action:', result);\n        \n        if (!result.action) {\n            this.speak('I could not understand that command.');\n            return;\n        }\n        \n        // Update action status\n        this.updateActionStatus(result.response || 'Processing command...', 'info');\n        \n        // Execute the requested action\n        switch (result.action) {\n            case 'start_detection':\n                if (!this.isDetecting) {\n                    await this.startDetection();\n                    this.updateStatus('Object detection started via voice command', 'success');\n                } else {\n                    this.speak('Detection is already running', true);\n                }\n                break;\n                \n            case 'stop_detection':\n            case 'stop':\n                if (this.isDetecting) {\n                    this.stopDetection();\n                    this.updateStatus('Object detection stopped via voice command', 'success');\n                } else {\n                    this.speak('Detection is not currently running', true);\n                }\n                break;\n                \n            case 'navigate':\n                console.log('Gemini navigate action:', result.destination);\n                if (result.destination) {\n                    await this.navigateToLocation(result.destination);\n                } else {\n                    this.speak('I need a destination to navigate to. Please say the name of any place, landmark, or address.', true);\n                }\n                break;\n                \n            case 'preview_route':\n                console.log('Gemini preview action:', result.destination);\n                if (result.destination) {\n                    await this.previewRoute(result.destination);\n                } else {\n                    this.speak('I need a destination to preview the route', true);\n                }\n                break;\n                \n            case 'stop_navigation':\n                console.log('Gemini stop navigation action');\n                this.stopNavigation();\n                break;\n                \n            case 'enable_location':\n                await this.requestLocation();\n                break;\n                \n            case 'change_language':\n                if (result.language) {\n                    this.changeLanguage(result.language);\n                } else {\n                    this.speak('Language not supported', true);\n                }\n                break;\n                \n            case 'change_tone':\n                if (result.tone) {\n                    this.changeTone(result.tone);\n                } else {\n                    this.speak('Tone not supported', true);\n                }\n                break;\n                \n            case 'get_location':\n                if (this.userLocation) {\n                    this.speak(`You are currently at latitude ${this.userLocation.latitude.toFixed(4)}, longitude ${this.userLocation.longitude.toFixed(4)}`, true);\n                } else {\n                    this.speak('Location not available. Please enable location services first.', true);\n                }\n                break;\n                \n            default:\n                console.log('Unknown action:', action);\n                if (!response) {\n                    this.speak('I understood your command but could not perform the action.', true);\n                }\n        }\n    }\n\n    /**\n     * Navigate to any worldwide destination using Google APIs\n     */\n    async navigateToLocation(destination) {\n        console.log('navigateToLocation called with:', destination);\n        \n        if (!this.userLocation) {\n            this.speak('Location access is required for navigation. Please enable location first.', true);\n            await this.requestLocation();\n            if (!this.userLocation) {\n                return;\n            }\n        }\n        \n        try {\n            this.updateStatus(`Getting directions to ${destination}...`, 'primary');\n            this.speak(`Getting directions to ${destination}`, true);\n            \n            console.log('User location:', this.userLocation);\n            console.log('Destination:', destination);\n            \n            // Use the enhanced navigation system that handles geocoding + directions\n            if (window.blindMateNavigation) {\n                // Use the enhanced navigation system\n                window.blindMateNavigation.currentDestination = destination;\n                await window.blindMateNavigation.startNavigation(destination);\n            } else {\n                // Fallback to direct API call\n                const response = await fetch('/api/directions', {\n                    method: 'POST',\n                    headers: { 'Content-Type': 'application/json' },\n                    body: JSON.stringify({\n                        origin: `${this.userLocation.latitude},${this.userLocation.longitude}`,\n                        destination: destination\n                    })\n                });\n                \n                const data = await response.json();\n                \n                if (data.success) {\n                    this.currentRoute = data;\n                    this.currentStepIndex = 0;\n                    this.isNavigating = true;\n                    \n                    // Update navigation status\n                    if (this.elements.navigationStatus) {\n                        this.elements.navigationStatus.textContent = 'Navigating';\n                        this.elements.navigationStatus.className = 'badge bg-success';\n                    }\n                    \n                    // Speak route overview\n                    await this.speakRouteOverview(data, destination);\n                    \n                    // Start position tracking for rerouting\n                    this.startLocationTracking();\n                    \n                    this.updateStatus(`Navigating to ${destination}`, 'success');\n                } else {\n                    this.speak(data.message || `Could not get directions to ${destination}. Please try again.`, true);\n                }\n            }\n            \n        } catch (error) {\n            console.error('Navigation error:', error);\n            this.speak(`Sorry, I couldn't get directions to ${destination}. Please try again.`, true);\n        }\n    }\n    \n    /**\n     * Preview route to any destination without starting navigation\n     */\n    async previewRoute(destination) {\n        console.log('previewRoute called with:', destination);\n        \n        if (!this.userLocation) {\n            this.speak('Location access is required for route preview. Please enable location first.', true);\n            await this.requestLocation();\n            if (!this.userLocation) {\n                return;\n            }\n        }\n        \n        try {\n            this.updateStatus(`Previewing route to ${destination}...`, 'primary');\n            \n            // Use the enhanced navigation system for route preview\n            const response = await fetch('/api/directions', {\n                method: 'POST',\n                headers: { 'Content-Type': 'application/json' },\n                body: JSON.stringify({\n                    origin: `${this.userLocation.latitude},${this.userLocation.longitude}`,\n                    destination: destination\n                })\n            });\n            \n            const data = await response.json();\n            \n            if (data.success) {\n                await this.speakRoutePreview(data, destination);\n                this.updateStatus(`Route preview completed for ${destination}`, 'success');\n            } else {\n                this.speak(data.message || `Could not get route preview to ${destination}. Please try again.`, true);\n            }\n            \n        } catch (error) {\n            console.error('Route preview error:', error);\n            this.speak(`Sorry, I couldn't preview the route to ${destination}`, true);\n        }\n    }\n    \n    /**\n     * Get directions from Google Maps API\n     */\n    async getDirections(originLat, originLng, destLat, destLng) {\n        try {\n            // Use backend proxy to avoid exposing API key\n            const response = await fetch('/api/directions', {\n                method: 'POST',\n                headers: {\n                    'Content-Type': 'application/json',\n                },\n                body: JSON.stringify({\n                    origin: `${originLat},${originLng}`,\n                    destination: `${destLat},${destLng}`,\n                    mode: 'walking'\n                })\n            });\n            \n            if (!response.ok) {\n                throw new Error(`HTTP error! status: ${response.status}`);\n            }\n            \n            const data = await response.json();\n            \n            if (data.status === 'OK' && data.routes && data.routes.length > 0) {\n                return data.routes[0];\n            } else {\n                throw new Error('No routes found');\n            }\n            \n        } catch (error) {\n            console.error('Directions API error:', error);\n            // Fallback: calculate straight-line distance and basic directions\n            return this.getFallbackDirections(originLat, originLng, destLat, destLng);\n        }\n    }\n    \n    /**\n     * Fallback directions when API is unavailable\n     */\n    getFallbackDirections(originLat, originLng, destLat, destLng) {\n        const distance = this.calculateDistance(originLat, originLng, destLat, destLng);\n        const bearing = this.calculateBearing(originLat, originLng, destLat, destLng);\n        const direction = this.getDirectionFromBearing(bearing);\n        \n        return {\n            legs: [{\n                distance: { text: `${Math.round(distance)} meters`, value: distance },\n                duration: { text: `${Math.round(distance / 1.4)} minutes`, value: Math.round(distance / 1.4) * 60 },\n                steps: [{\n                    distance: { text: `${Math.round(distance)} meters`, value: distance },\n                    duration: { text: `${Math.round(distance / 1.4)} minutes`, value: Math.round(distance / 1.4) * 60 },\n                    html_instructions: `Walk ${direction} for ${Math.round(distance)} meters`,\n                    start_location: { lat: originLat, lng: originLng },\n                    end_location: { lat: destLat, lng: destLng }\n                }]\n            }]\n        };\n    }\n    \n    /**\n     * Speak route overview when starting navigation\n     */\n    async speakRouteOverview(route, destinationName) {\n        const leg = route.legs[0];\n        const totalDistance = leg.distance.text;\n        const totalTime = leg.duration.text;\n        \n        this.speak(`You are ${totalDistance} from ${destinationName}. Estimated walking time: ${totalTime}`, true);\n        \n        // Speak first 2-3 steps\n        const steps = leg.steps.slice(0, 3);\n        for (let i = 0; i < steps.length; i++) {\n            const step = steps[i];\n            const instruction = this.cleanHtmlInstructions(step.html_instructions);\n            \n            setTimeout(() => {\n                this.speak(`Step ${i + 1}: ${instruction}`, true);\n            }, (i + 1) * 3000);\n        }\n    }\n    \n    /**\n     * Speak route preview (first few steps only)\n     */\n    async speakRoutePreview(route, destinationName) {\n        const leg = route.legs[0];\n        const totalDistance = leg.distance.text;\n        const totalTime = leg.duration.text;\n        \n        this.speak(`Route preview to ${destinationName}: ${totalDistance}, about ${totalTime} walking`, true);\n        \n        setTimeout(() => {\n            if (leg.steps.length > 0) {\n                const firstStep = this.cleanHtmlInstructions(leg.steps[0].html_instructions);\n                this.speak(`First step: ${firstStep}`, true);\n            }\n        }, 2000);\n        \n        if (leg.steps.length > 1) {\n            setTimeout(() => {\n                const secondStep = this.cleanHtmlInstructions(leg.steps[1].html_instructions);\n                this.speak(`Then: ${secondStep}`, true);\n            }, 4000);\n        }\n    }\n    \n    /**\n     * Clean HTML instructions from Google Maps API\n     */\n    cleanHtmlInstructions(htmlInstructions) {\n        return htmlInstructions\n            .replace(/<[^>]*>/g, '') // Remove HTML tags\n            .replace(/&nbsp;/g, ' ') // Replace non-breaking spaces\n            .replace(/&amp;/g, '&') // Replace HTML entities\n            .trim();\n    }\n    \n    /**\n     * Start location tracking for rerouting\n     */\n    startLocationTracking() {\n        if (!navigator.geolocation) {\n            console.warn('Geolocation not supported for tracking');\n            return;\n        }\n        \n        // Watch position every 5 seconds\n        this.locationWatcher = navigator.geolocation.watchPosition(\n            (position) => {\n                this.checkRouteDeviation(position.coords.latitude, position.coords.longitude);\n            },\n            (error) => {\n                console.error('Location tracking error:', error);\n            },\n            {\n                enableHighAccuracy: true,\n                timeout: 10000,\n                maximumAge: 5000\n            }\n        );\n    }\n    \n    /**\n     * Check if user has deviated from the route\n     */\n    checkRouteDeviation(currentLat, currentLng) {\n        if (!this.isNavigating || !this.currentRoute) return;\n        \n        const currentStep = this.currentRoute.legs[0].steps[this.currentStepIndex];\n        if (!currentStep) return;\n        \n        // Calculate distance to expected route point\n        const expectedLat = currentStep.start_location.lat;\n        const expectedLng = currentStep.start_location.lng;\n        const deviation = this.calculateDistance(currentLat, currentLng, expectedLat, expectedLng);\n        \n        // If user is too far off track, reroute\n        if (deviation > this.routeDeviationThreshold) {\n            this.speak('You have moved off the path. Recalculating route...', true);\n            this.reroute(currentLat, currentLng);\n        }\n    }\n    \n    /**\n     * Reroute from current position\n     */\n    async reroute(currentLat, currentLng) {\n        if (!this.isNavigating) return;\n        \n        // Find the destination from current route\n        const originalDestination = this.currentRoute.legs[0].end_location;\n        \n        try {\n            const newRoute = await this.getDirections(\n                currentLat, currentLng,\n                originalDestination.lat, originalDestination.lng\n            );\n            \n            if (newRoute) {\n                this.currentRoute = newRoute;\n                this.currentStepIndex = 0;\n                \n                const leg = newRoute.legs[0];\n                this.speak(`New route calculated. ${leg.distance.text} remaining.`, true);\n                \n                // Speak next instruction\n                if (leg.steps.length > 0) {\n                    setTimeout(() => {\n                        const instruction = this.cleanHtmlInstructions(leg.steps[0].html_instructions);\n                        this.speak(instruction, true);\n                    }, 2000);\n                }\n            }\n        } catch (error) {\n            console.error('Rerouting error:', error);\n            this.speak('Could not recalculate route. Please use your navigation app.', true);\n        }\n    }\n    \n    /**\n     * Stop navigation and location tracking\n     */\n    stopNavigation() {\n        this.isNavigating = false;\n        this.currentRoute = null;\n        this.currentStepIndex = 0;\n        \n        if (this.locationWatcher) {\n            navigator.geolocation.clearWatch(this.locationWatcher);\n            this.locationWatcher = null;\n        }\n        \n        this.speak('Navigation stopped', true);\n        this.updateStatus('Navigation stopped', 'info');\n        \n        // Update navigation status\n        this.elements.navigationStatus.textContent = 'Ready';\n        this.elements.navigationStatus.className = 'badge bg-secondary';\n    }\n    \n    /**\n     * Calculate distance between two points in meters\n     */\n    calculateDistance(lat1, lng1, lat2, lng2) {\n        const R = 6371e3; // Earth's radius in meters\n        const œÜ1 = lat1 * Math.PI / 180;\n        const œÜ2 = lat2 * Math.PI / 180;\n        const ŒîœÜ = (lat2 - lat1) * Math.PI / 180;\n        const ŒîŒª = (lng2 - lng1) * Math.PI / 180;\n        \n        const a = Math.sin(ŒîœÜ/2) * Math.sin(ŒîœÜ/2) +\n                Math.cos(œÜ1) * Math.cos(œÜ2) *\n                Math.sin(ŒîŒª/2) * Math.sin(ŒîŒª/2);\n        const c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1-a));\n        \n        return R * c;\n    }\n    \n    /**\n     * Calculate bearing between two points\n     */\n    calculateBearing(lat1, lng1, lat2, lng2) {\n        const œÜ1 = lat1 * Math.PI / 180;\n        const œÜ2 = lat2 * Math.PI / 180;\n        const ŒîŒª = (lng2 - lng1) * Math.PI / 180;\n        \n        const y = Math.sin(ŒîŒª) * Math.cos(œÜ2);\n        const x = Math.cos(œÜ1) * Math.sin(œÜ2) - Math.sin(œÜ1) * Math.cos(œÜ2) * Math.cos(ŒîŒª);\n        \n        const Œ∏ = Math.atan2(y, x);\n        return (Œ∏ * 180 / Math.PI + 360) % 360;\n    }\n    \n    /**\n     * Get direction name from bearing\n     */\n    getDirectionFromBearing(bearing) {\n        const directions = ['north', 'northeast', 'east', 'southeast', 'south', 'southwest', 'west', 'northwest'];\n        const index = Math.round(bearing / 45) % 8;\n        return directions[index];\n    }\n\n    /**\n     * Provide basic navigation assistance\n     */\n    provideNavigationGuidance(destination) {\n        const guidance = [\n            `I'm helping you navigate to ${destination}.`,\n            \"Since I opened navigation in your maps app, please follow the turn-by-turn directions there.\",\n            \"You can still use voice commands with me:\",\n            \"Say 'start detection' to scan for obstacles while walking.\",\n            \"Say 'stop' to pause any features.\",\n            \"Stay safe and be aware of your surroundings.\"\n        ];\n        \n        guidance.forEach((message, index) => {\n            setTimeout(() => this.speak(message), index * 3000);\n        });\n    }\n\n    /**\n     * Request user location\n     */\n    async requestLocation() {\n        try {\n            this.updateStatus('Requesting location access...', 'warning');\n            \n            const position = await new Promise((resolve, reject) => {\n                navigator.geolocation.getCurrentPosition(resolve, reject, {\n                    enableHighAccuracy: true,\n                    timeout: 10000,\n                    maximumAge: 60000\n                });\n            });\n            \n            this.userLocation = {\n                latitude: position.coords.latitude,\n                longitude: position.coords.longitude\n            };\n            \n            this.updateStatus('Location access granted.', 'success');\n            this.elements.locationBtn.className = 'btn btn-success btn-lg';\n            this.elements.locationBtn.innerHTML = '<i class=\"fas fa-check\" aria-hidden=\"true\"></i> Location Enabled';\n            \n            this.speak('Location access granted. I can now provide navigation assistance.');\n            \n        } catch (error) {\n            console.error('Location error:', error);\n            this.updateStatus('Location access denied.', 'danger');\n            this.speak('Location access is required for navigation features. Please enable location in your browser settings.');\n        }\n    }\n\n    /**\n     * Change application language\n     */\n    changeLanguage(langCode) {\n        console.log('Changing language to:', langCode);\n        \n        this.currentLanguage = langCode;\n        this.elements.languageSelect.value = langCode;\n        \n        // Update recognition language\n        if (this.commandRecognition) {\n            this.commandRecognition.lang = langCode;\n        }\n        if (this.continuousRecognition) {\n            this.continuousRecognition.lang = langCode;\n        }\n        \n        // Update language preference on server\n        this.updateServerPreferences();\n        \n        this.speak(`Language changed to ${this.getLanguageName(langCode)}`);\n    }\n\n    /**\n     * Change voice tone\n     */\n    changeTone(tone) {\n        console.log('Changing tone to:', tone);\n        \n        this.currentTone = tone;\n        this.elements.toneSelect.value = tone;\n        \n        // Update tone preference on server\n        this.updateServerPreferences();\n        \n        // Speak confirmation with new tone\n        this.speak(`Voice tone changed to ${tone}`, true);\n    }\n\n    /**\n     * Update preferences on server\n     */\n    async updateServerPreferences() {\n        try {\n            await fetch('/api/preferences', {\n                method: 'POST',\n                headers: { 'Content-Type': 'application/json' },\n                body: JSON.stringify({\n                    language: this.currentLanguage,\n                    tone: this.currentTone\n                })\n            });\n        } catch (error) {\n            console.error('Error updating preferences:', error);\n        }\n    }\n\n    /**\n     * Load preferences from server\n     */\n    async loadServerPreferences() {\n        try {\n            const response = await fetch('/api/preferences');\n            const preferences = await response.json();\n            \n            if (preferences.language) {\n                this.currentLanguage = preferences.language;\n                this.elements.languageSelect.value = preferences.language;\n            }\n            \n            if (preferences.tone) {\n                this.currentTone = preferences.tone;\n                this.elements.toneSelect.value = preferences.tone;\n            }\n        } catch (error) {\n            console.error('Error loading preferences:', error);\n        }\n    }\n\n    /**\n     * Get display name for language code\n     */\n    getLanguageName(langCode) {\n        const languageNames = {\n            'en-IN': 'English',\n            'hi-IN': 'Hindi',\n            'ta-IN': 'Tamil',\n            'te-IN': 'Telugu',\n            'bn-IN': 'Bengali',\n            'mr-IN': 'Marathi',\n            'gu-IN': 'Gujarati',\n            'es-ES': 'Spanish',\n            'fr-FR': 'French',\n            'de-DE': 'German',\n            'it-IT': 'Italian',\n            'pt-PT': 'Portuguese',\n            'ja-JP': 'Japanese',\n            'zh-CN': 'Chinese',\n            'ar-SA': 'Arabic'\n        };\n        return languageNames[langCode] || langCode;\n    }\n\n    /**\n     * Get tone-specific voice settings\n     */\n    getToneSettings(tone) {\n        const toneSettings = {\n            'friendly': { rate: 0.9, pitch: 1.1, volume: 0.8 },\n            'formal': { rate: 0.7, pitch: 0.9, volume: 0.8 },\n            'energetic': { rate: 1.1, pitch: 1.2, volume: 0.9 },\n            'calm': { rate: 0.6, pitch: 0.8, volume: 0.7 },\n            'robotic': { rate: 0.8, pitch: 0.7, volume: 0.8 }\n        };\n        return toneSettings[tone] || toneSettings['friendly'];\n    }\n\n    /**\n     * Find appropriate voice for tone\n     */\n    findVoiceForTone(voices, language, tone) {\n        // Try to find voices that match tone characteristics\n        const langVoices = voices.filter(v => v.lang === language || v.lang.startsWith(language.split('-')[0]));\n        \n        if (langVoices.length === 0) return null;\n        \n        // Different tone preferences for voice selection\n        switch (tone) {\n            case 'formal':\n                return langVoices.find(v => v.name.toLowerCase().includes('professional') || \n                                          v.name.toLowerCase().includes('formal')) || langVoices[0];\n            case 'energetic':\n                return langVoices.find(v => v.name.toLowerCase().includes('young') || \n                                          v.name.toLowerCase().includes('bright')) || langVoices[0];\n            case 'calm':\n                return langVoices.find(v => v.name.toLowerCase().includes('calm') || \n                                          v.name.toLowerCase().includes('soft')) || langVoices[0];\n            case 'robotic':\n                return langVoices.find(v => v.name.toLowerCase().includes('robotic') || \n                                          v.name.toLowerCase().includes('computer')) || langVoices[0];\n            default:\n                return langVoices[0];\n        }\n    }\n\n    /**\n     * Text-to-speech function with queue management and cooldown\n     */\n    speak(text, priority = false, isObjectAnnouncement = false) {\n        if (!this.synth || !text) {\n            return;\n        }\n\n        const now = Date.now();\n        \n        // Handle object announcements with special delay logic\n        if (isObjectAnnouncement && !priority) {\n            this._handleObjectAnnouncement(text, now);\n            return;\n        }\n        \n        // If high priority or enough time has passed since last speech\n        if (priority || (now - this.lastSpeechTime > this.speechCooldown && !this.isSpeaking)) {\n            this._speakNow(text);\n        } else if (!priority) {\n            // Add to queue for non-priority speech\n            this.speechQueue.push(text);\n            if (!this.isSpeaking) {\n                this._processNextSpeech();\n            }\n        }\n    }\n    \n    /**\n     * Handle object announcements with special delay logic\n     */\n    _handleObjectAnnouncement(text, now) {\n        // Cancel any pending announcement\n        if (this.speechDelayTimer) {\n            clearTimeout(this.speechDelayTimer);\n            this.speechDelayTimer = null;\n        }\n        \n        // Store the pending announcement\n        this.pendingAnnouncement = text;\n        \n        // Calculate delay needed\n        const timeSinceLastSpeech = now - this.lastSpeechTime;\n        const minimumDelay = this.minObjectAnnouncementDelay;\n        \n        if (this.isSpeaking || timeSinceLastSpeech < minimumDelay) {\n            // Need to delay announcement\n            const delayNeeded = this.isSpeaking ? \n                minimumDelay : // Wait full delay if currently speaking\n                minimumDelay - timeSinceLastSpeech; // Wait remaining time\n                \n            this.isAnnouncementDelayed = true;\n            \n            console.log(`Object announcement delayed by ${delayNeeded}ms for clarity`);\n            \n            this.speechDelayTimer = setTimeout(() => {\n                if (this.pendingAnnouncement) {\n                    this._speakNow(this.pendingAnnouncement);\n                    this.pendingAnnouncement = null;\n                    this.isAnnouncementDelayed = false;\n                }\n            }, delayNeeded);\n        } else {\n            // Can announce immediately\n            this._speakNow(text);\n            this.pendingAnnouncement = null;\n        }\n    }\n\n    /**\n     * Internal function to speak immediately\n     */\n    _speakNow(text) {\n        try {\n            // Cancel any ongoing speech immediately to prevent overlaps\n            this.synth.cancel();\n            \n            // Small delay to ensure cancellation is processed\n            setTimeout(() => {\n                this.isSpeaking = true;\n                this.lastSpeechTime = Date.now();\n                \n                const utterance = new SpeechSynthesisUtterance(text);\n                utterance.lang = this.currentLanguage;\n                \n                // Apply tone-specific voice settings\n                const toneSettings = this.getToneSettings(this.currentTone);\n                utterance.rate = toneSettings.rate;\n                utterance.pitch = toneSettings.pitch;\n                utterance.volume = toneSettings.volume;\n                \n                // Find appropriate voice based on language and tone\n                const voices = this.synth.getVoices();\n                if (voices.length > 0) {\n                    let voice = this.findVoiceForTone(voices, this.currentLanguage, this.currentTone);\n                    \n                    if (!voice) {\n                        voice = voices.find(v => v.lang === this.currentLanguage) || \n                               voices.find(v => v.lang.startsWith(this.currentLanguage.split('-')[0])) ||\n                               voices.find(v => v.default);\n                    }\n                    \n                    if (voice) {\n                        utterance.voice = voice;\n                    }\n                }\n                \n                utterance.onstart = () => {\n                    console.log('Speech started successfully');\n                };\n                \n                utterance.onend = () => {\n                    console.log('Speech ended normally');\n                    this.isSpeaking = false;\n                    // Longer delay before next speech for better clarity\n                    setTimeout(() => this._processNextSpeech(), 750);\n                };\n                \n                utterance.onerror = (event) => {\n                    console.warn('Speech error:', event);\n                    this.isSpeaking = false;\n                    setTimeout(() => this._processNextSpeech(), 750);\n                };\n                \n                this.synth.speak(utterance);\n                \n            }, 50); // Small delay to ensure proper cancellation\n            \n        } catch (error) {\n            this.isSpeaking = false;\n            console.warn('Speech synthesis error:', error);\n        }\n    }\n    \n    /**\n     * Process next item in speech queue\n     */\n    _processNextSpeech() {\n        if (this.speechQueue.length > 0 && !this.isSpeaking) {\n            const text = this.speechQueue.shift();\n            this._speakNow(text);\n        }\n    }\n\n    /**\n     * Check if this is a first-time user and offer tutorial\n     */\n    checkFirstTimeUser() {\n        const hasCompletedTutorial = localStorage.getItem('blindmate_tutorial_completed');\n        const hasUsedApp = localStorage.getItem('blindmate_first_use');\n        \n        if (!hasCompletedTutorial && !hasUsedApp) {\n            // Mark that the user has seen the app\n            localStorage.setItem('blindmate_first_use', 'true');\n            \n            // Wait a moment for the interface to load, then offer tutorial\n            setTimeout(() => {\n                this.speak('Welcome to BlindMate! This is your first time using the app. Would you like to start with a guided tutorial to learn all the features? You can also access the tutorial anytime by saying \"start tutorial\" or clicking the tutorial button.');\n                \n                // Show tutorial button prominently\n                const tutorialButton = document.getElementById('tutorialButton');\n                if (tutorialButton) {\n                    tutorialButton.classList.add('btn-warning');\n                    tutorialButton.innerHTML = '<i class=\"fas fa-graduation-cap\"></i> Recommended: Start Tutorial';\n                }\n            }, 2000);\n        }\n    }\n\n    /**\n     * Update system status display\n     */\n    updateStatus(message, type = 'info') {\n        if (this.elements && this.elements.systemStatus) {\n            this.elements.systemStatus.textContent = message;\n            this.elements.systemStatus.className = `alert alert-${type}`;\n            \n            // Auto-clear success and warning messages\n            if (type === 'success' || type === 'warning') {\n                setTimeout(() => {\n                    if (this.elements.systemStatus && this.elements.systemStatus.textContent === message) {\n                        this.updateStatus('System ready', 'info');\n                    }\n                }, 5000);\n            }\n        } else {\n            console.log('Status update:', message, type);\n        }\n    }\n}\n\n// Initialize the application when the page loads\ndocument.addEventListener('DOMContentLoaded', () => {\n    window.blindMate = new BlindMate();\n});\n\n// Handle page visibility changes to pause/resume detection\ndocument.addEventListener('visibilitychange', () => {\n    if (window.blindMate) {\n        if (document.hidden && window.blindMate.isDetecting) {\n            // Pause detection when page is hidden\n            window.blindMate.isDetecting = false;\n        } else if (!document.hidden && window.blindMate.stream) {\n            // Resume detection when page becomes visible\n            window.blindMate.isDetecting = true;\n            window.blindMate.detectObjects();\n        }\n    }\n});\n","size_bytes":99546},"app.py":{"content":"import os\nimport logging\nimport requests\nimport re\nfrom flask import Flask, request, jsonify, render_template, session, send_from_directory\nfrom flask_cors import CORS\nfrom gemini_service import GeminiService\n\n# Configure logging with UTF-8 encoding\nlogging.basicConfig(\n    level=logging.DEBUG,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.StreamHandler()\n    ]\n)\n\n# Create Flask app with proper template and static folders\napp = Flask(__name__, \n            template_folder='templates',\n            static_folder='static')\napp.secret_key = os.environ.get(\"SESSION_SECRET\", \"blindmate-secret-key-2024\")\n\n# Enable CORS for frontend communication\nCORS(app, origins=[\"*\"])\n\n# Initialize Gemini service\ngemini_service = GeminiService()\n\n@app.route('/')\ndef index():\n    \"\"\"Serve the main application page\"\"\"\n    try:\n        return render_template('index.html')\n    except Exception as e:\n        logging.error(f\"Error serving index.html: {e}\")\n        return \"Application files not found\", 404\n\n# Static files are now served automatically by Flask from /static folder\n\n@app.route('/tutorial')\ndef tutorial():\n    \"\"\"Serve the onboarding tutorial page\"\"\"\n    try:\n        return render_template('onboarding.html')\n    except Exception as e:\n        logging.error(f\"Error serving onboarding.html: {e}\")\n        return \"Tutorial not found\", 404\n\n@app.route('/navigation')\ndef navigation():\n    \"\"\"Serve navigation page\"\"\"\n    try:\n        return render_template('navigation.html')\n    except Exception as e:\n        logging.error(f\"Error serving navigation.html: {e}\")\n        return \"Navigation page not found\", 404\n\n@app.route('/simple-navigation')\ndef simple_navigation():\n    \"\"\"Serve simple navigation page\"\"\"\n    try:\n        return render_template('simple_navigation.html')\n    except Exception as e:\n        logging.error(f\"Error serving simple_navigation.html: {e}\")\n        return \"Simple navigation page not found\", 404\n\n# JavaScript files are now served automatically by Flask from /static folder\n\n@app.route('/api/process-command', methods=['POST'])\ndef process_command():\n    \"\"\"Process voice commands using Gemini API\"\"\"\n    try:\n        data = request.get_json()\n        \n        if not data or 'command' not in data:\n            return jsonify({'error': 'Missing command in request'}), 400\n        \n        command = data['command']\n        language = data.get('language', session.get('current_language', 'en-IN'))\n        tone = data.get('tone', session.get('current_tone', 'friendly'))\n        \n        logging.info(f\"Processing command: {command} in language: {language} with tone: {tone}\")\n        \n        # Check for language/tone change commands\n        result = gemini_service.process_voice_command(command, language, tone)\n        \n        # Update session if language or tone changed\n        if result.get('action') == 'change_language' and result.get('language'):\n            session['current_language'] = result['language']\n            logging.info(f\"Language changed to: {result['language']}\")\n        \n        if result.get('action') == 'change_tone' and result.get('tone'):\n            session['current_tone'] = result['tone']\n            logging.info(f\"Tone changed to: {result['tone']}\")\n        \n        # Add current session preferences to response\n        result['current_language'] = session.get('current_language', 'en-IN')\n        result['current_tone'] = session.get('current_tone', 'friendly')\n        \n        return jsonify(result)\n        \n    except Exception as e:\n        logging.error(f\"Error processing command: {e}\")\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/api/directions', methods=['POST'])\ndef get_directions():\n    \"\"\"Get walking directions using Google Directions API and Google Geocoding API\"\"\"\n    try:\n        data = request.get_json()\n        \n        if not data or 'origin' not in data or 'destination' not in data:\n            return jsonify({'success': False, 'message': 'Missing origin or destination'}), 400\n        \n        origin = data['origin']  # Expected format: \"lat,lng\"\n        destination = data['destination']  # Can be address text or \"lat,lng\"\n        \n        # Get Google API key from environment\n        api_key = os.environ.get('GOOGLE_MAPS_API_KEY')\n        if not api_key:\n            logging.error(\"GOOGLE_MAPS_API_KEY not found in environment variables\")\n            return jsonify({'success': False, 'message': 'Navigation service not configured'}), 500\n        \n        # Validate origin coordinates format\n        coord_pattern = r'^-?\\d+\\.?\\d*,-?\\d+\\.?\\d*$'\n        if not re.match(coord_pattern, origin):\n            return jsonify({'success': False, 'message': 'Invalid origin coordinates format'}), 400\n        \n        # Parse origin coordinates\n        try:\n            origin_lat, origin_lng = map(float, origin.split(','))\n            if not (-90 <= origin_lat <= 90) or not (-180 <= origin_lng <= 180):\n                return jsonify({'success': False, 'message': 'Origin coordinates out of range'}), 400\n        except ValueError:\n            return jsonify({'success': False, 'message': 'Invalid origin coordinates'}), 400\n        \n        # Check if destination is coordinates or address text\n        destination_coords = destination\n        if not re.match(coord_pattern, destination):\n            # Destination is text address - try to geocode it first\n            logging.info(f\"Geocoding destination: {destination}\")\n            \n            # Try standard geocoding first\n            geocoded_coords = geocode_address(destination, api_key)\n            \n            # If geocoding fails and it's a generic term, try Places API search\n            if not geocoded_coords or 'error' in geocoded_coords:\n                logging.info(f\"Standard geocoding failed, trying Places API for: {destination}\")\n                places_result = find_nearby_place(destination, origin, api_key)\n                if places_result and 'lat' in places_result:\n                    geocoded_coords = places_result\n                    logging.info(f\"Found place via Places API: {destination}\")\n                else:\n                    return jsonify({'success': False, 'message': f'Could not find \"{destination}\" near your location. Please try a more specific address.'}), 404\n            \n            if 'error' in geocoded_coords:\n                return jsonify({'success': False, 'message': geocoded_coords['message']}), 404\n            destination_coords = f\"{geocoded_coords['lat']},{geocoded_coords['lng']}\"\n            logging.info(f\"Geocoded '{destination}' to {destination_coords}\")\n        \n        # Get walking directions from Google Directions API\n        directions_data = get_google_directions(origin, destination_coords, api_key)\n        \n        if not directions_data:\n            return jsonify({'success': False, 'message': 'Route not available'}), 404\n        elif 'error' in directions_data:\n            return jsonify({'success': False, 'message': directions_data['message']}), 404\n        \n        # Parse and format the directions for voice navigation\n        try:\n            navigation_data = parse_google_directions(directions_data, destination)\n            return jsonify(navigation_data)\n        except Exception as parse_error:\n            logging.error(f\"Error parsing Google directions: {parse_error}\")\n            return jsonify({'success': False, 'message': 'Failed to parse navigation data'}), 500\n        \n    except requests.exceptions.Timeout:\n        logging.error(\"Google API timeout\")\n        return jsonify({'success': False, 'message': 'Navigation service timeout'}), 504\n    except requests.exceptions.RequestException as e:\n        logging.error(f\"HTTP error getting directions: {e}\")\n        return jsonify({'success': False, 'message': 'Navigation service unavailable'}), 503\n    except Exception as e:\n        logging.error(f\"Error getting directions: {e}\")\n        return jsonify({'success': False, 'message': 'Navigation service error'}), 500\n\n@app.route('/api/google-maps-key', methods=['GET'])\ndef get_google_maps_key():\n    \"\"\"Get Google Maps API key for frontend\"\"\"\n    api_key = os.environ.get('GOOGLE_MAPS_API_KEY')\n    if not api_key:\n        return jsonify({'error': 'Google Maps API key not configured'}), 500\n    return jsonify({'key': api_key})\n\n@app.route('/api/preferences', methods=['GET', 'POST'])\ndef preferences():\n    \"\"\"Get or set user language and tone preferences\"\"\"\n    if request.method == 'GET':\n        return jsonify({\n            'language': session.get('current_language', 'en-IN'),\n            'tone': session.get('current_tone', 'friendly')\n        })\n    \n    elif request.method == 'POST':\n        data = request.get_json()\n        \n        if 'language' in data:\n            session['current_language'] = data['language']\n            logging.info(f\"Language preference updated to: {data['language']}\")\n        \n        if 'tone' in data:\n            session['current_tone'] = data['tone']\n            logging.info(f\"Tone preference updated to: {data['tone']}\")\n        \n        return jsonify({\n            'success': True,\n            'language': session.get('current_language'),\n            'tone': session.get('current_tone')\n        })\n\ndef enhance_search_terms(address):\n    \"\"\"Enhance generic search terms for better geocoding results\"\"\"\n    address_lower = address.lower().strip()\n    \n    # Add \"near me\" to generic terms to get local results\n    generic_terms = {\n        'library': 'library near me',\n        'hospital': 'hospital near me',\n        'school': 'school near me',\n        'restaurant': 'restaurant near me',\n        'pharmacy': 'pharmacy near me',\n        'bank': 'bank near me',\n        'grocery store': 'grocery store near me',\n        'gas station': 'gas station near me',\n        'shopping mall': 'shopping mall near me',\n        'park': 'park near me',\n        'gym': 'gym near me',\n        'university': 'university near me',\n        'college': 'college near me',\n        'airport': 'airport near me',\n        'train station': 'train station near me',\n        'bus station': 'bus station near me',\n        'hotel': 'hotel near me',\n        'cinema': 'cinema near me',\n        'movie theater': 'movie theater near me',\n        'coffee shop': 'coffee shop near me',\n        'post office': 'post office near me'\n    }\n    \n    # Check if it's a generic term\n    for term, enhanced in generic_terms.items():\n        if address_lower == term or address_lower == term + 's':\n            return enhanced\n    \n    # If it's already a specific address, return as is\n    return address\n\ndef find_nearby_place(place_type, origin_coords, api_key):\n    \"\"\"Find nearby places using Google Places API\"\"\"\n    try:\n        # Extract coordinates from origin\n        origin_lat, origin_lng = map(float, origin_coords.split(','))\n        \n        # Map common terms to Google Places types\n        place_type_mapping = {\n            'library': 'library',\n            'hospital': 'hospital', \n            'school': 'school',\n            'restaurant': 'restaurant',\n            'pharmacy': 'pharmacy',\n            'bank': 'bank',\n            'grocery store': 'grocery_or_supermarket',\n            'gas station': 'gas_station',\n            'shopping mall': 'shopping_mall',\n            'park': 'park',\n            'gym': 'gym',\n            'university': 'university',\n            'college': 'university',\n            'airport': 'airport',\n            'train station': 'train_station',\n            'bus station': 'bus_station',\n            'hotel': 'lodging',\n            'cinema': 'movie_theater',\n            'movie theater': 'movie_theater',\n            'coffee shop': 'cafe',\n            'post office': 'post_office'\n        }\n        \n        # Get the Places API type\n        search_type = place_type_mapping.get(place_type.lower(), place_type.lower())\n        \n        # Use Places API Nearby Search\n        url = 'https://maps.googleapis.com/maps/api/place/nearbysearch/json'\n        params = {\n            'location': f\"{origin_lat},{origin_lng}\",\n            'radius': 5000,  # 5km radius\n            'type': search_type,\n            'key': api_key\n        }\n        \n        logging.info(f\"Searching for nearby {search_type} at {origin_lat},{origin_lng}\")\n        response = requests.get(url, params=params, timeout=30)\n        response.raise_for_status()\n        \n        data = response.json()\n        status = data.get('status')\n        \n        if status == 'OK' and data.get('results'):\n            # Get the first (closest) result\n            place = data['results'][0]\n            location = place['geometry']['location']\n            place_name = place.get('name', place_type)\n            \n            logging.info(f\"Found nearby {place_type}: {place_name}\")\n            return {\n                'lat': location['lat'],\n                'lng': location['lng'],\n                'name': place_name\n            }\n        else:\n            logging.warning(f\"No nearby {place_type} found via Places API\")\n            return None\n            \n    except Exception as e:\n        logging.error(f\"Error finding nearby place: {e}\")\n        return None\n\ndef geocode_address(address, api_key):\n    \"\"\"Geocode address using Google Geocoding API with enhanced search\"\"\"\n    try:\n        # Enhance generic search terms for better geocoding results\n        enhanced_address = enhance_search_terms(address)\n        \n        url = 'https://maps.googleapis.com/maps/api/geocode/json'\n        params = {\n            'address': enhanced_address,\n            'key': api_key\n        }\n        \n        logging.info(f\"Geocoding address: {address} (enhanced: {enhanced_address})\")\n        response = requests.get(url, params=params, timeout=30)\n        response.raise_for_status()\n        \n        try:\n            data = response.json()\n        except Exception as e:\n            logging.error(f\"Failed to parse geocoding response: {e}\")\n            return None\n        \n        # Check for Google API errors with specific handling\n        status = data.get('status')\n        if status != 'OK':\n            if status == 'ZERO_RESULTS':\n                logging.warning(f\"No results found for address: {address}\")\n                return {'error': 'ZERO_RESULTS', 'message': 'Location not found, please try again.'}\n            elif status == 'REQUEST_DENIED':\n                logging.error(f\"Google API request denied for address: {address}\")\n                return {'error': 'REQUEST_DENIED', 'message': 'Navigation service unavailable'}\n            else:\n                logging.error(f\"Google Geocoding API error: {status}\")\n                return {'error': status, 'message': 'Unable to find location'}\n        \n        # Extract coordinates\n        if data.get('results') and len(data['results']) > 0:\n            location = data['results'][0]['geometry']['location']\n            return {\n                'lat': location['lat'],\n                'lng': location['lng']\n            }\n        \n        return None\n        \n    except requests.exceptions.Timeout:\n        logging.error(\"Google Geocoding API timeout\")\n        return None\n    except requests.exceptions.RequestException as e:\n        logging.error(f\"HTTP error geocoding address: {e}\")\n        return None\n    except Exception as e:\n        logging.error(f\"Geocoding error: {e}\")\n        return None\n\ndef get_google_directions(origin, destination, api_key):\n    \"\"\"Get walking directions from Google Directions API\"\"\"\n    try:\n        url = 'https://maps.googleapis.com/maps/api/directions/json'\n        params = {\n            'origin': origin,\n            'destination': destination,\n            'mode': 'walking',\n            'units': 'metric',\n            'language': 'en',\n            'key': api_key\n        }\n        \n        logging.info(f\"Getting Google directions from {origin} to {destination}\")\n        response = requests.get(url, params=params, timeout=30)\n        response.raise_for_status()\n        \n        # Parse response safely\n        try:\n            data = response.json()\n        except Exception as e:\n            logging.error(f\"Failed to parse Google response as JSON: {e}\")\n            logging.error(f\"Response content: {response.text[:500]}\")\n            return None\n        \n        # Check for Google API errors with specific handling\n        status = data.get('status')\n        if status != 'OK':\n            if status == 'ZERO_RESULTS':\n                logging.warning(f\"No route found from {origin} to {destination}\")\n                return {'error': 'ZERO_RESULTS', 'message': 'Route not available'}\n            elif status == 'NOT_FOUND':\n                logging.warning(f\"Location not found for directions: {origin} to {destination}\")\n                return {'error': 'NOT_FOUND', 'message': 'Location not found, please try again.'}\n            elif status == 'REQUEST_DENIED':\n                logging.error(f\"Google Directions API request denied\")\n                return {'error': 'REQUEST_DENIED', 'message': 'Navigation service unavailable'}\n            else:\n                error_msg = data.get('error_message', 'Unknown error')\n                logging.error(f\"Google Directions API error: {status} - {error_msg}\")\n                return {'error': status, 'message': 'Route not available'}\n        \n        # Validate Google response structure\n        if not data.get('routes') or len(data['routes']) == 0:\n            logging.error(\"Google returned no routes\")\n            return None\n            \n        route = data['routes'][0]\n        if not route.get('legs') or len(route['legs']) == 0:\n            logging.error(\"Google route has no legs\")\n            return None\n            \n        return data\n        \n    except requests.exceptions.Timeout:\n        logging.error(\"Google Directions API timeout\")\n        return None\n    except requests.exceptions.RequestException as e:\n        logging.error(f\"HTTP error getting Google directions: {e}\")\n        return None\n    except Exception as e:\n        logging.error(f\"Google directions error: {e}\")\n        return None\n\ndef parse_google_directions(directions_data, destination_name):\n    \"\"\"Parse Google Directions API response for voice navigation\"\"\"\n    try:\n        # Validate main structure\n        if not directions_data.get('routes') or len(directions_data['routes']) == 0:\n            raise ValueError(\"No routes in Google response\")\n            \n        route = directions_data['routes'][0]\n        \n        # Get route legs (Google uses legs instead of segments)\n        legs = route.get('legs', [])\n        if not legs:\n            raise ValueError(\"Missing route legs\")\n        \n        # Calculate total distance and duration from all legs\n        total_distance_m = 0\n        total_duration_s = 0\n        \n        # Parse each step from all legs\n        steps = []\n        step_number = 1\n        \n        for leg in legs:\n            # Add leg distance and duration to totals\n            total_distance_m += leg.get('distance', {}).get('value', 0)\n            total_duration_s += leg.get('duration', {}).get('value', 0)\n            \n            # Get steps from this leg\n            leg_steps = leg.get('steps', [])\n            \n            for step in leg_steps:\n                # Extract step information\n                distance_m = step.get('distance', {}).get('value', 0)\n                duration_s = step.get('duration', {}).get('value', 0)\n                html_instruction = step.get('html_instructions', 'Continue straight')\n                \n                # Clean HTML tags from instruction\n                instruction = clean_html_instruction(html_instruction)\n                \n                # Format step distance and duration\n                step_distance = f\"{distance_m:.0f} m\" if distance_m < 1000 else f\"{distance_m/1000:.1f} km\"\n                step_duration = f\"{duration_s//60:.0f} min\" if duration_s >= 60 else f\"{duration_s:.0f} sec\"\n                \n                # Get start and end coordinates\n                start_location = step.get('start_location', {})\n                end_location = step.get('end_location', {})\n                \n                step_data = {\n                    'step_number': step_number,\n                    'instruction': clean_instruction_text(instruction),\n                    'distance': step_distance,\n                    'duration': step_duration,\n                    'distance_meters': distance_m,\n                    'distance_value': distance_m,  # Add for frontend compatibility\n                    'duration_seconds': duration_s,\n                    'start_location': {\n                        'lat': start_location.get('lat', 0),\n                        'lng': start_location.get('lng', 0)\n                    },\n                    'end_location': {\n                        'lat': end_location.get('lat', 0),\n                        'lng': end_location.get('lng', 0)\n                    },\n                    'maneuver': step.get('maneuver', 'straight'),\n                    'travel_mode': step.get('travel_mode', 'WALKING')\n                }\n                steps.append(step_data)\n                step_number += 1\n        \n        # Format total distance and duration\n        total_distance = f\"{total_distance_m:.0f} m\" if total_distance_m < 1000 else f\"{total_distance_m/1000:.1f} km\"\n        total_duration = f\"{total_duration_s//60:.0f} min\" if total_duration_s >= 60 else f\"{total_duration_s:.0f} sec\"\n        \n        # Ensure we have at least one step\n        if not steps:\n            steps.append({\n                'step_number': 1,\n                'instruction': f'Walk to {destination_name}',\n                'distance': total_distance,\n                'duration': total_duration,\n                'distance_meters': total_distance_m,\n                'duration_seconds': total_duration_s,\n                'start_location': {'lat': 0, 'lng': 0},\n                'end_location': {'lat': 0, 'lng': 0},\n                'maneuver': 'straight',\n                'travel_mode': 'WALKING'\n            })\n        \n        # Get route overview\n        overview_polyline = route.get('overview_polyline', {}).get('points', '')\n        bounds = route.get('bounds', {})\n        start_address = legs[0].get('start_address', 'Current Location') if legs else 'Current Location'\n        end_address = legs[-1].get('end_address', destination_name) if legs else destination_name\n        \n        return {\n            'success': True,\n            'route': {\n                'distance': total_distance,\n                'duration': total_duration,\n                'distance_meters': total_distance_m,\n                'duration_seconds': total_duration_s,\n                'steps': steps,\n                'start_address': start_address,\n                'end_address': end_address,\n                'overview_polyline': overview_polyline,\n                'bounds': bounds\n            }\n        }\n        \n    except (KeyError, IndexError, TypeError) as e:\n        logging.error(f\"Error parsing Google directions data: {e}\")\n        logging.error(f\"Google response structure keys: {list(directions_data.keys()) if isinstance(directions_data, dict) else 'Not a dict'}\")\n        raise ValueError(\"Invalid Google directions data format\")\n\ndef clean_html_instruction(html_instruction):\n    \"\"\"Remove HTML tags from Google's instruction text\"\"\"\n    import re\n    \n    if not html_instruction:\n        return \"Continue straight\"\n    \n    # Remove HTML tags\n    clean_text = re.sub('<[^<]+?>', '', html_instruction)\n    \n    # Decode HTML entities\n    clean_text = clean_text.replace('&nbsp;', ' ')\n    clean_text = clean_text.replace('&amp;', '&')\n    clean_text = clean_text.replace('&lt;', '<')\n    clean_text = clean_text.replace('&gt;', '>')\n    clean_text = clean_text.replace('&quot;', '\"')\n    \n    return clean_text.strip()\n\ndef clean_instruction_text(instruction):\n    \"\"\"Clean and optimize navigation instructions for voice\"\"\"\n    if not instruction:\n        return \"Continue straight\"\n    \n    # Clean up text\n    clean_text = instruction.strip()\n    \n    # Make instructions more voice-friendly\n    clean_text = clean_text.replace('toward', 'towards')\n    clean_text = clean_text.replace('Destination will be on the right', 'Your destination will be on the right')\n    clean_text = clean_text.replace('Destination will be on the left', 'Your destination will be on the left')\n    clean_text = clean_text.replace('Continue on', 'Continue along')\n    \n    return clean_text\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000, debug=True)","size_bytes":24600},"gemini_service.py":{"content":"import os\nimport json\nimport logging\nimport re\nfrom typing import Dict, Any\nfrom google import genai\nfrom google.genai import types\n\nclass GeminiService:\n    \"\"\"Service for handling Gemini AI interactions\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize Gemini client\"\"\"\n        self.api_key = os.environ.get(\"GEMINI_API_KEY\")\n        if not self.api_key:\n            logging.warning(\"GEMINI_API_KEY not found in environment variables\")\n            self.client = None\n        else:\n            try:\n                self.client = genai.Client(api_key=self.api_key)\n                logging.info(\"Gemini client initialized successfully\")\n            except Exception as e:\n                logging.error(f\"Failed to initialize Gemini client: {e}\")\n                self.client = None\n        \n        # Language translations for common responses\n        self.translations = {\n            'en-IN': {\n                'start_detection': 'Starting object detection now.',\n                'stop_detection': 'Stopping object detection.',\n                'enable_location': 'Enabling location services.',\n                'navigation_ready': 'Navigation is ready.',\n                'unknown_command': 'I did not understand that command. Please try again.',\n                'language_changed': 'Language has been changed.',\n            },\n            'hi-IN': {\n                'start_detection': '‡§Ö‡§¨ ‡§ë‡§¨‡•ç‡§ú‡•á‡§ï‡•ç‡§ü ‡§°‡§ø‡§ü‡•á‡§ï‡•ç‡§∂‡§® ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Ç‡•§',\n                'stop_detection': '‡§ë‡§¨‡•ç‡§ú‡•á‡§ï‡•ç‡§ü ‡§°‡§ø‡§ü‡•á‡§ï‡•ç‡§∂‡§® ‡§¨‡§Ç‡§¶ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Ç‡•§',\n                'enable_location': '‡§≤‡•ã‡§ï‡•á‡§∂‡§® ‡§∏‡•á‡§µ‡§æ‡§è‡§Ç ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Ç‡•§',\n                'navigation_ready': '‡§®‡•á‡§µ‡•Ä‡§ó‡•á‡§∂‡§® ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à‡•§',\n                'unknown_command': '‡§Æ‡•Å‡§ù‡•á ‡§µ‡§π ‡§ï‡§Æ‡§æ‡§Ç‡§° ‡§∏‡§Æ‡§ù ‡§®‡§π‡•Ä‡§Ç ‡§Ü‡§à‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§´‡§ø‡§∞ ‡§∏‡•á ‡§ï‡•ã‡§∂‡§ø‡§∂ ‡§ï‡§∞‡•á‡§Ç‡•§',\n                'language_changed': '‡§≠‡§æ‡§∑‡§æ ‡§¨‡§¶‡§≤ ‡§¶‡•Ä ‡§ó‡§à ‡§π‡•à‡•§',\n            },\n            'ta-IN': {\n                'start_detection': '‡Æá‡Æ™‡Øç‡Æ™‡Øã‡Æ§‡ØÅ ‡Æ™‡Øä‡Æ∞‡ØÅ‡Æ≥‡Øç ‡Æï‡Æ£‡Øç‡Æü‡Æ±‡Æø‡Æ§‡Æ≤‡Øà‡Æ§‡Øç ‡Æ§‡Øä‡Æü‡Æô‡Øç‡Æï‡ØÅ‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç.',\n                'stop_detection': '‡Æ™‡Øä‡Æ∞‡ØÅ‡Æ≥‡Øç ‡Æï‡Æ£‡Øç‡Æü‡Æ±‡Æø‡Æ§‡Æ≤‡Øà ‡Æ®‡Æø‡Æ±‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç.',\n                'enable_location': '‡Æá‡Æü‡Æö‡Øç ‡Æö‡Øá‡Æµ‡Øà‡Æï‡Æ≥‡Øà ‡Æá‡ÆØ‡Æï‡Øç‡Æï‡ØÅ‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç.',\n                'navigation_ready': '‡Æµ‡Æ¥‡Æø‡Æö‡ØÜ‡Æ≤‡ØÅ‡Æ§‡Øç‡Æ§‡Æ≤‡Øç ‡Æ§‡ÆØ‡Ææ‡Æ∞‡Øç.',\n                'unknown_command': '‡ÆÖ‡Æ®‡Øç‡Æ§ ‡Æï‡Æü‡Øç‡Æü‡Æ≥‡Øà‡ÆØ‡Øà ‡Æ®‡Ææ‡Æ©‡Øç ‡Æ™‡ØÅ‡Æ∞‡Æø‡Æ®‡Øç‡Æ§‡ØÅ‡Æï‡Øä‡Æ≥‡Øç‡Æ≥‡Æµ‡Æø‡Æ≤‡Øç‡Æ≤‡Øà. ‡ÆÆ‡ØÄ‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç ‡ÆÆ‡ØÅ‡ÆØ‡Æ±‡Øç‡Æö‡Æø‡Æï‡Øç‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç.',\n                'language_changed': '‡ÆÆ‡Øä‡Æ¥‡Æø ‡ÆÆ‡Ææ‡Æ±‡Øç‡Æ±‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü‡Æ§‡ØÅ.',\n            },\n            'te-IN': {\n                'start_detection': '‡∞á‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞µ‡∞∏‡±ç‡∞§‡±Å ‡∞ó‡±Å‡∞∞‡±ç‡∞§‡∞ø‡∞Ç‡∞™‡±Å‡∞®‡±Å ‡∞™‡±ç‡∞∞‡∞æ‡∞∞‡∞Ç‡∞≠‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å.',\n                'stop_detection': '‡∞µ‡∞∏‡±ç‡∞§‡±Å ‡∞ó‡±Å‡∞∞‡±ç‡∞§‡∞ø‡∞Ç‡∞™‡±Å‡∞®‡±Å ‡∞Ü‡∞™‡±Å‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å.',\n                'enable_location': '‡∞∏‡±ç‡∞•‡∞æ‡∞® ‡∞∏‡±á‡∞µ‡∞≤‡∞®‡±Å ‡∞™‡±ç‡∞∞‡∞æ‡∞∞‡∞Ç‡∞≠‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å.',\n                'navigation_ready': '‡∞®‡∞æ‡∞µ‡∞ø‡∞ó‡±á‡∞∑‡∞®‡±ç ‡∞∏‡∞ø‡∞¶‡±ç‡∞ß‡∞Ç‡∞ó‡∞æ ‡∞â‡∞Ç‡∞¶‡∞ø.',\n                'unknown_command': '‡∞Ü ‡∞ï‡∞Æ‡∞æ‡∞Ç‡∞°‡±ç ‡∞®‡∞æ‡∞ï‡±Å ‡∞Ö‡∞∞‡±ç‡∞•‡∞Ç ‡∞ï‡∞æ‡∞≤‡±á‡∞¶‡±Å. ‡∞¶‡∞Ø‡∞ö‡±á‡∞∏‡∞ø ‡∞Æ‡∞≥‡±ç‡∞≤‡±Ä ‡∞™‡±ç‡∞∞‡∞Ø‡∞§‡±ç‡∞®‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø.',\n                'language_changed': '‡∞≠‡∞æ‡∞∑ ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ö‡∞¨‡∞°‡∞ø‡∞Ç‡∞¶‡∞ø.',\n            },\n            'bn-IN': {\n                'start_detection': '‡¶è‡¶ñ‡¶® ‡¶Ö‡¶¨‡¶ú‡ßá‡¶ï‡ßç‡¶ü ‡¶°‡¶ø‡¶ü‡ßá‡¶ï‡¶∂‡¶® ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡¶õ‡¶ø‡•§',\n                'stop_detection': '‡¶Ö‡¶¨‡¶ú‡ßá‡¶ï‡ßç‡¶ü ‡¶°‡¶ø‡¶ü‡ßá‡¶ï‡¶∂‡¶® ‡¶¨‡¶®‡ßç‡¶ß ‡¶ï‡¶∞‡¶õ‡¶ø‡•§',\n                'enable_location': '‡¶≤‡ßã‡¶ï‡ßá‡¶∂‡¶® ‡¶∏‡ßá‡¶¨‡¶æ ‡¶ö‡¶æ‡¶≤‡ßÅ ‡¶ï‡¶∞‡¶õ‡¶ø‡•§',\n                'navigation_ready': '‡¶®‡ßá‡¶≠‡¶ø‡¶ó‡ßá‡¶∂‡¶® ‡¶™‡ßç‡¶∞‡¶∏‡ßç‡¶§‡ßÅ‡¶§‡•§',\n                'unknown_command': '‡¶∏‡ßá‡¶á ‡¶ï‡¶Æ‡¶æ‡¶®‡ßç‡¶°‡¶ü‡¶ø ‡¶Ü‡¶Æ‡¶ø ‡¶¨‡ßÅ‡¶ù‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶ø‡¶®‡¶ø‡•§ ‡¶¶‡¶Ø‡¶º‡¶æ ‡¶ï‡¶∞‡ßá ‡¶Ü‡¶¨‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®‡•§',\n                'language_changed': '‡¶≠‡¶æ‡¶∑‡¶æ ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶® ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§',\n            },\n            'mr-IN': {\n                'start_detection': '‡§Ü‡§§‡§æ ‡§ë‡§¨‡•ç‡§ú‡•á‡§ï‡•ç‡§ü ‡§°‡§ø‡§ü‡•á‡§ï‡•ç‡§∂‡§® ‡§∏‡•Å‡§∞‡•Ç ‡§ï‡§∞‡§§ ‡§Ü‡§π‡•á‡•§',\n                'stop_detection': '‡§ë‡§¨‡•ç‡§ú‡•á‡§ï‡•ç‡§ü ‡§°‡§ø‡§ü‡•á‡§ï‡•ç‡§∂‡§® ‡§•‡§æ‡§Ç‡§¨‡§µ‡§§ ‡§Ü‡§π‡•á‡•§',\n                'enable_location': '‡§≤‡•ã‡§ï‡•á‡§∂‡§® ‡§∏‡•á‡§µ‡§æ ‡§∏‡§ï‡•ç‡§∑‡§Æ ‡§ï‡§∞‡§§ ‡§Ü‡§π‡•á‡•§',\n                'navigation_ready': '‡§®‡•á‡§µ‡•ç‡§π‡§ø‡§ó‡•á‡§∂‡§® ‡§§‡§Ø‡§æ‡§∞ ‡§Ü‡§π‡•á‡•§',\n                'unknown_command': '‡§Æ‡§≤‡§æ ‡§§‡•Ä ‡§ï‡§Æ‡§æ‡§Ç‡§° ‡§∏‡§Æ‡§ú‡§≤‡•Ä ‡§®‡§æ‡§π‡•Ä. ‡§ï‡•É‡§™‡§Ø‡§æ ‡§™‡•Å‡§®‡•ç‡§π‡§æ ‡§™‡•ç‡§∞‡§Ø‡§§‡•ç‡§® ‡§ï‡§∞‡§æ.',\n                'language_changed': '‡§≠‡§æ‡§∑‡§æ ‡§¨‡§¶‡§≤‡§≤‡•Ä ‡§Ü‡§π‡•á.',\n            },\n            'gu-IN': {\n                'start_detection': '‡™π‡™µ‡´á ‡™ì‡™¨‡´ç‡™ú‡´á‡™ï‡´ç‡™ü ‡™°‡™ø‡™ü‡´á‡™ï‡´ç‡™∂‡™® ‡™∂‡™∞‡´Ç ‡™ï‡™∞‡´Ä ‡™∞‡™π‡´ç‡™Ø‡´ã ‡™õ‡´Å‡™Ç.',\n                'stop_detection': '‡™ì‡™¨‡´ç‡™ú‡´á‡™ï‡´ç‡™ü ‡™°‡™ø‡™ü‡´á‡™ï‡´ç‡™∂‡™® ‡™¨‡™Ç‡™ß ‡™ï‡™∞‡´Ä ‡™∞‡™π‡´ç‡™Ø‡´ã ‡™õ‡´Å‡™Ç.',\n                'enable_location': '‡™≤‡´ã‡™ï‡´á‡™∂‡™® ‡™∏‡´á‡™µ‡™æ‡™ì ‡™∏‡™ï‡´ç‡™∑‡™Æ ‡™ï‡™∞‡´Ä ‡™∞‡™π‡´ç‡™Ø‡´ã ‡™õ‡´Å‡™Ç.',\n                'navigation_ready': '‡™®‡´á‡™µ‡™ø‡™ó‡´á‡™∂‡™® ‡™§‡´à‡™Ø‡™æ‡™∞ ‡™õ‡´á.',\n                'unknown_command': '‡™Æ‡™®‡´á ‡™§‡´á ‡™ï‡™Æ‡™æ‡™®‡´ç‡™° ‡™∏‡™Æ‡™ú‡™æ‡™à ‡™®‡™π‡´Ä‡™Ç. ‡™ï‡´É‡™™‡™æ ‡™ï‡™∞‡´Ä‡™®‡´á ‡™´‡™∞‡´Ä‡™•‡´Ä ‡™™‡´ç‡™∞‡™Ø‡™æ‡™∏ ‡™ï‡™∞‡´ã.',\n                'language_changed': '‡™≠‡™æ‡™∑‡™æ ‡™¨‡™¶‡™≤‡™µ‡™æ‡™Æ‡™æ‡™Ç ‡™Ü‡™µ‡´Ä ‡™õ‡´á.',\n            }\n        }\n\n    def process_voice_command(self, command: str, language: str = 'en-IN', tone: str = 'friendly') -> Dict[str, Any]:\n        \"\"\"\n        Process voice command using Gemini AI\n        \n        Args:\n            command: User's voice command\n            language: Language code (e.g., 'en-IN', 'hi-IN')\n            tone: Voice tone preference (e.g., 'friendly', 'formal', 'energetic')\n            \n        Returns:\n            Dictionary with action, destination (if applicable), and response\n        \"\"\"\n        try:\n            # If Gemini is not available, use fallback logic\n            if not self.client:\n                return self._fallback_command_processing(command, language, tone)\n            \n            # Create system prompt for command processing\n            system_prompt = self._create_system_prompt(language, tone)\n            \n            # Process command with Gemini - ensure UTF-8 encoding\n            user_prompt = f\"User command: '{command}'\"\n            \n            response = self.client.models.generate_content(\n                model=\"gemini-2.5-flash\",\n                contents=[\n                    types.Content(role=\"user\", parts=[types.Part(text=user_prompt)])\n                ],\n                config=types.GenerateContentConfig(\n                    system_instruction=system_prompt,\n                    response_mime_type=\"application/json\",\n                    temperature=0.3,\n                    max_output_tokens=1000\n                )\n            )\n            \n            if not response.text:\n                raise ValueError(\"Empty response from Gemini\")\n            \n            # Parse JSON response with proper encoding\n            result = json.loads(response.text)\n            \n            # Validate and enhance response\n            return self._validate_and_enhance_response(result, language, tone)\n            \n        except Exception as e:\n            logging.error(f\"Error processing command with Gemini: {e}\")\n            return self._fallback_command_processing(command, language, tone)\n\n    def _create_system_prompt(self, language: str, tone: str = 'friendly') -> str:\n        \"\"\"Create system prompt for Gemini based on language and tone\"\"\"\n        \n        # Define tone characteristics\n        tone_instructions = {\n            'friendly': 'Use a warm, friendly, and encouraging tone. Be supportive and cheerful.',\n            'formal': 'Use a professional, polite, and respectful tone. Be clear and courteous.',\n            'energetic': 'Use an enthusiastic, vibrant, and motivating tone. Be upbeat and inspiring.',\n            'calm': 'Use a gentle, soothing, and peaceful tone. Be reassuring and steady.',\n            'robotic': 'Use a neutral, precise, and direct tone. Be factual and systematic.'\n        }\n        \n        tone_instruction = tone_instructions.get(tone, tone_instructions['friendly'])\n        \n        base_prompt = f\"\"\"You are BlindMate, an AI assistant for visually impaired users. Process voice commands and return JSON responses.\n\nIMPORTANT: {tone_instruction}\n\nAvailable actions:\n1. start_detection - Start object detection\n2. stop_detection - Stop object detection  \n3. navigate - Navigate to a destination\n4. preview_route - Preview route to destination\n5. stop_navigation - Stop current navigation\n6. show_map - Show navigation map during navigation\n7. emergency_stop - Emergency stop navigation immediately\n8. test_voice - Test voice recognition functionality\n9. toggle_obstacle_alerts - Enable/disable obstacle alerts during navigation\n10. save_location - Save current location with a name\n11. enable_location - Enable location services\n12. change_language - Change interface language\n13. change_tone - Change voice tone/style\n14. unknown - For unrecognized commands\n\nResponse format (JSON only):\n{{\n    \"action\": \"action_name\",\n    \"destination\": \"place_name\",\n    \"response\": \"what_to_speak_to_user\",\n    \"language\": \"language_code_if_changed\",\n    \"tone\": \"tone_if_changed\"\n}}\n\nCommand examples:\n- \"start detection\" -> {{\"action\": \"start_detection\", \"response\": \"Starting object detection\"}}\n- \"show map\" or \"show navigation map\" -> {{\"action\": \"show_map\", \"response\": \"Showing navigation map\"}}\n- \"emergency stop\" or \"stop navigation now\" -> {{\"action\": \"emergency_stop\", \"response\": \"Stopping navigation immediately\"}}\n- \"test voice\" or \"check microphone\" -> {{\"action\": \"test_voice\", \"response\": \"Testing voice recognition\"}}\n- \"toggle obstacle alerts\" or \"disable alerts\" -> {{\"action\": \"toggle_obstacle_alerts\", \"response\": \"Toggling obstacle alerts\"}}\n- \"take me to library\" -> {{\"action\": \"navigate\", \"destination\": \"library\", \"response\": \"Navigating to library\"}}\n- \"change language to Hindi\" -> {{\"action\": \"change_language\", \"language\": \"hi-IN\", \"response\": \"‡§≠‡§æ‡§∑‡§æ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡§¶‡§≤ ‡§¶‡•Ä ‡§ó‡§à ‡§π‡•à\"}}\n- \"change tone to formal\" -> {{\"action\": \"change_tone\", \"tone\": \"formal\", \"response\": \"Voice tone changed to formal\"}}\n- \"speak in friendly voice\" -> {{\"action\": \"change_tone\", \"tone\": \"friendly\", \"response\": \"Voice tone changed to friendly\"}}\n\nLanguage change commands:\n- Detect commands like \"change language to [language]\", \"speak in [language]\", \"switch to [language]\"\n- Support: English, Hindi, Spanish, French, German, Italian, Portuguese, Japanese, Chinese, Arabic\n\nTone change commands:\n- Detect commands like \"change tone to [tone]\", \"speak in [tone] voice\", \"be more [tone]\"\n- Support: friendly, formal, energetic, calm, robotic\n\nNavigation works with ANY location worldwide - users can name any place, address, or landmark.\n\nRespond only with valid JSON, no extra text.\"\"\"\n\n        # Add language-specific instructions\n        if language.startswith('hi'):\n            base_prompt += f\"\\n\\nRespond in Hindi (‡§π‡§ø‡§Ç‡§¶‡•Ä). {tone_instruction}\"\n        elif language.startswith('ta'):\n            base_prompt += f\"\\n\\nRespond in Tamil (‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç). {tone_instruction}\"\n        elif language.startswith('te'):\n            base_prompt += f\"\\n\\nRespond in Telugu (‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å). {tone_instruction}\"\n        elif language.startswith('bn'):\n            base_prompt += f\"\\n\\nRespond in Bengali (‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ). {tone_instruction}\"\n        elif language.startswith('mr'):\n            base_prompt += f\"\\n\\nRespond in Marathi (‡§Æ‡§∞‡§æ‡§†‡•Ä). {tone_instruction}\"\n        elif language.startswith('gu'):\n            base_prompt += f\"\\n\\nRespond in Gujarati (‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä). {tone_instruction}\"\n        elif language.startswith('es'):\n            base_prompt += f\"\\n\\nRespond in Spanish (Espa√±ol). {tone_instruction}\"\n        elif language.startswith('fr'):\n            base_prompt += f\"\\n\\nRespond in French (Fran√ßais). {tone_instruction}\"\n        elif language.startswith('de'):\n            base_prompt += f\"\\n\\nRespond in German (Deutsch). {tone_instruction}\"\n        elif language.startswith('it'):\n            base_prompt += f\"\\n\\nRespond in Italian (Italiano). {tone_instruction}\"\n        elif language.startswith('pt'):\n            base_prompt += f\"\\n\\nRespond in Portuguese (Portugu√™s). {tone_instruction}\"\n        elif language.startswith('ja'):\n            base_prompt += f\"\\n\\nRespond in Japanese (Êó•Êú¨Ë™û). {tone_instruction}\"\n        elif language.startswith('zh'):\n            base_prompt += f\"\\n\\nRespond in Chinese (‰∏≠Êñá). {tone_instruction}\"\n        elif language.startswith('ar'):\n            base_prompt += f\"\\n\\nRespond in Arabic (ÿßŸÑÿπÿ±ÿ®Ÿäÿ©). {tone_instruction}\"\n        \n        return base_prompt\n\n    def _validate_and_enhance_response(self, result: Dict[str, Any], language: str, tone: str = 'friendly') -> Dict[str, Any]:\n        \"\"\"Validate and enhance the Gemini response\"\"\"\n        \n        # Ensure required fields exist\n        if 'action' not in result:\n            result['action'] = 'unknown'\n        \n        if 'response' not in result:\n            result['response'] = self._get_translation('unknown_command', language)\n        \n        # Validate action\n        valid_actions = ['start_detection', 'stop_detection', 'navigate', 'enable_location', 'change_language', 'change_tone', 'unknown']\n        if result['action'] not in valid_actions:\n            result['action'] = 'unknown'\n            result['response'] = self._get_translation('unknown_command', language)\n        \n        # Add current tone to response\n        result['current_tone'] = tone\n        \n        return result\n\n    def _fallback_command_processing(self, command: str, language: str, tone: str = 'friendly') -> Dict[str, Any]:\n        \"\"\"Fallback command processing when Gemini is unavailable\"\"\"\n        \n        command_lower = command.lower().strip()\n        \n        # Filter out meaningless commands first\n        meaningless_patterns = [\n            r'^(um|uh|ah|er|hm|hmm|yes|yeah|no|okay|ok)$',\n            r'^sorry i didn\\'?t understand',\n            r'^please try again',\n            r'^what$',\n            r'^\\s*$',  # Empty or whitespace only\n            r'^.{1,2}$'  # Very short commands (1-2 characters)\n        ]\n        \n        for pattern in meaningless_patterns:\n            if re.match(pattern, command_lower, re.IGNORECASE):\n                # Return silent action for meaningless commands\n                return {\n                    'action': 'silent',\n                    'response': ''\n                }\n        \n        # Simple keyword matching\n        if any(word in command_lower for word in ['start', 'begin', '‡§∂‡•Å‡§∞‡•Ç', '‡Æ§‡Øä‡Æü‡Æô‡Øç‡Æï‡ØÅ', '‡∞™‡±ç‡∞∞‡∞æ‡∞∞‡∞Ç‡∞≠‡∞ø‡∞Ç‡∞ö‡±Å']):\n            if any(word in command_lower for word in ['detection', 'scanning', '‡§°‡§ø‡§ü‡•á‡§ï‡•ç‡§∂‡§®', '‡§ï‡•Ö‡§®', '‡Æï‡Æ£‡Øç‡Æü‡Æ±‡Æø‡Æ§‡Æ≤‡Øç']):\n                return {\n                    'action': 'start_detection',\n                    'response': self._get_translation('start_detection', language)\n                }\n        \n        elif any(word in command_lower for word in ['stop', 'pause', '‡§¨‡§Ç‡§¶', '‡§∞‡•ã‡§ï', '‡Æ®‡Æø‡Æ±‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ', '‡∞Ü‡∞™‡±Å']):\n            return {\n                'action': 'stop_detection',\n                'response': self._get_translation('stop_detection', language)\n            }\n        \n        elif any(word in command_lower for word in ['take me', 'navigate', 'go to', '‡§≤‡•á ‡§ö‡§≤‡•ã', '‡§ú‡§æ‡§®‡§æ', '‡Æé‡Æ©‡Øç‡Æ©‡Øà ‡ÆÖ‡Æ¥‡Øà‡Æ§‡Øç‡Æ§‡ØÅ‡Æö‡Øç ‡Æö‡ØÜ‡Æ≤‡Øç', '‡∞®‡∞®‡±ç‡∞®‡±Å ‡∞§‡±Ä‡∞∏‡±Å‡∞ï‡±Ü‡∞≥‡±ç‡∞≤‡±Å']):\n            # Extract destination\n            destination = self._extract_destination(command_lower)\n            return {\n                'action': 'navigate',\n                'destination': destination,\n                'response': f\"{self._get_translation('navigation_ready', language)} {destination}\"\n            }\n        \n        elif any(word in command_lower for word in ['location', 'enable location', '‡§≤‡•ã‡§ï‡•á‡§∂‡§®', '‡§∏‡•ç‡§•‡§æ‡§®', '‡Æá‡Æü‡ÆÆ‡Øç', '‡∞∏‡±ç‡∞•‡∞æ‡∞®‡∞Ç']):\n            return {\n                'action': 'enable_location',\n                'response': self._get_translation('enable_location', language)\n            }\n        \n        elif any(word in command_lower for word in ['change language', 'switch language', 'speak in', '‡§≠‡§æ‡§∑‡§æ ‡§¨‡§¶‡§≤‡•ã', '‡§Æ‡•ã‡§ù‡•Ä ‡§¨‡§¶‡§≤‡§æ', '‡ÆÆ‡Øä‡Æ¥‡Æø‡ÆØ‡Øà ‡ÆÆ‡Ææ‡Æ±‡Øç‡Æ±‡ØÅ']):\n            new_language = self._extract_language(command_lower)\n            return {\n                'action': 'change_language',\n                'language': new_language,\n                'response': self._get_translation('language_changed', language)\n            }\n        \n        elif any(word in command_lower for word in ['change tone', 'switch tone', 'voice tone', 'be more', 'speak', 'sound']):\n            new_tone = self._extract_tone(command_lower)\n            if new_tone:\n                return {\n                    'action': 'change_tone',\n                    'tone': new_tone,\n                    'response': f\"Voice tone changed to {new_tone}\"\n                }\n        \n        # Unknown command\n        return {\n            'action': 'unknown',\n            'response': self._get_translation('unknown_command', language)\n        }\n\n    def _extract_destination(self, command: str) -> str:\n        \"\"\"Extract destination from navigation command\"\"\"\n        # Simple extraction - look for common location words\n        patterns = [\n            r'(?:take me to|go to|navigate to)\\s+(.+)',\n            r'(?:‡§≤‡•á ‡§ö‡§≤‡•ã|‡§ú‡§æ‡§®‡§æ ‡§π‡•à)\\s+(.+)',\n            r'(?:‡Æé‡Æ©‡Øç‡Æ©‡Øà ‡ÆÖ‡Æ¥‡Øà‡Æ§‡Øç‡Æ§‡ØÅ‡Æö‡Øç ‡Æö‡ØÜ‡Æ≤‡Øç)\\s+(.+)',\n            r'(?:‡∞®‡∞®‡±ç‡∞®‡±Å ‡∞§‡±Ä‡∞∏‡±Å‡∞ï‡±Ü‡∞≥‡±ç‡∞≤‡±Å)\\s+(.+)'\n        ]\n        \n        for pattern in patterns:\n            match = re.search(pattern, command, re.IGNORECASE)\n            if match:\n                return match.group(1).strip()\n        \n        # If no specific pattern matches, return a generic destination\n        return \"the requested location\"\n\n    def _extract_language(self, command: str) -> str:\n        \"\"\"Extract language from language change command\"\"\"\n        language_map = {\n            'hindi': 'hi-IN',\n            '‡§π‡§ø‡§Ç‡§¶‡•Ä': 'hi-IN',\n            'tamil': 'ta-IN',\n            '‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç': 'ta-IN',\n            'telugu': 'te-IN',\n            '‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å': 'te-IN',\n            'bengali': 'bn-IN',\n            '‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ': 'bn-IN',\n            'marathi': 'mr-IN',\n            '‡§Æ‡§∞‡§æ‡§†‡•Ä': 'mr-IN',\n            'gujarati': 'gu-IN',\n            '‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä': 'gu-IN',\n            'english': 'en-IN'\n        }\n        \n        command_lower = command.lower()\n        for lang_name, lang_code in language_map.items():\n            if lang_name in command_lower:\n                return lang_code\n        \n        return 'en-IN'  # Default to English\n\n    def _extract_tone(self, command: str) -> str:\n        \"\"\"Extract tone from tone change command\"\"\"\n        tone_map = {\n            'friendly': 'friendly',\n            'formal': 'formal',\n            'professional': 'formal',\n            'energetic': 'energetic',\n            'enthusiastic': 'energetic',\n            'excited': 'energetic',\n            'calm': 'calm',\n            'peaceful': 'calm',\n            'soothing': 'calm',\n            'robotic': 'robotic',\n            'robot': 'robotic',\n            'neutral': 'robotic'\n        }\n        \n        command_lower = command.lower()\n        for tone_name, tone_code in tone_map.items():\n            if tone_name in command_lower:\n                return tone_code\n        \n        return None  # No tone detected\n\n    def _get_translation(self, key: str, language: str) -> str:\n        \"\"\"Get translated text for the given key and language\"\"\"\n        if language in self.translations and key in self.translations[language]:\n            return self.translations[language][key]\n        \n        # Fallback to English\n        return self.translations['en-IN'].get(key, 'Command processed.')\n","size_bytes":20959},"main.py":{"content":"from app import app\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000, debug=True)\n","size_bytes":99},"navigation.js":{"content":"/**\n * BlindMate Enhanced Navigation System\n * High-accuracy GPS tracking with optimized battery usage and human-friendly speech instructions\n * Features: Smart tracking frequency, robust error handling, clear voice navigation\n */\nclass UniversalNavigation {\n    constructor() {\n        // Navigation states\n        this.isNavigating = false;\n        this.currentRoute = null;\n        this.currentStepIndex = 0;\n        this.watchId = null;\n        this.currentPosition = null;\n        this.awaitingConfirmation = false;\n        this.currentDestination = null;\n        this.lastLocationUpdateTime = null;\n        this.userSpeed = 0; // m/s\n        this.stationary = false;\n        this.destinationReached = false;\n        \n        // Enhanced navigation configuration\n        this.config = {\n            stepProximityThreshold: 15, // meters - when to advance to next step\n            routeDeviationThreshold: 30, // meters - when to reroute\n            destinationReachedThreshold: 10, // meters - when destination is reached\n            // Battery optimization thresholds\n            highFrequencyInterval: 2000, // ms - when user is moving\n            lowFrequencyInterval: 8000, // ms - when user is stationary\n            stationarySpeedThreshold: 0.5, // m/s - below this is considered stationary\n            // Voice instruction optimization\n            voicePreviewDistance: 50, // meters - when to announce \"in X meters\"\n            repeatInstructionDistance: 25, // meters - repeat instructions if user hasn't moved\n            urgentAnnouncementDistance: 10, // meters - for urgent turn warnings\n        };\n        \n        // Google Maps integration\n        this.map = null;\n        this.directionsService = null;\n        this.directionsRenderer = null;\n        this.userMarker = null;\n        this.googleMapsApiKey = null;\n        \n        // Enhanced speech recognition and synthesis\n        this.recognition = null;\n        this.confirmationRecognition = null;\n        this.speechSynthesis = window.speechSynthesis;\n        this.isSpeaking = false;\n        this.speechQueue = [];\n        this.lastUtterance = null;\n        this.speechCancellationTimer = null;\n        \n        // COCO-SSD model for obstacle detection\n        this.model = null;\n        this.isDetecting = false;\n        this.detectionCanvas = null;\n        this.detectionContext = null;\n        this.camera = null;\n        \n        // Enhanced permissions and error handling\n        this.permissions = {\n            camera: false,\n            microphone: false,\n            location: false\n        };\n        this.errorStates = {\n            gpsLost: false,\n            speechFailed: false,\n            routingFailed: false\n        };\n        \n        // Mobile device detection\n        this.isMobile = this.detectMobileDevice();\n        \n        this.initialize();\n    }\n    \n    /**\n     * Detect if running on mobile device for battery optimization\n     */\n    detectMobileDevice() {\n        const userAgent = navigator.userAgent;\n        const isMobile = /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(userAgent) || \n                         'ontouchstart' in window || \n                         navigator.maxTouchPoints > 0;\n        \n        console.log('Mobile device detected:', isMobile, {\n            userAgent: userAgent,\n            ontouchstart: 'ontouchstart' in window,\n            maxTouchPoints: navigator.maxTouchPoints\n        });\n        \n        return isMobile;\n    }\n    \n    /**\n     * Initialize the navigation system\n     */\n    async initialize() {\n        console.log('Initializing BlindMate Navigation System...');\n        \n        this.setupSpeechRecognition();\n        this.setupUIEventListeners();\n        \n        // Request all permissions on page load\n        await this.requestAllPermissions();\n        \n        // Initialize camera for obstacle detection\n        await this.initializeCamera();\n        \n        // Load object detection model\n        await this.loadModel();\n        \n        // Get Google Maps API key\n        await this.getGoogleMapsApiKey();\n        \n        console.log('BlindMate Navigation System initialized');\n        \n        // Setup mobile-specific optimizations\n        if (this.isMobile) {\n            console.log('Checking mobile device for double-tap setup:', this.isMobile);\n            this.setupMobileOptimizations();\n        } else {\n            console.log('Desktop device - double-tap not enabled');\n        }\n    }\n    \n    /**\n     * Setup mobile-specific optimizations for battery life\n     */\n    setupMobileOptimizations() {\n        // Enable high accuracy mode for mobile devices\n        this.config.highAccuracyMode = true;\n        \n        // Reduce detection frequency on mobile to save battery\n        this.config.obstacleDetectionInterval = 3000;\n        \n        // Enable more aggressive stationary detection\n        this.config.stationaryTimeout = 30000; // 30 seconds\n        \n        console.log('Mobile optimizations enabled for battery efficiency');\n    }\n    \n    /**\n     * Get Google Maps API key from backend\n     */\n    async getGoogleMapsApiKey() {\n        try {\n            const response = await fetch('/api/google-maps-key');\n            if (response.ok) {\n                const data = await response.json();\n                this.googleMapsApiKey = data.key;\n                console.log('Google Maps API key retrieved');\n                \n                // Initialize Google Maps if key is available\n                if (window.google && window.google.maps) {\n                    this.initializeGoogleMaps();\n                }\n            } else {\n                console.error('Failed to get Google Maps API key');\n            }\n        } catch (error) {\n            console.error('Error getting Google Maps API key:', error);\n        }\n    }\n    \n    /**\n     * Initialize Google Maps\n     */\n    initializeGoogleMaps() {\n        if (!this.googleMapsApiKey) {\n            console.error('Google Maps API key not available');\n            return;\n        }\n        \n        console.log('Google Maps JavaScript API will be used for navigation');\n        \n        // Initialize map\n        const defaultCenter = this.currentPosition ? \n            { lat: this.currentPosition.latitude, lng: this.currentPosition.longitude } :\n            { lat: 28.6139, lng: 77.2090 }; // Default to Delhi\n        \n        this.map = new google.maps.Map(document.getElementById('map'), {\n            zoom: 16,\n            center: defaultCenter,\n            mapTypeId: google.maps.MapTypeId.ROADMAP\n        });\n        \n        this.directionsService = new google.maps.DirectionsService();\n        this.directionsRenderer = new google.maps.DirectionsRenderer({\n            map: this.map,\n            suppressMarkers: false\n        });\n        \n        console.log('Google Maps initialized');\n    }\n    \n    /**\n     * Initialize map (called by Google Maps API callback)\n     */\n    initializeMap() {\n        console.log('Google Maps callback triggered');\n        this.initializeGoogleMaps();\n    }\n    \n    /**\n     * Request all permissions on page load\n     */\n    async requestAllPermissions() {\n        console.log('Requesting all permissions...');\n        \n        try {\n            // Request microphone permission\n            console.log('Requesting microphone permission...');\n            const audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });\n            this.permissions.microphone = true;\n            audioStream.getTracks().forEach(track => track.stop());\n            console.log('Microphone permission granted');\n            \n            // Request camera permission\n            console.log('Requesting camera permission...');\n            const videoStream = await navigator.mediaDevices.getUserMedia({ video: true });\n            this.permissions.camera = true;\n            videoStream.getTracks().forEach(track => track.stop());\n            console.log('Camera permission granted');\n            \n            // Request location permission\n            console.log('Requesting location permission...');\n            this.currentPosition = await this.getCurrentPosition();\n            this.permissions.location = true;\n            console.log('Location permission granted');\n            \n            this.speak('All permissions granted. Navigation system ready.');\n            \n        } catch (error) {\n            console.error('Permission request failed:', error);\n            this.speak('Some permissions were denied. Please enable all permissions for full functionality.');\n        }\n    }\n    \n    /**\n     * Get current position with enhanced accuracy and error handling\n     */\n    getCurrentPosition() {\n        return new Promise((resolve, reject) => {\n            if (!navigator.geolocation) {\n                const error = new Error('Geolocation not supported on this device');\n                this.handleLocationError(error);\n                reject(error);\n                return;\n            }\n            \n            navigator.geolocation.getCurrentPosition(\n                position => {\n                    this.errorStates.gpsLost = false;\n                    console.log('GPS position acquired successfully');\n                    resolve(position.coords);\n                },\n                error => {\n                    this.handleLocationError(error);\n                    reject(error);\n                },\n                {\n                    enableHighAccuracy: true,\n                    timeout: 10000,\n                    maximumAge: 0 // Always get fresh location\n                }\n            );\n        });\n    }\n    \n    /**\n     * Handle location errors with user-friendly messages and UI updates\n     */\n    handleLocationError(error) {\n        this.errorStates.gpsLost = true;\n        let errorMessage = '';\n        let uiMessage = '';\n        \n        switch(error.code) {\n            case error.PERMISSION_DENIED:\n                errorMessage = 'Location access denied. Please enable location permissions in your browser settings.';\n                uiMessage = 'Location Permission Denied';\n                break;\n            case error.POSITION_UNAVAILABLE:\n                errorMessage = 'Location information unavailable. Please check your GPS connection.';\n                uiMessage = 'GPS Signal Unavailable';\n                break;\n            case error.TIMEOUT:\n                errorMessage = 'Location request timed out. Trying again...';\n                uiMessage = 'GPS Signal Weak';\n                break;\n            default:\n                errorMessage = 'Unknown location error occurred. Please try again.';\n                uiMessage = 'Location Error';\n                break;\n        }\n        \n        console.error('Location error:', error.message, errorMessage);\n        this.speakErrorMessage(errorMessage);\n        this.updateStatusDisplay(uiMessage, errorMessage);\n        this.showErrorInUI(uiMessage, errorMessage);\n    }\n    \n    /**\n     * Show error message in UI with clear visual indication\n     */\n    showErrorInUI(title, message) {\n        const errorElement = document.getElementById('errorDisplay');\n        if (errorElement) {\n            errorElement.innerHTML = `\n                <div class=\"error-message\">\n                    <i class=\"fas fa-exclamation-triangle\"></i>\n                    <strong>${title}</strong>\n                    <p>${message}</p>\n                </div>\n            `;\n            errorElement.style.display = 'block';\n            \n            // Auto-hide after 10 seconds\n            setTimeout(() => {\n                errorElement.style.display = 'none';\n            }, 10000);\n        }\n    }\n    \n    /**\n     * Initialize camera for obstacle detection\n     */\n    async initializeCamera() {\n        try {\n            const video = document.getElementById('webcam');\n            if (!video) {\n                console.warn('Webcam element not found');\n                return;\n            }\n            \n            this.camera = video;\n            const stream = await navigator.mediaDevices.getUserMedia({\n                video: { width: 640, height: 480 }\n            });\n            \n            video.srcObject = stream;\n            await new Promise(resolve => {\n                video.onloadedmetadata = () => {\n                    video.play();\n                    resolve();\n                };\n            });\n            \n            console.log('Camera initialized successfully');\n            \n            // Setup detection canvas\n            this.detectionCanvas = document.getElementById('canvas') || document.createElement('canvas');\n            this.detectionContext = this.detectionCanvas.getContext('2d');\n            \n        } catch (error) {\n            console.error('Camera initialization failed:', error);\n        }\n    }\n    \n    /**\n     * Load COCO-SSD model for object detection\n     */\n    async loadModel() {\n        try {\n            if (typeof cocoSsd === 'undefined') {\n                console.warn('COCO-SSD not loaded, obstacle detection disabled');\n                return;\n            }\n            \n            console.log('Loading COCO-SSD model for navigation...');\n            this.model = await cocoSsd.load();\n            console.log('COCO-SSD model loaded successfully');\n            console.log('COCO-SSD model loaded for navigation');\n            \n        } catch (error) {\n            console.error('Failed to load COCO-SSD model:', error);\n        }\n    }\n    \n    /**\n     * Setup speech recognition for navigation commands\n     */\n    setupSpeechRecognition() {\n        if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {\n            console.error('Speech recognition not supported');\n            return;\n        }\n        \n        console.log('Initializing speech recognition...');\n        \n        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n        \n        // Main navigation recognition\n        this.recognition = new SpeechRecognition();\n        this.recognition.continuous = false;\n        this.recognition.interimResults = false;\n        this.recognition.lang = 'en-US';\n        \n        console.log('Speech recognition object created successfully');\n        \n        this.recognition.onstart = () => {\n            console.log('Speech recognition started');\n            this.updateStatusDisplay('Listening...', 'Speak your destination');\n        };\n        \n        this.recognition.onresult = (event) => {\n            if (event.results && event.results[0]) {\n                const command = event.results[0][0].transcript.toLowerCase().trim();\n                console.log('Navigation command received:', command);\n                this.processNavigationCommand(command);\n            }\n        };\n        \n        this.recognition.onerror = (event) => {\n            console.error('Speech recognition error:', event.error);\n            this.speak('Sorry, I didn\\'t understand. Please try again.');\n        };\n        \n        // Confirmation recognition\n        this.confirmationRecognition = new SpeechRecognition();\n        this.confirmationRecognition.continuous = false;\n        this.confirmationRecognition.interimResults = false;\n        this.confirmationRecognition.lang = 'en-US';\n        \n        this.confirmationRecognition.onresult = (event) => {\n            if (event.results && event.results[0]) {\n                const response = event.results[0][0].transcript.toLowerCase().trim();\n                this.processConfirmation(response);\n            }\n        };\n    }\n    \n    /**\n     * Setup enhanced UI event listeners\n     */\n    setupUIEventListeners() {\n        console.log('UI event listeners setup complete');\n        \n        // Volume key detection for hands-free operation\n        document.addEventListener('keydown', (event) => {\n            if (event.code === 'AudioVolumeUp' || event.key === 'AudioVolumeUp') {\n                event.preventDefault();\n                this.startListening();\n            }\n        });\n        \n        // Main navigation button (primary interface)\n        const mainBtn = document.getElementById('mainButton');\n        if (mainBtn) {\n            mainBtn.addEventListener('click', () => {\n                if (this.isNavigating) {\n                    this.stopNavigation();\n                } else {\n                    this.startListening();\n                }\n                this.updateMainButtonState();\n            });\n        }\n        \n        // Emergency stop button\n        const emergencyBtn = document.getElementById('emergencyStop');\n        if (emergencyBtn) {\n            emergencyBtn.addEventListener('click', () => {\n                this.emergencyStop();\n            });\n        }\n        \n        // Accessibility: Allow Enter key to activate main button\n        if (mainBtn) {\n            mainBtn.addEventListener('keydown', (event) => {\n                if (event.key === 'Enter' || event.key === ' ') {\n                    event.preventDefault();\n                    mainBtn.click();\n                }\n            });\n        }\n        \n        // Legacy button support\n        const startNavBtn = document.getElementById('startNavigationBtn');\n        if (startNavBtn) {\n            startNavBtn.addEventListener('click', () => this.startListening());\n        }\n        \n        const stopNavBtn = document.getElementById('stopNavigationBtn');\n        if (stopNavBtn) {\n            stopNavBtn.addEventListener('click', () => this.stopNavigation());\n        }\n    }\n    \n    /**\n     * Update main button state based on navigation status\n     */\n    updateMainButtonState() {\n        const mainBtn = document.getElementById('mainButton');\n        if (!mainBtn) return;\n        \n        if (this.isNavigating) {\n            mainBtn.innerHTML = '<i class=\"fas fa-stop\"></i> Stop Navigation';\n            mainBtn.classList.add('navigating');\n            mainBtn.classList.remove('listening');\n        } else if (this.awaitingConfirmation) {\n            mainBtn.innerHTML = '<i class=\"fas fa-microphone\"></i> Listening...';\n            mainBtn.classList.add('listening');\n            mainBtn.classList.remove('navigating');\n        } else {\n            mainBtn.innerHTML = '<i class=\"fas fa-microphone\"></i> Start Listening';\n            mainBtn.classList.remove('listening', 'navigating');\n        }\n        \n        // Show/hide emergency stop button\n        const emergencyBtn = document.getElementById('emergencyStop');\n        if (emergencyBtn) {\n            emergencyBtn.style.display = this.isNavigating ? 'block' : 'none';\n        }\n    }\n    \n    /**\n     * Emergency stop with immediate feedback\n     */\n    emergencyStop() {\n        console.log('Emergency stop activated');\n        this.speakWithPriority('Navigation stopped immediately.', 'high');\n        this.stopNavigation();\n        this.updateMainButtonState();\n    }\n    \n    /**\n     * Start listening for navigation commands\n     */\n    startListening() {\n        if (this.awaitingConfirmation) {\n            this.confirmationRecognition.start();\n        } else {\n            this.recognition.start();\n        }\n    }\n    \n    /**\n     * Process navigation commands\n     */\n    async processNavigationCommand(command) {\n        console.log('Processing navigation command:', command);\n        \n        // Extract destination from command\n        let destination = this.extractDestination(command);\n        if (!destination) {\n            this.speak('I didn\\'t understand the destination. Please say \"take me to\" followed by a location.');\n            return;\n        }\n        \n        // Confirm navigation\n        this.currentDestination = destination;\n        this.awaitingConfirmation = true;\n        this.speak(`Should I start navigation to ${destination}?`);\n        this.updateStatusDisplay('Waiting for confirmation', 'Say \"yes\" or \"no\"');\n    }\n    \n    /**\n     * Extract destination from voice command\n     */\n    extractDestination(command) {\n        // Common patterns for navigation commands\n        const patterns = [\n            /(?:take me to|navigate to|go to|direction to|directions to)\\s+(.+)/i,\n            /(?:how to get to|where is|find)\\s+(.+)/i,\n            /(.+)/i // fallback - treat entire command as destination\n        ];\n        \n        for (const pattern of patterns) {\n            const match = command.match(pattern);\n            if (match && match[1]) {\n                return match[1].trim();\n            }\n        }\n        \n        return null;\n    }\n    \n    /**\n     * Process confirmation responses\n     */\n    async processConfirmation(response) {\n        this.awaitingConfirmation = false;\n        \n        if (response.includes('yes') || response.includes('yeah') || response.includes('start')) {\n            this.speak('Starting navigation...');\n            await this.startNavigation(this.currentDestination);\n        } else {\n            this.speak('Navigation cancelled.');\n            this.currentDestination = null;\n            this.updateStatusDisplay('Ready', 'Press button to start navigation');\n        }\n    }\n    \n    /**\n     * Enhanced navigation startup with comprehensive error handling\n     */\n    async startNavigation(destination) {\n        try {\n            // Reset navigation state\n            this.destinationReached = false;\n            this.urgentWarningGiven = false;\n            this.previewWarningGiven = false;\n            \n            if (!this.currentPosition) {\n                try {\n                    this.currentPosition = await this.getCurrentPosition();\n                } catch (error) {\n                    this.handleLocationError(error);\n                    return;\n                }\n            }\n            \n            this.updateStatusDisplay('Getting directions...', 'Please wait');\n            this.speakWithPriority('Getting directions to your destination.', 'normal');\n            \n            const origin = `${this.currentPosition.latitude},${this.currentPosition.longitude}`;\n            \n            // Call backend API to get directions\n            const response = await fetch('/api/directions', {\n                method: 'POST',\n                headers: { 'Content-Type': 'application/json' },\n                body: JSON.stringify({\n                    origin: origin,\n                    destination: destination\n                })\n            });\n            \n            const data = await response.json();\n            \n            if (!data.success) {\n                const errorMessage = data.message || 'Navigation failed. Please try again.';\n                this.errorStates.routingFailed = true;\n                this.speakErrorMessage(errorMessage);\n                this.updateStatusDisplay('Navigation Failed', errorMessage);\n                this.showErrorInUI('Route Not Found', errorMessage);\n                return;\n            }\n            \n            this.currentRoute = data;\n            this.currentStepIndex = 0;\n            this.isNavigating = true;\n            this.errorStates.routingFailed = false;\n            \n            // Start intelligent GPS tracking with battery optimization\n            this.startContinuousGPSTracking();\n            \n            // Display route on map if available\n            if (this.map && this.directionsRenderer) {\n                this.displayRouteOnMap();\n            }\n            \n            // Start navigation announcements\n            this.announceRoute();\n            \n            // Enable obstacle detection during navigation\n            this.startObstacleDetection();\n            \n            // Update UI state\n            this.updateMainButtonState();\n            \n            // Show navigation info overlay\n            const navigationInfo = document.getElementById('navigationInfo');\n            if (navigationInfo) {\n                navigationInfo.style.display = 'block';\n            }\n            \n            console.log('Enhanced navigation started successfully');\n            \n        } catch (error) {\n            console.error('Navigation start failed:', error);\n            this.errorStates.routingFailed = true;\n            const errorMessage = 'Failed to start navigation. Please check your connection and try again.';\n            this.speakErrorMessage(errorMessage);\n            this.showErrorInUI('Navigation Error', errorMessage);\n        }\n    }\n    \n    /**\n     * Announce route information\n     */\n    announceRoute() {\n        if (!this.currentRoute || !this.currentRoute.route) return;\n        \n        const route = this.currentRoute.route;\n        const totalDistance = route.distance;\n        const totalDuration = route.duration;\n        \n        this.speak(`Route found. Total distance ${totalDistance}, estimated time ${totalDuration}. Starting navigation.`);\n        \n        // Announce first step\n        setTimeout(() => {\n            this.announceCurrentStep();\n        }, 3000);\n    }\n    \n    /**\n     * Announce current navigation step with human-friendly voice instructions\n     */\n    announceCurrentStep() {\n        if (!this.isNavigating || !this.currentRoute) return;\n        \n        const steps = this.currentRoute.route.steps;\n        if (this.currentStepIndex >= steps.length) {\n            this.navigationComplete();\n            return;\n        }\n        \n        const currentStep = steps[this.currentStepIndex];\n        if (!currentStep) return;\n        \n        // Reset warning flags for new step\n        this.urgentWarningGiven = false;\n        this.previewWarningGiven = false;\n        \n        const instruction = this.optimizeVoiceInstruction(currentStep.instruction);\n        const distance = this.simplifyDistance(currentStep.distance_value || currentStep.distance_meters || 0);\n        \n        // Create clear, conversational voice instruction like \"Turn left in 20 meters\"\n        let voiceInstruction = `${instruction}`;\n        if (distance && !instruction.toLowerCase().includes('arrive')) {\n            voiceInstruction = `${instruction} in ${distance}`;\n        }\n        \n        this.speakWithPriority(voiceInstruction, 'normal');\n        this.updateStatusDisplay(`Step ${this.currentStepIndex + 1} of ${steps.length}`, instruction);\n        this.updateNavigationDisplay(instruction, distance);\n        \n        console.log(`Navigation step ${this.currentStepIndex + 1}: ${voiceInstruction}`);\n    }\n    \n    /**\n     * Update navigation display in UI\n     */\n    updateNavigationDisplay(instruction, distance) {\n        const currentStepElement = document.getElementById('currentStep');\n        const stepDistanceElement = document.getElementById('stepDistance');\n        \n        if (currentStepElement) {\n            currentStepElement.textContent = instruction;\n        }\n        \n        if (stepDistanceElement && distance) {\n            stepDistanceElement.textContent = `Distance: ${distance}`;\n        }\n        \n        // Show navigation info overlay\n        const navigationInfo = document.getElementById('navigationInfo');\n        if (navigationInfo) {\n            navigationInfo.style.display = 'block';\n        }\n    }\n    \n    /**\n     * Convert navigation data into human-friendly speech instructions\n     */\n    optimizeVoiceInstruction(instruction) {\n        let optimized = instruction.toLowerCase().trim();\n        \n        // Convert raw navigation data into conversational instructions\n        \n        // Handle turns with clear directional language\n        optimized = optimized.replace(/turn\\s+slight\\s+left/gi, 'bear left');\n        optimized = optimized.replace(/turn\\s+slight\\s+right/gi, 'bear right');\n        optimized = optimized.replace(/turn\\s+sharp\\s+left/gi, 'make a sharp left turn');\n        optimized = optimized.replace(/turn\\s+sharp\\s+right/gi, 'make a sharp right turn');\n        optimized = optimized.replace(/turn\\s+left/gi, 'turn left');\n        optimized = optimized.replace(/turn\\s+right/gi, 'turn right');\n        \n        // Handle straight movements\n        optimized = optimized.replace(/head\\s+north/gi, 'go straight ahead');\n        optimized = optimized.replace(/head\\s+south/gi, 'go straight ahead');\n        optimized = optimized.replace(/head\\s+east/gi, 'go straight ahead');\n        optimized = optimized.replace(/head\\s+west/gi, 'go straight ahead');\n        optimized = optimized.replace(/continue\\s+straight/gi, 'keep going straight');\n        optimized = optimized.replace(/proceed\\s+/gi, '');\n        optimized = optimized.replace(/continue\\s+/gi, 'keep going ');\n        \n        // Simplify common phrases\n        optimized = optimized.replace(/walk\\s+/gi, '');\n        optimized = optimized.replace(/go\\s+/gi, '');\n        optimized = optimized.replace(/head\\s+/gi, '');\n        optimized = optimized.replace(/use\\s+the\\s+/gi, 'take the ');\n        optimized = optimized.replace(/\\s+toward\\s+/gi, ' toward ');\n        optimized = optimized.replace(/\\s+towards\\s+/gi, ' toward ');\n        \n        // Handle destination phrases\n        optimized = optimized.replace(/destination\\s+will\\s+be\\s+on\\s+the\\s+/gi, 'your destination is on the ');\n        optimized = optimized.replace(/your\\s+destination\\s+is\\s+/gi, 'you will arrive ');\n        \n        // Handle street/road references\n        optimized = optimized.replace(/\\s+on\\s+([A-Za-z\\s]+)\\s+road/gi, ' on $1 Road');\n        optimized = optimized.replace(/\\s+on\\s+([A-Za-z\\s]+)\\s+street/gi, ' on $1 Street');\n        optimized = optimized.replace(/\\s+on\\s+([A-Za-z\\s]+)\\s+avenue/gi, ' on $1 Avenue');\n        \n        // Add helpful distance context\n        optimized = optimized.replace(/^(.+)$/i, (match) => {\n            // Don't repeat if already contains distance context\n            if (match.includes('meter') || match.includes('km') || match.includes('mile')) {\n                return match;\n            }\n            return match;\n        });\n        \n        // Clean up multiple spaces and capitalize\n        optimized = optimized.replace(/\\s+/g, ' ').trim();\n        optimized = optimized.charAt(0).toUpperCase() + optimized.slice(1);\n        \n        // Ensure instruction ends properly\n        if (!optimized.endsWith('.') && !optimized.endsWith('!')) {\n            optimized += '.';\n        }\n        \n        return optimized;\n    }\n    \n    /**\n     * Simplify distance for voice announcements\n     */\n    simplifyDistance(meters) {\n        if (meters < 50) {\n            return `${Math.round(meters / 10) * 10} meters`;\n        } else if (meters < 1000) {\n            return `${Math.round(meters / 50) * 50} meters`;\n        } else {\n            const km = (meters / 1000).toFixed(1);\n            return `${km} kilometers`;\n        }\n    }\n    \n    /**\n     * Start intelligent GPS tracking with battery optimization\n     */\n    startContinuousGPSTracking() {\n        if (this.watchId) {\n            navigator.geolocation.clearWatch(this.watchId);\n        }\n        \n        this.watchId = navigator.geolocation.watchPosition(\n            (position) => {\n                this.updateNavigationPosition(position.coords);\n            },\n            (error) => {\n                this.handleLocationError(error);\n                \n                // Retry GPS after error with exponential backoff\n                setTimeout(() => {\n                    if (this.isNavigating && !this.watchId) {\n                        console.log('Retrying GPS tracking after error...');\n                        this.startContinuousGPSTracking();\n                    }\n                }, 5000);\n            },\n            {\n                enableHighAccuracy: true,\n                timeout: 10000,\n                maximumAge: 0 // Always get fresh location for navigation\n            }\n        );\n        \n        console.log('Enhanced GPS tracking started with battery optimization');\n    }\n    \n    /**\n     * Calculate user speed and determine if stationary for battery optimization\n     */\n    calculateUserSpeed(newCoords) {\n        if (!this.lastLocationUpdateTime || !this.currentPosition) {\n            this.lastLocationUpdateTime = Date.now();\n            return 0;\n        }\n        \n        const now = Date.now();\n        const timeDelta = (now - this.lastLocationUpdateTime) / 1000; // seconds\n        \n        if (timeDelta < 1) return this.userSpeed; // Avoid too frequent calculations\n        \n        const distance = this.calculateDistance(\n            this.currentPosition.latitude, this.currentPosition.longitude,\n            newCoords.latitude, newCoords.longitude\n        );\n        \n        const speed = distance / timeDelta; // m/s\n        this.userSpeed = speed;\n        this.lastLocationUpdateTime = now;\n        \n        // Determine if user is stationary\n        const wasStationary = this.stationary;\n        this.stationary = speed < this.config.stationarySpeedThreshold;\n        \n        // Log speed changes for battery optimization\n        if (wasStationary !== this.stationary) {\n            console.log(`User movement changed: ${this.stationary ? 'Stationary' : 'Moving'} (Speed: ${speed.toFixed(2)} m/s)`);\n            this.optimizeTrackingFrequency();\n        }\n        \n        return speed;\n    }\n    \n    /**\n     * Optimize GPS tracking frequency based on user movement to save battery\n     */\n    optimizeTrackingFrequency() {\n        if (!this.isNavigating) return;\n        \n        // Restart tracking with optimized settings\n        if (this.watchId) {\n            navigator.geolocation.clearWatch(this.watchId);\n        }\n        \n        const trackingOptions = {\n            enableHighAccuracy: true,\n            timeout: 10000,\n            maximumAge: this.stationary ? this.config.lowFrequencyInterval : 1000\n        };\n        \n        console.log(`Optimizing GPS tracking: ${this.stationary ? 'Low' : 'High'} frequency mode`);\n        \n        this.watchId = navigator.geolocation.watchPosition(\n            (position) => {\n                this.updateNavigationPosition(position.coords);\n            },\n            (error) => {\n                this.handleLocationError(error);\n            },\n            trackingOptions\n        );\n    }\n    \n    /**\n     * Enhanced position update with destination detection and human-friendly instructions\n     */\n    updateNavigationPosition(coords) {\n        if (!this.isNavigating || !this.currentRoute) return;\n        \n        const newLat = coords.latitude;\n        const newLng = coords.longitude;\n        \n        // Calculate user speed for battery optimization\n        this.calculateUserSpeed(coords);\n        \n        // Update current position\n        this.currentPosition = coords;\n        \n        // Update map marker if available\n        if (this.map && this.userMarker) {\n            this.userMarker.setPosition({ lat: newLat, lng: newLng });\n        }\n        \n        // Check if destination is reached first (highest priority)\n        if (this.checkDestinationReached(newLat, newLng)) {\n            return; // Stop processing if destination reached\n        }\n        \n        // Check if user reached current step\n        this.checkStepProgress(newLat, newLng);\n        \n        // Check if user deviated from route\n        this.checkRouteDeviation(newLat, newLng);\n        \n        // Provide proximity-based voice instructions\n        this.provideProximityInstructions(newLat, newLng);\n    }\n    \n    /**\n     * Check if user has reached the final destination\n     */\n    checkDestinationReached(lat, lng) {\n        if (!this.currentRoute || this.destinationReached) return false;\n        \n        const steps = this.currentRoute.route.steps;\n        const finalStep = steps[steps.length - 1];\n        \n        if (!finalStep || !finalStep.end_location) return false;\n        \n        const distanceToDestination = this.calculateDistance(\n            lat, lng, \n            finalStep.end_location.lat, \n            finalStep.end_location.lng\n        );\n        \n        if (distanceToDestination <= this.config.destinationReachedThreshold) {\n            this.destinationReached = true;\n            this.navigationComplete();\n            return true;\n        }\n        \n        return false;\n    }\n    \n    /**\n     * Provide human-friendly proximity-based voice instructions\n     */\n    provideProximityInstructions(lat, lng) {\n        if (!this.currentRoute || this.currentStepIndex >= this.currentRoute.route.steps.length) return;\n        \n        const currentStep = this.currentRoute.route.steps[this.currentStepIndex];\n        if (!currentStep || !currentStep.end_location) return;\n        \n        const distanceToStepEnd = this.calculateDistance(\n            lat, lng,\n            currentStep.end_location.lat,\n            currentStep.end_location.lng\n        );\n        \n        // Provide urgent warning for immediate turns\n        if (distanceToStepEnd <= this.config.urgentAnnouncementDistance && !this.urgentWarningGiven) {\n            const instruction = this.optimizeVoiceInstruction(currentStep.instruction);\n            this.speakWithPriority(`${instruction} now!`, 'high');\n            this.urgentWarningGiven = true;\n        }\n        // Provide advance warning\n        else if (distanceToStepEnd <= this.config.voicePreviewDistance && !this.previewWarningGiven) {\n            const instruction = this.optimizeVoiceInstruction(currentStep.instruction);\n            const distance = this.simplifyDistance(distanceToStepEnd);\n            this.speakWithPriority(`${instruction} in ${distance}`, 'normal');\n            this.previewWarningGiven = true;\n        }\n    }\n    \n    /**\n     * Enhanced step progress checking with warning flag management\n     */\n    checkStepProgress(lat, lng) {\n        if (!this.currentRoute || this.currentStepIndex >= this.currentRoute.route.steps.length) return;\n        \n        const currentStep = this.currentRoute.route.steps[this.currentStepIndex];\n        if (!currentStep || !currentStep.end_location) return;\n        \n        const stepEndLat = currentStep.end_location.lat;\n        const stepEndLng = currentStep.end_location.lng;\n        \n        // Calculate distance to step endpoint\n        const distance = this.calculateDistance(lat, lng, stepEndLat, stepEndLng);\n        \n        // If within threshold of step endpoint, advance to next step\n        if (distance <= this.config.stepProximityThreshold) {\n            this.currentStepIndex++;\n            \n            // Reset warning flags for next step\n            this.urgentWarningGiven = false;\n            this.previewWarningGiven = false;\n            \n            if (this.currentStepIndex >= this.currentRoute.route.steps.length) {\n                this.navigationComplete();\n            } else {\n                // Announce next step with human-friendly instruction\n                setTimeout(() => {\n                    this.announceCurrentStep();\n                }, 1000);\n            }\n            \n            console.log(`Advanced to navigation step ${this.currentStepIndex + 1}`);\n        }\n    }\n    \n    /**\n     * Check if user has deviated significantly from the planned route\n     */\n    checkRouteDeviation(lat, lng) {\n        if (!this.currentRoute || this.currentStepIndex >= this.currentRoute.route.steps.length) return;\n        \n        const currentStep = this.currentRoute.route.steps[this.currentStepIndex];\n        const stepStartLat = currentStep.start_location.lat;\n        const stepStartLng = currentStep.start_location.lng;\n        const stepEndLat = currentStep.end_location.lat;\n        const stepEndLng = currentStep.end_location.lng;\n        \n        // Calculate distance from user to the step route line\n        const distanceToRoute = this.calculateDistanceToLine(\n            lat, lng, \n            stepStartLat, stepStartLng, \n            stepEndLat, stepEndLng\n        );\n        \n        // If user is more than 50 meters off route, trigger rerouting\n        if (distanceToRoute > 50) {\n            this.handleRouteDeviation();\n        }\n    }\n    \n    /**\n     * Handle when user deviates from planned route\n     */\n    async handleRouteDeviation() {\n        if (this.reroutingInProgress) return;\n        \n        this.reroutingInProgress = true;\n        this.speak('You seem off route. Getting updated directions...');\n        \n        try {\n            // Get new route from current position\n            await this.startNavigation(this.currentDestination);\n            this.speak('Route updated. Follow new directions.');\n        } catch (error) {\n            console.error('Rerouting failed:', error);\n            this.speak('Unable to update route. Continue to destination.');\n        } finally {\n            this.reroutingInProgress = false;\n        }\n    }\n    \n    /**\n     * Update position during navigation\n     */\n    updatePosition(newPosition) {\n        this.currentPosition = newPosition;\n        \n        if (this.userMarker && this.map) {\n            this.userMarker.setPosition({\n                lat: newPosition.latitude,\n                lng: newPosition.longitude\n            });\n            this.map.panTo({\n                lat: newPosition.latitude,\n                lng: newPosition.longitude\n            });\n        }\n        \n        // Check if user reached current step\n        this.checkStepProgress();\n    }\n    \n    /**\n     * Check if user has reached the current step\n     */\n    checkStepProgress() {\n        if (!this.isNavigating || !this.currentRoute) return;\n        \n        const steps = this.currentRoute.route.steps;\n        if (this.currentStepIndex >= steps.length) return;\n        \n        const currentStep = steps[this.currentStepIndex];\n        const targetLocation = currentStep.end_location;\n        \n        const distance = this.calculateDistance(\n            { lat: this.currentPosition.latitude, lng: this.currentPosition.longitude },\n            { lat: targetLocation.lat, lng: targetLocation.lng }\n        );\n        \n        // If within threshold, move to next step\n        if (distance <= this.config.stepProximityThreshold) {\n            this.currentStepIndex++;\n            \n            if (this.currentStepIndex >= steps.length) {\n                this.navigationComplete();\n            } else {\n                // Announce next step after a brief pause\n                setTimeout(() => {\n                    this.announceCurrentStep();\n                }, 1000);\n            }\n        }\n    }\n    \n    /**\n     * Calculate distance between two coordinates (Haversine formula)\n     */\n    calculateDistance(pos1, pos2) {\n        const R = 6371e3; // Earth's radius in meters\n        const œÜ1 = pos1.lat * Math.PI / 180;\n        const œÜ2 = pos2.lat * Math.PI / 180;\n        const ŒîœÜ = (pos2.lat - pos1.lat) * Math.PI / 180;\n        const ŒîŒª = (pos2.lng - pos1.lng) * Math.PI / 180;\n\n        const a = Math.sin(ŒîœÜ/2) * Math.sin(ŒîœÜ/2) +\n                Math.cos(œÜ1) * Math.cos(œÜ2) *\n                Math.sin(ŒîŒª/2) * Math.sin(ŒîŒª/2);\n        const c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1-a));\n\n        return R * c; // Distance in meters\n    }\n    \n    /**\n     * Display route on Google Maps\n     */\n    displayRouteOnMap() {\n        if (!this.map || !this.directionsService || !this.currentRoute) return;\n        \n        console.log('Displaying route on map');\n        \n        const steps = this.currentRoute.route.steps;\n        if (!steps || steps.length === 0) return;\n        \n        const origin = new google.maps.LatLng(\n            this.currentPosition.latitude,\n            this.currentPosition.longitude\n        );\n        \n        const destination = new google.maps.LatLng(\n            steps[steps.length - 1].end_location.lat,\n            steps[steps.length - 1].end_location.lng\n        );\n        \n        const request = {\n            origin: origin,\n            destination: destination,\n            travelMode: google.maps.TravelMode.WALKING\n        };\n        \n        this.directionsService.route(request, (result, status) => {\n            if (status === google.maps.DirectionsStatus.OK) {\n                this.directionsRenderer.setDirections(result);\n                \n                // Add user marker\n                if (!this.userMarker) {\n                    this.userMarker = new google.maps.Marker({\n                        position: origin,\n                        map: this.map,\n                        title: 'Your Location',\n                        icon: {\n                            path: google.maps.SymbolPath.CIRCLE,\n                            scale: 8,\n                            fillColor: '#4285F4',\n                            fillOpacity: 1,\n                            strokeColor: '#ffffff',\n                            strokeWeight: 2\n                        }\n                    });\n                }\n            }\n        });\n    }\n    \n    /**\n     * Start obstacle detection during navigation\n     */\n    startObstacleDetection() {\n        if (!this.model || !this.camera) return;\n        \n        this.isDetecting = true;\n        this.detectObjects();\n        console.log('Obstacle detection started during navigation');\n    }\n    \n    /**\n     * Detect objects for obstacle avoidance\n     */\n    async detectObjects() {\n        if (!this.isDetecting || !this.model || !this.camera) return;\n        \n        try {\n            const predictions = await this.model.detect(this.camera);\n            \n            // Filter for potential obstacles\n            const obstacles = predictions.filter(pred => \n                ['person', 'bicycle', 'car', 'motorcycle', 'bus', 'truck', 'traffic light', 'stop sign'].includes(pred.class) && \n                pred.score > 0.5\n            );\n            \n            if (obstacles.length > 0) {\n                const obstacleTypes = [...new Set(obstacles.map(obs => obs.class))];\n                this.speak(`Obstacle detected: ${obstacleTypes.join(', ')} ahead.`, 'high');\n            }\n            \n        } catch (error) {\n            console.error('Object detection error:', error);\n        }\n        \n        // Continue detection\n        if (this.isDetecting) {\n            setTimeout(() => this.detectObjects(), 2000);\n        }\n    }\n    \n    /**\n     * Navigation completed\n     */\n    navigationComplete() {\n        this.isNavigating = false;\n        \n        // Stop GPS tracking\n        if (this.watchId) {\n            navigator.geolocation.clearWatch(this.watchId);\n            this.watchId = null;\n        }\n        \n        // Stop obstacle detection\n        this.isDetecting = false;\n        \n        // Hide navigation controls\n        const navControls = document.getElementById('navigationControls');\n        if (navControls) {\n            navControls.style.display = 'none';\n        }\n        \n        // Announce completion\n        this.speak('You have arrived at your destination. Navigation complete.');\n        this.updateStatusDisplay('Navigation Complete', 'You have arrived at your destination');\n        \n        // Clear route data\n        this.currentRoute = null;\n        this.currentStepIndex = 0;\n        this.currentDestination = null;\n        \n        console.log('Navigation completed successfully');\n    }\n    \n    /**\n     * Enhanced navigation stop with complete state cleanup\n     */\n    stopNavigation() {\n        console.log('Stopping navigation with complete cleanup');\n        \n        this.isNavigating = false;\n        this.reroutingInProgress = false;\n        this.destinationReached = false;\n        \n        // Stop intelligent GPS tracking\n        if (this.watchId) {\n            navigator.geolocation.clearWatch(this.watchId);\n            this.watchId = null;\n        }\n        \n        // Stop obstacle detection\n        this.isDetecting = false;\n        \n        // Hide navigation UI elements\n        const navigationInfo = document.getElementById('navigationInfo');\n        if (navigationInfo) {\n            navigationInfo.style.display = 'none';\n        }\n        \n        const emergencyBtn = document.getElementById('emergencyStop');\n        if (emergencyBtn) {\n            emergencyBtn.style.display = 'none';\n        }\n        \n        // Legacy support\n        const navControls = document.getElementById('navigationControls');\n        if (navControls) {\n            navControls.style.display = 'none';\n        }\n        \n        // Clear map route\n        if (this.directionsRenderer) {\n            this.directionsRenderer.setDirections(null);\n        }\n        \n        // Cancel any current speech\n        if (this.speechSynthesis.speaking) {\n            this.speechSynthesis.cancel();\n        }\n        \n        // Announce stop\n        this.speakWithPriority('Navigation stopped.', 'high');\n        this.updateStatusDisplay('Ready to Navigate', 'Press the button or Volume Up key to start');\n        \n        // Clear route data and reset flags\n        this.currentRoute = null;\n        this.currentStepIndex = 0;\n        this.currentDestination = null;\n        this.awaitingConfirmation = false;\n        this.urgentWarningGiven = false;\n        this.previewWarningGiven = false;\n        this.userSpeed = 0;\n        this.stationary = false;\n        \n        // Update UI state\n        this.updateMainButtonState();\n        \n        console.log('Navigation stopped successfully');\n    }\n    \n    /**\n     * Calculate distance between two coordinates (Haversine formula)\n     */\n    calculateDistance(lat1, lng1, lat2, lng2) {\n        const R = 6371e3; // Earth's radius in meters\n        const œÜ1 = lat1 * Math.PI / 180;\n        const œÜ2 = lat2 * Math.PI / 180;\n        const ŒîœÜ = (lat2 - lat1) * Math.PI / 180;\n        const ŒîŒª = (lng2 - lng1) * Math.PI / 180;\n\n        const a = Math.sin(ŒîœÜ/2) * Math.sin(ŒîœÜ/2) +\n                Math.cos(œÜ1) * Math.cos(œÜ2) *\n                Math.sin(ŒîŒª/2) * Math.sin(ŒîŒª/2);\n        const c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1-a));\n\n        return R * c; // Distance in meters\n    }\n    \n    /**\n     * Calculate distance from point to line segment\n     */\n    calculateDistanceToLine(px, py, x1, y1, x2, y2) {\n        const A = px - x1;\n        const B = py - y1;\n        const C = x2 - x1;\n        const D = y2 - y1;\n\n        const dot = A * C + B * D;\n        const lenSq = C * C + D * D;\n        \n        if (lenSq === 0) {\n            return this.calculateDistance(px, py, x1, y1);\n        }\n        \n        let param = dot / lenSq;\n        \n        let xx, yy;\n        if (param < 0) {\n            xx = x1;\n            yy = y1;\n        } else if (param > 1) {\n            xx = x2;\n            yy = y2;\n        } else {\n            xx = x1 + param * C;\n            yy = y1 + param * D;\n        }\n\n        return this.calculateDistance(px, py, xx, yy);\n    }\n    \n    /**\n     * Enhanced speech synthesis with smooth overlapping voice cancellation\n     */\n    speak(text, priority = 'normal') {\n        this.speakWithPriority(text, priority);\n    }\n    \n    /**\n     * Speak with priority and overlapping voice management\n     */\n    speakWithPriority(text, priority = 'normal') {\n        console.log(`Speaking (${priority}): ${text}`);\n        \n        try {\n            // Cancel any pending speech cancellation\n            if (this.speechCancellationTimer) {\n                clearTimeout(this.speechCancellationTimer);\n                this.speechCancellationTimer = null;\n            }\n            \n            // Always cancel current speech before speaking new text\n            // This prevents overlapping voices and ensures clear communication\n            if (this.speechSynthesis.speaking || this.speechSynthesis.pending) {\n                this.speechSynthesis.cancel();\n                \n                // Small delay to ensure cancellation completes\n                setTimeout(() => {\n                    this.performSpeech(text, priority);\n                }, 100);\n            } else {\n                this.performSpeech(text, priority);\n            }\n            \n        } catch (error) {\n            console.error('Speech synthesis error:', error);\n            this.handleSpeechError(text);\n        }\n    }\n    \n    /**\n     * Perform the actual speech synthesis\n     */\n    performSpeech(text, priority = 'normal') {\n        try {\n            const utterance = new SpeechSynthesisUtterance(text);\n            \n            // Optimize speech settings for accessibility\n            utterance.rate = 0.9; // Slightly slower for clarity\n            utterance.volume = 1.0;\n            utterance.pitch = 1.0;\n            utterance.lang = 'en-US';\n            \n            // Set up event handlers\n            utterance.onstart = () => {\n                this.isSpeaking = true;\n                this.errorStates.speechFailed = false;\n                console.log('Speech started successfully');\n            };\n            \n            utterance.onend = () => {\n                this.isSpeaking = false;\n                console.log('Speech ended normally');\n            };\n            \n            utterance.onerror = (event) => {\n                console.error('Speech synthesis error:', event.error);\n                this.handleSpeechError(text);\n            };\n            \n            // Store last utterance for reference\n            this.lastUtterance = utterance;\n            \n            // Speak the text\n            this.speechSynthesis.speak(utterance);\n            \n        } catch (error) {\n            console.error('Speech creation error:', error);\n            this.handleSpeechError(text);\n        }\n    }\n    \n    /**\n     * Handle speech synthesis errors with fallback options\n     */\n    handleSpeechError(originalText) {\n        this.errorStates.speechFailed = true;\n        this.isSpeaking = false;\n        \n        console.error('Speech synthesis failed for text:', originalText);\n        \n        // Show the text in UI as fallback\n        this.showTextInUI(originalText);\n        \n        // Try to reinitialize speech synthesis\n        setTimeout(() => {\n            if ('speechSynthesis' in window) {\n                this.speechSynthesis = window.speechSynthesis;\n                console.log('Speech synthesis reinitialized after error');\n            }\n        }, 1000);\n    }\n    \n    /**\n     * Show text in UI when speech fails\n     */\n    showTextInUI(text) {\n        const textDisplay = document.getElementById('speechFallbackDisplay');\n        if (textDisplay) {\n            textDisplay.textContent = text;\n            textDisplay.style.display = 'block';\n            \n            // Auto-hide after 5 seconds\n            setTimeout(() => {\n                textDisplay.style.display = 'none';\n            }, 5000);\n        }\n    }\n    \n    /**\n     * Speak error messages with special handling\n     */\n    speakErrorMessage(errorText) {\n        // Use high priority for error messages\n        this.speakWithPriority(errorText, 'high');\n        \n        // Also show in UI for redundancy\n        this.showTextInUI(errorText);\n    }\n    \n    /**\n     * Update status display\n     */\n    updateStatusDisplay(title, subtitle) {\n        const statusTitle = document.getElementById('navigationStatus');\n        const statusSubtitle = document.getElementById('navigationSubtitle');\n        \n        if (statusTitle) statusTitle.textContent = title;\n        if (statusSubtitle) statusSubtitle.textContent = subtitle;\n    }\n}\n\n// Initialize navigation system when DOM is ready\ndocument.addEventListener('DOMContentLoaded', () => {\n    console.log('Initializing BlindMate Navigation System...');\n    window.blindMateNavigation = new UniversalNavigation();\n});\n\n// Enhanced error handling to prevent console errors\nwindow.addEventListener('error', (event) => {\n    console.error('Navigation system error:', event.error);\n});\n\nwindow.addEventListener('unhandledrejection', (event) => {\n    console.error('Navigation system unhandled promise rejection:', event.reason);\n    event.preventDefault();\n});","size_bytes":55822},"onboarding.js":{"content":"/**\n * BlindMate Onboarding Tutorial JavaScript\n * Comprehensive tutorial system for first-time users\n */\n\nclass OnboardingTutorial {\n    constructor() {\n        this.currentStep = 1;\n        this.totalSteps = 8;\n        this.audioEnabled = true;\n        this.synthesis = window.speechSynthesis;\n        this.practiceExercises = [\n            {\n                title: \"Exercise 1: Wake Word Practice\",\n                instruction: \"Say 'Hey BlindMate, start detection' and wait for a response.\",\n                expectedResponse: \"detection\",\n                successMessage: \"Perfect! You've mastered the wake word feature.\"\n            },\n            {\n                title: \"Exercise 2: Universal Navigation\",\n                instruction: \"Say 'Hey BlindMate, take me to Times Square' to practice global navigation.\",\n                expectedResponse: \"times square\",\n                successMessage: \"Excellent! You can now navigate anywhere worldwide.\"\n            },\n            {\n                title: \"Exercise 3: Voice Customization\",\n                instruction: \"Say 'Change tone to energetic' to practice voice customization.\",\n                expectedResponse: \"energetic\",\n                successMessage: \"Great! You've learned how to customize BlindMate's voice.\"\n            },\n            {\n                title: \"Exercise 4: Language Switching\",\n                instruction: \"Say 'Change language to Hindi' to practice multilingual features.\",\n                expectedResponse: \"hindi\",\n                successMessage: \"Wonderful! You can now use BlindMate in 15 different languages.\"\n            }\n        ];\n        this.currentExercise = 0;\n        this.practiceProgress = 0;\n        \n        this.init();\n    }\n    \n    init() {\n        this.updateProgressIndicator();\n        this.initializeSpeechRecognition();\n        this.speakCurrentStep();\n        \n        // Announce tutorial start\n        setTimeout(() => {\n            this.speak(\"Welcome to the BlindMate tutorial. This interactive guide will teach you how to use all features safely. You can navigate using the next and previous buttons, or use keyboard shortcuts. Press space for next, or escape to exit the tutorial.\");\n        }, 1000);\n        \n        // Keyboard shortcuts\n        document.addEventListener('keydown', (e) => {\n            switch(e.key) {\n                case ' ':\n                case 'ArrowRight':\n                    e.preventDefault();\n                    this.nextStep();\n                    break;\n                case 'ArrowLeft':\n                    e.preventDefault();\n                    this.previousStep();\n                    break;\n                case 'Escape':\n                    this.exitTutorial();\n                    break;\n                case 'r':\n                case 'R':\n                    if (e.ctrlKey) {\n                        e.preventDefault();\n                        this.repeatCurrentStep();\n                    }\n                    break;\n            }\n        });\n    }\n    \n    initializeSpeechRecognition() {\n        if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {\n            console.warn('Speech recognition not available');\n            return;\n        }\n        \n        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n        this.recognition = new SpeechRecognition();\n        \n        this.recognition.continuous = false;\n        this.recognition.interimResults = false;\n        this.recognition.lang = 'en-US';\n        \n        this.recognition.onresult = (event) => {\n            const command = event.results[0][0].transcript.toLowerCase().trim();\n            this.handleVoiceCommand(command);\n        };\n        \n        this.recognition.onerror = (event) => {\n            console.error('Speech recognition error:', event.error);\n        };\n    }\n    \n    handleVoiceCommand(command) {\n        // Tutorial navigation commands\n        if (command.includes('next') || command.includes('continue')) {\n            this.nextStep();\n        } else if (command.includes('previous') || command.includes('back')) {\n            this.previousStep();\n        } else if (command.includes('repeat')) {\n            this.repeatCurrentStep();\n        } else if (command.includes('exit') || command.includes('quit')) {\n            this.exitTutorial();\n        }\n        \n        // Practice exercise handling\n        if (this.currentStep === 7 && this.isPracticing) {\n            this.handlePracticeCommand(command);\n        }\n    }\n    \n    updateProgressIndicator() {\n        const progressDots = document.getElementById('progressDots');\n        const progressText = document.getElementById('progressText');\n        \n        // Create progress dots\n        progressDots.innerHTML = '';\n        for (let i = 1; i <= this.totalSteps; i++) {\n            const dot = document.createElement('div');\n            dot.className = 'progress-dot';\n            \n            if (i < this.currentStep) {\n                dot.classList.add('completed');\n            } else if (i === this.currentStep) {\n                dot.classList.add('active');\n            }\n            \n            progressDots.appendChild(dot);\n        }\n        \n        progressText.textContent = `Step ${this.currentStep} of ${this.totalSteps}`;\n    }\n    \n    showStep(stepNumber) {\n        // Hide all steps\n        document.querySelectorAll('.step-card').forEach(card => {\n            card.classList.remove('active');\n        });\n        \n        // Show current step\n        const currentStepCard = document.querySelector(`[data-step=\"${stepNumber}\"]`);\n        if (currentStepCard) {\n            currentStepCard.classList.add('active');\n        }\n        \n        this.updateProgressIndicator();\n        this.updateNavigationButtons();\n        \n        // Announce step change\n        setTimeout(() => {\n            this.speakCurrentStep();\n        }, 500);\n    }\n    \n    speakCurrentStep() {\n        if (!this.audioEnabled) return;\n        \n        const currentStepCard = document.querySelector(`[data-step=\"${this.currentStep}\"]`);\n        if (!currentStepCard) return;\n        \n        const title = currentStepCard.querySelector('.step-title').textContent;\n        const content = currentStepCard.querySelector('.step-content p').textContent;\n        \n        const announcement = `Step ${this.currentStep} of ${this.totalSteps}: ${title}. ${content}`;\n        this.speak(announcement);\n    }\n    \n    nextStep() {\n        if (this.currentStep < this.totalSteps) {\n            this.currentStep++;\n            this.showStep(this.currentStep);\n            \n            // Special handling for practice step\n            if (this.currentStep === 7) {\n                this.initializePracticeSession();\n            }\n        }\n    }\n    \n    previousStep() {\n        if (this.currentStep > 1) {\n            this.currentStep--;\n            this.showStep(this.currentStep);\n        }\n    }\n    \n    updateNavigationButtons() {\n        const prevButton = document.getElementById('prevButton');\n        const nextButton = document.getElementById('nextButton');\n        \n        prevButton.disabled = this.currentStep === 1;\n        \n        if (this.currentStep === this.totalSteps) {\n            nextButton.innerHTML = '<i class=\"fas fa-check\"></i> Complete';\n            nextButton.onclick = () => this.completeTutorial();\n        } else {\n            nextButton.innerHTML = 'Next <i class=\"fas fa-chevron-right\"></i>';\n            nextButton.onclick = () => this.nextStep();\n        }\n    }\n    \n    initializePracticeSession() {\n        this.currentExercise = 0;\n        this.practiceProgress = 0;\n        this.updatePracticeExercise();\n        \n        // Show practice progress\n        const practiceProgressDiv = document.getElementById('practiceProgress');\n        practiceProgressDiv.style.display = 'block';\n    }\n    \n    updatePracticeExercise() {\n        if (this.currentExercise >= this.practiceExercises.length) {\n            this.completePracticeSession();\n            return;\n        }\n        \n        const exercise = this.practiceExercises[this.currentExercise];\n        \n        document.getElementById('exerciseTitle').textContent = exercise.title;\n        document.getElementById('exerciseInstruction').textContent = exercise.instruction;\n        \n        const progressBar = document.getElementById('practiceProgressBar');\n        const progressPercent = (this.currentExercise / this.practiceExercises.length) * 100;\n        progressBar.style.width = `${progressPercent}%`;\n        \n        document.getElementById('practiceStatus').textContent = `Exercise ${this.currentExercise + 1} of ${this.practiceExercises.length}`;\n        \n        // Reset practice button\n        const practiceButton = document.getElementById('practiceButton');\n        practiceButton.innerHTML = '<i class=\"fas fa-microphone\"></i> Start Practice';\n        practiceButton.onclick = () => this.startPractice();\n        \n        this.isPracticing = false;\n        \n        this.speak(exercise.instruction);\n    }\n    \n    startPractice() {\n        if (!this.recognition) {\n            this.speak('Speech recognition is not available in your browser.');\n            return;\n        }\n        \n        this.isPracticing = true;\n        const practiceButton = document.getElementById('practiceButton');\n        practiceButton.innerHTML = '<i class=\"fas fa-stop\"></i> Stop Practice';\n        practiceButton.onclick = () => this.stopPractice();\n        \n        document.getElementById('practiceStatus').textContent = 'Listening... Speak now';\n        \n        this.speak('I\\'m listening. Please speak your command now.');\n        \n        setTimeout(() => {\n            try {\n                this.recognition.start();\n            } catch (error) {\n                console.error('Speech recognition error:', error);\n                this.speak('Unable to start speech recognition. Please try again.');\n                this.stopPractice();\n            }\n        }, 2000);\n    }\n    \n    stopPractice() {\n        this.isPracticing = false;\n        if (this.recognition) {\n            this.recognition.stop();\n        }\n        \n        const practiceButton = document.getElementById('practiceButton');\n        practiceButton.innerHTML = '<i class=\"fas fa-microphone\"></i> Start Practice';\n        practiceButton.onclick = () => this.startPractice();\n        \n        document.getElementById('practiceStatus').textContent = 'Practice stopped';\n    }\n    \n    handlePracticeCommand(command) {\n        const exercise = this.practiceExercises[this.currentExercise];\n        \n        if (command.includes(exercise.expectedResponse)) {\n            this.speak(exercise.successMessage);\n            document.getElementById('practiceStatus').textContent = 'Success! Moving to next exercise...';\n            \n            setTimeout(() => {\n                this.currentExercise++;\n                this.updatePracticeExercise();\n            }, 2000);\n        } else {\n            this.speak('Good try! Let\\'s practice that command again. ' + exercise.instruction);\n            document.getElementById('practiceStatus').textContent = 'Try again - listen for the instruction';\n        }\n        \n        this.stopPractice();\n    }\n    \n    skipExercise() {\n        this.speak('Skipping exercise');\n        this.currentExercise++;\n        this.updatePracticeExercise();\n    }\n    \n    completePracticeSession() {\n        const progressBar = document.getElementById('practiceProgressBar');\n        progressBar.style.width = '100%';\n        \n        document.getElementById('practiceStatus').textContent = 'All exercises completed!';\n        document.getElementById('exerciseTitle').textContent = 'Practice Session Complete';\n        document.getElementById('exerciseInstruction').textContent = 'Congratulations! You\\'ve practiced all the essential BlindMate commands.';\n        \n        const practiceButton = document.getElementById('practiceButton');\n        practiceButton.innerHTML = '<i class=\"fas fa-check\"></i> Practice Complete';\n        practiceButton.disabled = true;\n        \n        this.speak('Excellent work! You\\'ve completed all practice exercises and are ready to use BlindMate confidently.');\n    }\n    \n    repeatCurrentStep() {\n        this.speakCurrentStep();\n    }\n    \n    toggleAudio() {\n        this.audioEnabled = !this.audioEnabled;\n        const audioToggle = document.getElementById('audioToggle');\n        \n        if (this.audioEnabled) {\n            audioToggle.innerHTML = '<i class=\"fas fa-volume-up\"></i> Audio On';\n            this.speak('Audio enabled');\n        } else {\n            audioToggle.innerHTML = '<i class=\"fas fa-volume-mute\"></i> Audio Off';\n            this.synthesis.cancel();\n        }\n    }\n    \n    speak(text, priority = false) {\n        if (!this.audioEnabled) return;\n        \n        if (priority) {\n            this.synthesis.cancel();\n        }\n        \n        const utterance = new SpeechSynthesisUtterance(text);\n        utterance.rate = 0.8;\n        utterance.pitch = 1;\n        utterance.volume = 0.9;\n        \n        // Use a clear, friendly voice if available\n        const voices = this.synthesis.getVoices();\n        const englishVoice = voices.find(voice => \n            voice.lang.startsWith('en') && voice.name.includes('Female')\n        ) || voices.find(voice => voice.lang.startsWith('en'));\n        \n        if (englishVoice) {\n            utterance.voice = englishVoice;\n        }\n        \n        this.synthesis.speak(utterance);\n    }\n    \n    completeTutorial() {\n        this.speak('Congratulations! You have completed the BlindMate tutorial. You are now ready to navigate with confidence. Would you like to launch BlindMate now?');\n        \n        // Store tutorial completion\n        localStorage.setItem('blindmate_tutorial_completed', 'true');\n        localStorage.setItem('blindmate_tutorial_date', new Date().toISOString());\n        \n        setTimeout(() => {\n            if (confirm('Launch BlindMate now?')) {\n                this.launchBlindMate();\n            }\n        }, 3000);\n    }\n    \n    launchBlindMate() {\n        this.speak('Launching BlindMate. Welcome to your navigation assistant!');\n        setTimeout(() => {\n            window.location.href = '/';\n        }, 2000);\n    }\n    \n    restartTutorial() {\n        if (confirm('Are you sure you want to restart the tutorial from the beginning?')) {\n            this.currentStep = 1;\n            this.currentExercise = 0;\n            this.practiceProgress = 0;\n            this.showStep(1);\n            this.speak('Tutorial restarted. Welcome back to the BlindMate tutorial.');\n        }\n    }\n    \n    exitTutorial() {\n        if (confirm('Are you sure you want to exit the tutorial?')) {\n            this.speak('Exiting tutorial. Goodbye!');\n            setTimeout(() => {\n                window.location.href = '/';\n            }, 1500);\n        }\n    }\n}\n\n// Demo functions for button interactions\nfunction speakExample(text) {\n    const tutorial = window.tutorialInstance;\n    tutorial.speak(`Example command: ${text}`);\n}\n\nfunction playDetectionDemo() {\n    const tutorial = window.tutorialInstance;\n    tutorial.speak('Enhanced detection demo with anti-overlap technology: Person ahead at 3 meters. Pausing for clarity. Car approaching from the right. Smart delay prevents voice overlap. Chair to your left with distance awareness.');\n}\n\nfunction playNavigationDemo() {\n    const tutorial = window.tutorialInstance;\n    tutorial.speak('Universal navigation demo: Should I start navigation to Times Square, New York? Route calculated using Google Maps. Distance: 2.5 kilometers. Battery-optimized GPS tracking enabled. Turn left in 50 meters onto Broadway. Smart rerouting available if you deviate. Automatic arrival detection when within 10 meters. You have arrived at Times Square.');\n}\n\nfunction playEmergencyDemo() {\n    const tutorial = window.tutorialInstance;\n    tutorial.speak('Emergency demo: Emergency mode activated. Broadcasting location. Sending alerts to emergency contacts. Loud audio beacon active. Emergency services have been notified of your location.');\n}\n\nfunction startPractice() {\n    const tutorial = window.tutorialInstance;\n    tutorial.startPractice();\n}\n\nfunction skipExercise() {\n    const tutorial = window.tutorialInstance;\n    tutorial.skipExercise();\n}\n\nfunction nextStep() {\n    const tutorial = window.tutorialInstance;\n    tutorial.nextStep();\n}\n\nfunction previousStep() {\n    const tutorial = window.tutorialInstance;\n    tutorial.previousStep();\n}\n\nfunction toggleAudio() {\n    const tutorial = window.tutorialInstance;\n    tutorial.toggleAudio();\n}\n\nfunction launchBlindMate() {\n    const tutorial = window.tutorialInstance;\n    tutorial.launchBlindMate();\n}\n\nfunction restartTutorial() {\n    const tutorial = window.tutorialInstance;\n    tutorial.restartTutorial();\n}\n\n// Initialize tutorial when page loads\ndocument.addEventListener('DOMContentLoaded', () => {\n    window.tutorialInstance = new OnboardingTutorial();\n});\n\n// Service worker registration for offline access\nif ('serviceWorker' in navigator) {\n    navigator.serviceWorker.register('/sw.js').catch(console.error);\n}","size_bytes":17231},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"email-validator>=2.2.0\",\n    \"flask-cors>=6.0.1\",\n    \"flask>=3.1.1\",\n    \"flask-sqlalchemy>=3.1.1\",\n    \"google-genai>=1.27.0\",\n    \"gunicorn>=23.0.0\",\n    \"psycopg2-binary>=2.9.10\",\n    \"sift-stack-py>=0.7.0\",\n    \"requests>=2.32.4\",\n]\n","size_bytes":385},"replit.md":{"content":"# BlindMate - AI Assistant for Visually Impaired Users\n\n## Overview\n\nBlindMate is a comprehensive web-based assistive technology application designed to empower visually impaired users with intelligent navigation and real-time environmental awareness. The system combines computer vision for object detection, voice-activated AI assistance, and GPS-based navigation to provide a complete accessibility solution. Users can interact entirely through voice commands, receive audio feedback about their surroundings, and navigate to any location worldwide using walking directions with obstacle detection.\n\n## User Preferences\n\nPreferred communication style: Simple, everyday language.\n\n## System Architecture\n\n### Frontend Architecture\nThe application uses a vanilla JavaScript approach with modular class-based components. The main application (`BlindMate` class) handles camera access, object detection, and voice synthesis. A separate `UniversalNavigation` class manages GPS tracking and turn-by-turn navigation. The UI is built with Bootstrap 5 for responsive design and accessibility, featuring high contrast themes, large buttons, and keyboard navigation support. TensorFlow.js runs the COCO-SSD model client-side for real-time object detection without server dependencies.\n\n### Voice Recognition System\nImplements Web Speech API for continuous voice command recognition with wake word detection (\"Hey BlindMate\"). The system maintains separate recognition instances for commands and navigation confirmations, with built-in error handling and automatic restart capabilities. Speech synthesis includes intelligent queueing to prevent overlapping audio, priority-based announcements, and multilingual support for 7 Indian languages plus English.\n\n### Real-time Object Detection & Obstacle Alert System\nUses TensorFlow.js with COCO-SSD model to detect objects in webcam feed. Features a comprehensive Real-time Obstacle Alert System that automatically warns users about obstacles during navigation. Critical obstacles (people, vehicles, bicycles) and warning obstacles (furniture, barriers) are detected with confidence thresholds and distance estimation. The system provides intelligent audio alerts with position information (left, center, right) and distance levels (very close, close, medium, far). Alerts are throttled with a 3-second cooldown and persistence tracking to prevent spam while ensuring safety.\n\n### Navigation System\nGPS-based navigation uses `navigator.geolocation.watchPosition()` for continuous location tracking with adaptive frequency based on user movement speed. The system integrates with Google Directions API for route calculation and provides step-by-step voice guidance. Features automatic rerouting when users deviate from the planned path, destination proximity detection, and battery optimization through dynamic tracking intervals. Enhanced with four navigation control features: Show Navigation Map (visual route display), Emergency Stop Navigation (immediate cancellation), Test Voice Recognition (microphone testing), and Toggle Obstacle Alerts (safety system control).\n\n### Backend Architecture\nFlask-based REST API handles AI command processing and external API integration. The `GeminiService` class manages Google Gemini AI interactions for natural language command interpretation, returning structured JSON responses. The backend serves static files, processes voice commands, and provides secure API key management for Google services. Session management maintains user language and tone preferences across interactions.\n\n### Progressive Web App Features\nIncludes service worker for offline caching of core resources, app manifest for PWA installation, and background sync capabilities. The application can function partially offline with cached TensorFlow models and stored user preferences.\n\n## External Dependencies\n\n### AI and Machine Learning Services\n- **Google Gemini AI**: Natural language processing for voice command interpretation with multilingual support\n- **TensorFlow.js**: Client-side machine learning framework for object detection\n- **COCO-SSD Model**: Pre-trained object detection model for 80+ object classes\n\n### Google Cloud Services\n- **Google Directions API**: Walking route calculation and turn-by-turn navigation\n- **Google Geocoding API**: Address to coordinates conversion for universal destination support\n- **Google Maps JavaScript API**: Interactive maps and location visualization\n\n### Web APIs and Browser Features\n- **Web Speech API**: Browser-native speech recognition and synthesis\n- **MediaDevices API**: Webcam access for object detection\n- **Geolocation API**: GPS tracking for navigation and location services\n- **Service Worker API**: PWA capabilities and offline functionality\n\n### Frontend Libraries\n- **Bootstrap 5**: Responsive UI framework with accessibility features\n- **Font Awesome**: Icon library for visual indicators\n- **HTML5 Canvas API**: Real-time rendering of object detection overlays\n\n### Python Backend Dependencies\n- **Flask**: Web application framework with CORS support\n- **Google Generative AI**: Python SDK for Gemini API integration\n- **Requests**: HTTP client for external API calls\n- **Gunicorn**: WSGI server for production deployment","size_bytes":5246},"styles.css":{"content":"/* BlindMate Custom Styles */\n\n/* High contrast and accessibility improvements */\n:root {\n    --text-size-large: 1.25rem;\n    --text-size-xlarge: 1.5rem;\n    --border-radius-large: 0.75rem;\n    --shadow-strong: 0 4px 6px -1px rgba(0, 0, 0, 0.4);\n}\n\n/* Base accessibility improvements */\nbody {\n    font-size: var(--text-size-large);\n    line-height: 1.6;\n    min-height: 100vh;\n    display: flex;\n    flex-direction: column;\n}\n\n/* Enhanced focus indicators for keyboard navigation */\nbutton:focus,\nselect:focus,\ninput:focus {\n    outline: 3px solid var(--bs-warning) !important;\n    outline-offset: 2px !important;\n    box-shadow: 0 0 0 0.2rem rgba(255, 193, 7, 0.25) !important;\n}\n\n/* Large, accessible buttons */\n.btn-lg {\n    padding: 1rem 2rem;\n    font-size: var(--text-size-xlarge);\n    font-weight: 600;\n    border-radius: var(--border-radius-large);\n    min-height: 60px;\n    box-shadow: var(--shadow-strong);\n    transition: all 0.3s ease;\n}\n\n.btn-lg:hover {\n    transform: translateY(-2px);\n    box-shadow: 0 6px 8px -1px rgba(0, 0, 0, 0.5);\n}\n\n.btn-lg:active {\n    transform: translateY(0);\n}\n\n/* Video container styling */\n#webcam {\n    border-radius: var(--border-radius-large);\n    min-height: 400px;\n    background-color: var(--bs-dark);\n}\n\n#canvas {\n    border-radius: var(--border-radius-large);\n    pointer-events: none;\n}\n\n/* Loading overlay */\n#loadingOverlay {\n    background-color: rgba(0, 0, 0, 0.8);\n    border-radius: var(--border-radius-large);\n    padding: 2rem;\n    color: white;\n    z-index: 10;\n}\n\n/* Status indicators */\n.badge {\n    font-size: 1rem;\n    padding: 0.5rem 1rem;\n}\n\n/* System status alerts */\n#systemStatus {\n    font-size: var(--text-size-large);\n    border-radius: var(--border-radius-large);\n    border: 2px solid;\n}\n\n/* Language selector */\n#languageSelect {\n    font-size: var(--text-size-large);\n    padding: 0.75rem 1rem;\n    border-radius: var(--border-radius-large);\n}\n\n/* Voice status indicator */\n#voiceStatus {\n    border-radius: var(--border-radius-large);\n    border: 2px solid var(--bs-primary);\n}\n\n/* Animation for listening indicator */\n@keyframes pulse {\n    0%, 100% {\n        opacity: 1;\n    }\n    50% {\n        opacity: 0.5;\n    }\n}\n\n.animate-pulse {\n    animation: pulse 2s infinite;\n}\n\n/* Card styling improvements */\n.card {\n    border-radius: var(--border-radius-large);\n    box-shadow: var(--shadow-strong);\n    border: 2px solid var(--bs-border-color);\n}\n\n.card-header {\n    border-radius: var(--border-radius-large) var(--border-radius-large) 0 0;\n    font-weight: 600;\n    font-size: var(--text-size-xlarge);\n}\n\n/* Header styling */\nheader h1 {\n    text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5);\n}\n\n/* Enhanced Navigation Styles */\n.navigation-display {\n    position: sticky;\n    top: 20px;\n    z-index: 100;\n}\n\n.navigation-display .card {\n    box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);\n    border: 2px solid var(--bs-primary);\n}\n\n.navigation-display .card-header {\n    font-weight: 600;\n    background: linear-gradient(45deg, var(--bs-primary), var(--bs-info));\n}\n\n.navigation-display .progress {\n    background-color: rgba(255, 255, 255, 0.2);\n    height: 8px;\n}\n\n.navigation-display .progress-bar {\n    background: linear-gradient(90deg, var(--bs-success), var(--bs-info));\n    transition: width 0.3s ease;\n}\n\n/* Mobile-specific navigation styles */\n@media (max-width: 768px) {\n    .navigation-display {\n        position: fixed;\n        bottom: 0;\n        left: 0;\n        right: 0;\n        top: auto;\n        z-index: 1050;\n        margin: 10px;\n        border-radius: 15px 15px 0 0;\n    }\n    \n    .navigation-display .card {\n        margin: 0;\n        border-radius: 15px 15px 0 0;\n    }\n    \n    /* Ensure main content doesn't hide behind fixed navigation */\n    body.navigation-active {\n        padding-bottom: 220px;\n    }\n    \n    /* Larger touch targets on mobile */\n    .btn-lg {\n        min-height: 70px;\n        font-size: 1.3rem;\n    }\n}\n\n/* Emergency stop button styling */\n#emergencyStop {\n    background: linear-gradient(45deg, var(--bs-danger), #a71e2a);\n    border: 2px solid var(--bs-danger);\n    font-weight: bold;\n    box-shadow: 0 2px 8px rgba(220, 53, 69, 0.3);\n    color: white;\n}\n\n#emergencyStop:hover {\n    background: linear-gradient(45deg, #a71e2a, #7a1719);\n    box-shadow: 0 4px 12px rgba(220, 53, 69, 0.4);\n    transform: translateY(-1px);\n}\n\n/* Enhanced detection indicator */\n.detection-indicator {\n    position: absolute;\n    top: 10px;\n    right: 10px;\n    background: rgba(0, 0, 0, 0.9);\n    color: white;\n    padding: 8px 12px;\n    border-radius: 20px;\n    font-size: 14px;\n    font-weight: 600;\n    display: flex;\n    align-items: center;\n    gap: 5px;\n    animation: pulse-detection 2s infinite;\n    border: 2px solid var(--bs-success);\n}\n\n@keyframes pulse-detection {\n    0% { opacity: 1; transform: scale(1); }\n    50% { opacity: 0.8; transform: scale(1.05); }\n    100% { opacity: 1; transform: scale(1); }\n}\n\n/* Voice feedback visual indicators */\n.voice-listening {\n    background: linear-gradient(45deg, var(--bs-warning), #e0a800);\n    animation: listening-pulse 1.5s ease-in-out infinite;\n    border-color: var(--bs-warning);\n}\n\n@keyframes listening-pulse {\n    0%, 100% { transform: scale(1); box-shadow: 0 0 0 0 rgba(255, 193, 7, 0.4); }\n    50% { transform: scale(1.02); box-shadow: 0 0 0 10px rgba(255, 193, 7, 0); }\n}\n\n.voice-speaking {\n    background: linear-gradient(45deg, var(--bs-info), #138496);\n    animation: speaking-wave 0.8s ease-in-out infinite;\n    border-color: var(--bs-info);\n}\n\n@keyframes speaking-wave {\n    0%, 100% { opacity: 1; }\n    25% { opacity: 0.9; }\n    50% { opacity: 0.8; }\n    75% { opacity: 0.9; }\n}\n\n/* Navigation step indicators */\n#currentInstruction {\n    font-size: 1.1rem;\n    line-height: 1.4;\n    color: var(--bs-dark);\n}\n\n#stepDistance {\n    color: var(--bs-primary);\n    font-weight: 700;\n}\n\n/* High contrast mode for better accessibility */\n@media (prefers-contrast: high) {\n    .card {\n        border: 3px solid #000000;\n        background: #ffffff;\n        color: #000000;\n    }\n    \n    .btn {\n        border: 3px solid;\n        font-weight: bold;\n    }\n    \n    .badge {\n        border: 2px solid #000000;\n        color: #000000;\n    }\n    \n    .navigation-display .card-header {\n        background: #000000;\n        color: #ffffff;\n    }\n}\n\n/* Reduced motion for accessibility */\n@media (prefers-reduced-motion: reduce) {\n    *, *::before, *::after {\n        animation-duration: 0.01ms !important;\n        animation-iteration-count: 1 !important;\n        transition-duration: 0.01ms !important;\n    }\n    \n    .btn-lg:hover {\n        transform: none;\n    }\n}\n\n/* Responsive video container */\n@media (max-width: 992px) {\n    .card-body {\n        min-height: 300px;\n    }\n    \n    #webcam {\n        min-height: 300px;\n    }\n}\n\n@media (max-width: 576px) {\n    .btn-lg {\n        padding: 0.75rem 1.5rem;\n        font-size: var(--text-size-large);\n        min-height: 50px;\n    }\n    \n    .card-body {\n        min-height: 250px;\n    }\n    \n    #webcam {\n        min-height: 250px;\n    }\n}\n\n/* High contrast mode support */\n@media (prefers-contrast: high) {\n    .btn {\n        border-width: 3px;\n    }\n    \n    .card {\n        border-width: 3px;\n    }\n    \n    #systemStatus {\n        border-width: 3px;\n    }\n}\n\n/* Reduced motion support */\n@media (prefers-reduced-motion: reduce) {\n    .btn-lg {\n        transition: none;\n    }\n    \n    .btn-lg:hover {\n        transform: none;\n    }\n    \n    .animate-pulse {\n        animation: none;\n    }\n}\n\n/* Screen reader only content */\n.sr-only {\n    position: absolute;\n    width: 1px;\n    height: 1px;\n    padding: 0;\n    margin: -1px;\n    overflow: hidden;\n    clip: rect(0, 0, 0, 0);\n    white-space: nowrap;\n    border: 0;\n}\n\n/* Canvas overlay for object detection */\n#canvas {\n    position: absolute !important;\n    top: 0 !important;\n    left: 0 !important;\n    width: 100% !important;\n    height: 100% !important;\n    pointer-events: none !important;\n    z-index: 10 !important;\n}\n\n/* Video and canvas container */\n.card-body {\n    position: relative !important;\n    overflow: hidden !important;\n}\n\n/* Object detection visual indicators */\n.detection-indicator {\n    position: absolute;\n    top: 10px;\n    right: 10px;\n    background: rgba(0, 255, 0, 0.8);\n    color: black;\n    padding: 5px 10px;\n    border-radius: 5px;\n    font-weight: bold;\n    font-size: 12px;\n    z-index: 15;\n}\n\n.detection-indicator.active {\n    animation: pulse-green 2s infinite;\n}\n\n@keyframes pulse-green {\n    0%, 100% { background: rgba(0, 255, 0, 0.8); }\n    50% { background: rgba(0, 255, 0, 1); }\n}\n\n/* Navigation directions display */\n.directions-panel {\n    position: fixed;\n    top: 50%;\n    left: 50%;\n    transform: translate(-50%, -50%);\n    background-color: var(--bs-dark);\n    color: white;\n    padding: 2rem;\n    border-radius: var(--border-radius-large);\n    box-shadow: var(--shadow-strong);\n    z-index: 1000;\n    max-width: 90vw;\n    max-height: 80vh;\n    overflow-y: auto;\n}\n\n/* Ensure proper contrast for all text */\n.text-light {\n    color: #ffffff !important;\n}\n\n.text-dark {\n    color: #000000 !important;\n}\n\n/* Universal Navigation Styles */\n.main-nav-button {\n    min-height: 60px;\n    font-size: 1.2rem;\n    font-weight: 600;\n    transition: all 0.3s ease;\n}\n\n.main-nav-button.listening {\n    background-color: #dc3545 !important;\n    border-color: #dc3545 !important;\n    animation: pulse 1.5s infinite;\n}\n\n.main-nav-button.navigating {\n    background-color: #198754 !important;\n    border-color: #198754 !important;\n}\n\n@keyframes pulse {\n    0% { box-shadow: 0 0 0 0 rgba(220, 53, 69, 0.7); }\n    70% { box-shadow: 0 0 0 10px rgba(220, 53, 69, 0); }\n    100% { box-shadow: 0 0 0 0 rgba(220, 53, 69, 0); }\n}\n\n.navigation-container {\n    position: relative;\n    z-index: 100;\n}\n\n.navigation-instruction {\n    background-color: #f8f9fa !important;\n    border: 2px solid #007bff;\n    border-radius: 8px;\n    padding: 15px;\n    font-size: 1.4rem;\n    font-weight: 700;\n    color: #000 !important;\n    line-height: 1.4;\n}\n","size_bytes":10040},"sw.js":{"content":"/**\n * Service Worker for BlindMate PWA\n * Provides offline capabilities and caching\n */\n\nconst CACHE_NAME = 'blindmate-v1';\nconst urlsToCache = [\n    '/',\n    '/navigation.js',\n    '/styles.css',\n    'https://cdn.replit.com/agent/bootstrap-agent-dark-theme.min.css',\n    'https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css',\n    'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.17.0/dist/tf.min.js',\n    'https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd@2.2.2/dist/coco-ssd.min.js'\n];\n\n// Install event - cache resources\nself.addEventListener('install', (event) => {\n    event.waitUntil(\n        caches.open(CACHE_NAME)\n            .then((cache) => {\n                console.log('Opened cache');\n                return cache.addAll(urlsToCache);\n            })\n    );\n});\n\n// Fetch event - serve cached content when offline\nself.addEventListener('fetch', (event) => {\n    event.respondWith(\n        caches.match(event.request)\n            .then((response) => {\n                // Return cached version or fetch from network\n                return response || fetch(event.request);\n            })\n    );\n});\n\n// Activate event - clean up old caches\nself.addEventListener('activate', (event) => {\n    event.waitUntil(\n        caches.keys().then((cacheNames) => {\n            return Promise.all(\n                cacheNames.map((cacheName) => {\n                    if (cacheName !== CACHE_NAME) {\n                        console.log('Deleting old cache:', cacheName);\n                        return caches.delete(cacheName);\n                    }\n                })\n            );\n        })\n    );\n});","size_bytes":1630},"attached_assets/content-1754815736805.md":{"content":"```json\n{\n   \"error_message\" : \"The provided API key is invalid. \",\n   \"routes\" : [],\n   \"status\" : \"REQUEST_DENIED\"\n}\n\n```","size_bytes":123},"static/js/app.js":{"content":"/**\n * BlindMate - AI Assistant for Visually Impaired Users\n * Main Application JavaScript\n */\n\nclass BlindMate {\n    constructor() {\n        // Core properties\n        this.video = document.getElementById('webcam');\n        this.canvas = document.getElementById('canvas');\n        this.ctx = this.canvas.getContext('2d');\n        this.model = null;\n        this.isDetecting = false;\n        this.stream = null;\n        this.currentLanguage = 'en-IN';\n        this.currentTone = 'friendly';\n        this.userLocation = null;\n        \n        // Voice synthesis and recognition\n        this.synth = window.speechSynthesis;\n        this.recognition = null;\n        this.isListening = false;\n        \n        // UI elements\n        this.elements = {\n            startBtn: document.getElementById('startDetectionBtn'),\n            stopBtn: document.getElementById('stopDetectionBtn'),\n            voiceBtn: document.getElementById('voiceCommandBtn'),\n            locationBtn: document.getElementById('locationBtn'),\n            languageSelect: document.getElementById('languageSelect'),\n            toneSelect: document.getElementById('toneSelect'),\n            systemStatus: document.getElementById('systemStatus'),\n            detectionStatus: document.getElementById('detectionStatus'),\n            voiceStatus: document.getElementById('voiceStatus'),\n            loadingOverlay: document.getElementById('loadingOverlay'),\n            detectionIndicator: document.getElementById('detectionIndicator')\n        };\n        \n        // Language configurations\n        this.languages = {\n            'en-IN': { name: 'English', voice: 'en-IN', greeting: 'Hello! Should I start detection, Sir?' },\n            'hi-IN': { name: 'Hindi', voice: 'hi-IN', greeting: '‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§ï‡•ç‡§Ø‡§æ ‡§Æ‡•à‡§Ç ‡§°‡§ø‡§ü‡•á‡§ï‡•ç‡§∂‡§® ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§∞‡•Ç‡§Ç, ‡§∏‡§∞?' },\n            'ta-IN': { name: 'Tamil', voice: 'ta-IN', greeting: '‡Æµ‡Æ£‡Æï‡Øç‡Æï‡ÆÆ‡Øç! ‡Æ®‡Ææ‡Æ©‡Øç ‡Æï‡Æ£‡Øç‡Æü‡Æ±‡Æø‡Æ§‡Æ≤‡Øà‡Æ§‡Øç ‡Æ§‡Øä‡Æü‡Æô‡Øç‡Æï ‡Æµ‡Øá‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Ææ, ‡Æê‡ÆØ‡Ææ?' },\n            'te-IN': { name: 'Telugu', voice: 'te-IN', greeting: '‡∞®‡∞Æ‡∞∏‡±ç‡∞ï‡∞æ‡∞∞‡∞Ç! ‡∞®‡±á‡∞®‡±Å ‡∞ó‡±Å‡∞∞‡±ç‡∞§‡∞ø‡∞Ç‡∞™‡±Å‡∞®‡±Å ‡∞™‡±ç‡∞∞‡∞æ‡∞∞‡∞Ç‡∞≠‡∞ø‡∞Ç‡∞ö‡∞æ‡∞≤‡∞æ, ‡∞∏‡∞æ‡∞∞‡±ç?' },\n            'bn-IN': { name: 'Bengali', voice: 'bn-IN', greeting: '‡¶®‡¶Æ‡¶∏‡ßç‡¶ï‡¶æ‡¶∞! ‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶ø ‡¶∏‡¶®‡¶æ‡¶ï‡ßç‡¶§‡¶ï‡¶∞‡¶£ ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡¶¨, ‡¶∏‡ßç‡¶Ø‡¶æ‡¶∞?' },\n            'mr-IN': { name: 'Marathi', voice: 'mr-IN', greeting: '‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞! ‡§Æ‡•Ä ‡§ì‡§≥‡§ñ ‡§∏‡•Å‡§∞‡•Ç ‡§ï‡§∞‡§æ‡§µ‡•Ä ‡§ï‡§æ, ‡§∏‡§∞?' },\n            'gu-IN': { name: 'Gujarati', voice: 'gu-IN', greeting: '‡™®‡™Æ‡™∏‡´ç‡™§‡´á! ‡™∂‡´Å‡™Ç ‡™Æ‡™æ‡™∞‡´á ‡™°‡™ø‡™ü‡´á‡™ï‡´ç‡™∂‡™® ‡™∂‡™∞‡´Ç ‡™ï‡™∞‡™µ‡´Å‡™Ç ‡™ú‡´ã‡™à‡™è, ‡™∏‡™∞?' }\n        };\n        \n        // Detection settings\n        this.detectionThreshold = 0.5;\n        this.lastDetections = [];\n        this.lastAnnouncement = 0;\n        this.announcementInterval = 5000; // 5 seconds between announcements\n        \n        // Smart object announcement tracking system\n        this.objectAnnouncementCount = new Map(); // Track how many times each object was announced\n        this.objectLastSeen = new Map(); // Track when each object was last seen\n        this.objectDisappearanceTime = new Map(); // Track when object disappeared\n        this.maxAnnouncements = 3; // Maximum announcements per object\n        this.cooldownPeriod = 7000; // 7 seconds cooldown after object disappears\n        this.lastSpeechTime = 0;\n        this.speechCooldown = 2000; // 2 seconds cooldown between speech\n        this.isSpeaking = false;\n        this.speechQueue = [];\n        \n        // Enhanced speech delay configuration for object announcements\n        this.speechDelayTimer = null; // Timer for delaying speech\n        this.minObjectAnnouncementDelay = 1500; // 1.5 second minimum delay between object announcements\n        this.pendingAnnouncement = null; // Store pending announcement\n        this.isAnnouncementDelayed = false; // Flag to track if announcement is delayed\n        \n        // Navigation settings\n        this.isNavigating = false;\n        this.currentRoute = null;\n        this.currentStepIndex = 0;\n        this.locationWatcher = null;\n        this.routeDeviationThreshold = 15; // meters\n        \n        // Wake word detection\n        this.isListeningForWakeWord = true;\n        this.wakeWords = ['hey blindmate', 'hey blind mate', 'blindmate'];\n        this.continuousRecognition = null;\n        \n        // Volume key detection\n        this.volumeUpPressed = false;\n        this.volumeKeyTimeout = null;\n        this.currentListeningTimeout = null;\n        this.speechDetected = false;\n        \n        // Mobile double-tap gesture detection\n        this.lastTapTime = 0;\n        this.tapTimeout = null;\n        this.doubleTapDelay = 400; // milliseconds between taps (increased for better detection)\n        this.isMobileDevice = this.detectMobileDevice();\n        console.log('Mobile device detected:', this.isMobileDevice, {\n            userAgent: navigator.userAgent,\n            ontouchstart: 'ontouchstart' in window,\n            maxTouchPoints: navigator.maxTouchPoints\n        });\n        \n        this.init();\n    }\n\n\n\n    /**\n     * Get current position with error handling\n     */\n    getCurrentPosition() {\n        return new Promise((resolve, reject) => {\n            if (!navigator.geolocation) {\n                reject(new Error('Geolocation is not supported'));\n                return;\n            }\n            \n            navigator.geolocation.getCurrentPosition(\n                (position) => {\n                    resolve({\n                        lat: position.coords.latitude,\n                        lng: position.coords.longitude\n                    });\n                },\n                (error) => {\n                    let errorMessage = 'Location access failed. ';\n                    switch (error.code) {\n                        case error.PERMISSION_DENIED:\n                            errorMessage += 'Please enable GPS in your browser settings.';\n                            break;\n                        case error.POSITION_UNAVAILABLE:\n                            errorMessage += 'Location information is unavailable.';\n                            break;\n                        case error.TIMEOUT:\n                            errorMessage += 'Location request timed out.';\n                            break;\n                        default:\n                            errorMessage += 'An unknown error occurred.';\n                            break;\n                    }\n                    this.showError(errorMessage);\n                    this.speak('Location access is required. Please enable GPS in your browser settings.');\n                    reject(error);\n                },\n                {\n                    enableHighAccuracy: true,\n                    timeout: 10000,\n                    maximumAge: 60000\n                }\n            );\n        });\n    }\n\n    /**\n     * Update action status display\n     */\n    updateActionStatus(message, type = 'info') {\n        if (this.elements && this.elements.status && this.elements.statusText) {\n            this.elements.statusText.textContent = message;\n            this.elements.status.style.display = 'block';\n            this.elements.status.className = `alert alert-${type} mt-2`;\n            \n            // Auto-hide after 5 seconds for non-critical messages\n            if (type !== 'danger') {\n                setTimeout(() => {\n                    if (this.elements.status && this.elements.statusText.textContent === message) {\n                        this.elements.status.style.display = 'none';\n                    }\n                }, 5000);\n            }\n        }\n    }\n\n    /**\n     * Show error message\n     */\n    showError(message) {\n        if (this.elements && this.elements.errorMessage && this.elements.errorText) {\n            this.elements.errorText.textContent = message;\n            this.elements.errorMessage.style.display = 'block';\n            \n            // Auto-hide after 8 seconds\n            setTimeout(() => {\n                if (this.elements.errorMessage && this.elements.errorText.textContent === message) {\n                    this.elements.errorMessage.style.display = 'none';\n                }\n            }, 8000);\n        }\n    }\n\n    /**\n     * Monitor user position for route deviation\n     */\n    monitorPosition(expectedPath) {\n        if (this.locationWatcher) {\n            navigator.geolocation.clearWatch(this.locationWatcher);\n        }\n        \n        this.locationWatcher = navigator.geolocation.watchPosition(\n            (position) => {\n                const currentPos = {\n                    lat: position.coords.latitude,\n                    lng: position.coords.longitude\n                };\n                \n                // Check if user has deviated from route\n                if (this.isNavigating && this.currentRoute && this.currentRoute.legs) {\n                    const currentStep = this.getCurrentRouteStep();\n                    if (currentStep) {\n                        const distance = this.calculateDistance(\n                            currentPos,\n                            {\n                                lat: currentStep.end_location.lat(),\n                                lng: currentStep.end_location.lng()\n                            }\n                        );\n                        \n                        // If user is more than threshold distance away, re-route\n                        if (distance > this.routeDeviationThreshold) {\n                            this.handleRouteDeviation(currentPos);\n                        }\n                    }\n                }\n            },\n            (error) => {\n                console.warn('Position monitoring error:', error);\n                this.showError('GPS monitoring failed. Navigation accuracy may be reduced.');\n            },\n            {\n                enableHighAccuracy: true,\n                timeout: 5000,\n                maximumAge: 10000\n            }\n        );\n    }\n\n    /**\n     * Handle route deviation and re-calculate route\n     */\n    async handleRouteDeviation(currentPosition) {\n        try {\n            this.speak('You have moved off the route, recalculating...', true);\n            this.updateActionStatus('Re-routing...', 'warning');\n            \n            // Get the destination from current route\n            const destination = this.currentDestination;\n            if (!destination) {\n                this.showError('Cannot re-route: destination unknown');\n                return;\n            }\n            \n            // Re-calculate route from current position\n            await this.getDirections(currentPosition, destination);\n            \n            this.updateActionStatus('Route recalculated', 'success');\n            this.speak('New route calculated. Continuing navigation.');\n            \n        } catch (error) {\n            console.error('Re-routing failed:', error);\n            this.showError('Failed to recalculate route');\n            this.speak('Route recalculation failed. Please navigate manually.');\n        }\n    }\n\n    /**\n     * Get current route step\n     */\n    getCurrentRouteStep() {\n        if (!this.currentRoute || !this.currentRoute.legs || !this.currentRoute.legs[0]) {\n            return null;\n        }\n        \n        const steps = this.currentRoute.legs[0].steps;\n        if (this.currentStepIndex < steps.length) {\n            return steps[this.currentStepIndex];\n        }\n        \n        return null;\n    }\n\n    /**\n     * Calculate distance between two coordinates (Haversine formula)\n     */\n    calculateDistance(pos1, pos2) {\n        const R = 6371e3; // Earth's radius in meters\n        const œÜ1 = pos1.lat * Math.PI / 180;\n        const œÜ2 = pos2.lat * Math.PI / 180;\n        const ŒîœÜ = (pos2.lat - pos1.lat) * Math.PI / 180;\n        const ŒîŒª = (pos2.lng - pos1.lng) * Math.PI / 180;\n\n        const a = Math.sin(ŒîœÜ/2) * Math.sin(ŒîœÜ/2) +\n                Math.cos(œÜ1) * Math.cos(œÜ2) *\n                Math.sin(ŒîŒª/2) * Math.sin(ŒîŒª/2);\n        const c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1-a));\n\n        return R * c; // Distance in meters\n    }\n\n    /**\n     * Get location coordinates (supports both hardcoded and saved locations)\n     */\n    getLocationCoordinates(destinationName) {\n        // This function is deprecated - all destinations now go through Google Geocoding API\n        // Return null to force use of the enhanced navigation system\n        return null;\n    }\n\n    /**\n     * Simple stop navigation function\n     */\n    stopNavigationSimple() {\n        console.log('Stopping navigation');\n        \n        this.isNavigating = false;\n        this.currentRoute = null;\n        this.currentStepIndex = 0;\n        this.currentDestination = null;\n        \n        // Stop position monitoring\n        if (this.locationWatcher) {\n            navigator.geolocation.clearWatch(this.locationWatcher);\n            this.locationWatcher = null;\n        }\n        \n        this.updateActionStatus('Navigation stopped', 'warning');\n        this.speak('Navigation has been stopped', true);\n    }\n\n    /**\n     * Initialize the application\n     */\n    async init() {\n        try {\n            this.updateStatus('Initializing BlindMate...', 'info');\n            \n            // Load user preferences and check if this is a first-time user\n            await this.loadServerPreferences();\n            this.checkFirstTimeUser();\n            \n            // Initialize DOM elements first\n            this.initDOMElements();\n            \n            // Setup event listeners\n            this.setupEventListeners();\n            \n            // Initialize speech recognition\n            this.initSpeechRecognition();\n            \n            // Load TensorFlow model (optional - app works without it)\n            await this.loadModel();\n            \n            // Ensure loading overlay is hidden after initialization\n            const loadingOverlay = document.getElementById('loadingOverlay');\n            if (loadingOverlay) {\n                loadingOverlay.style.display = 'none';\n                console.log('Initialization complete - loading overlay hidden');\n            }\n            \n            // Start voice interaction\n            this.startVoiceInteraction();\n            \n        } catch (error) {\n            console.error('Initialization error:', error);\n            this.updateStatus('Failed to initialize. Please refresh the page.', 'danger');\n            this.speak('Sorry, there was an error initializing the application. Please refresh the page.');\n        }\n    }\n    \n    /**\n     * Initialize DOM elements with fallback for missing elements\n     */\n    initDOMElements() {\n        this.elements = {\n            video: document.getElementById('webcam'),\n            canvas: document.getElementById('canvas'),\n            startBtn: document.getElementById('startDetectionBtn'),\n            stopBtn: document.getElementById('stopDetectionBtn'),\n            voiceBtn: document.getElementById('voiceCommandBtn'),\n            locationBtn: document.getElementById('locationBtn'),\n            languageSelect: document.getElementById('languageSelect'),\n            toneSelect: document.getElementById('toneSelect'),\n            detectionStatus: document.getElementById('detectionStatus'),\n            voiceStatus: document.getElementById('voiceStatus'),\n            loadingOverlay: document.getElementById('loadingOverlay'),\n            detectionIndicator: document.getElementById('detectionIndicator'),\n            systemStatus: document.getElementById('systemStatus'),\n            status: document.getElementById('status'),\n            statusText: document.getElementById('statusText'),\n            errorMessage: document.getElementById('error-message'),\n            errorText: document.getElementById('errorText'),\n            navigationStatus: document.getElementById('navigationStatus') || this.createNavigationStatus(),\n            loadingOverlay: document.getElementById('loadingOverlay'),\n            detectionIndicator: document.getElementById('detectionIndicator')\n        };\n\n        // Ensure all critical elements exist\n        this.validateElements();\n    }\n\n    /**\n     * Validate that essential elements exist and create fallbacks if needed\n     */\n    validateElements() {\n        const requiredElements = ['video', 'canvas', 'startBtn', 'stopBtn', 'voiceBtn', 'locationBtn', 'languageSelect', 'toneSelect', 'systemStatus'];\n        \n        for (const elementKey of requiredElements) {\n            if (!this.elements[elementKey]) {\n                console.warn(`Missing element: ${elementKey}`);\n                \n                // Create fallback element to prevent crashes\n                if (elementKey === 'systemStatus') {\n                    this.elements[elementKey] = this.createStatusElement();\n                }\n            }\n        }\n    }\n\n    /**\n     * Create fallback status element\n     */\n    createStatusElement() {\n        const statusDiv = document.createElement('div');\n        statusDiv.className = 'alert alert-info';\n        statusDiv.textContent = 'System ready';\n        return statusDiv;\n    }\n\n    /**\n     * Detect if device is mobile\n     */\n    detectMobileDevice() {\n        return /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent) || \n               ('ontouchstart' in window) || \n               (navigator.maxTouchPoints > 0);\n    }\n\n    /**\n     * Setup mobile double-tap gesture detection\n     */\n    setupMobileDoubleTap() {\n        let firstTapTime = 0;\n        let tapCount = 0;\n        let tapTimeout = null;\n        \n        console.log('Setting up mobile double-tap gesture detection...');\n        \n        // Add touch event listener to entire document for full-screen double-tap\n        document.addEventListener('touchend', (e) => {\n            const currentTime = Date.now();\n            \n            // Clear existing timeout\n            if (tapTimeout) {\n                clearTimeout(tapTimeout);\n                tapTimeout = null;\n            }\n            \n            // Prevent interference with UI elements that need single taps\n            const target = e.target;\n            const isUIElement = target.tagName === 'BUTTON' || \n                              target.tagName === 'SELECT' || \n                              target.tagName === 'INPUT' ||\n                              target.closest('button') || \n                              target.closest('select') ||\n                              target.closest('input') ||\n                              target.closest('.btn') ||\n                              target.id.includes('Btn') ||\n                              target.className.includes('btn') ||\n                              target.classList.contains('form-control') ||\n                              target.classList.contains('form-select');\n            \n            // Skip double-tap detection on UI elements\n            if (isUIElement) {\n                console.log('Tap on UI element ignored:', target.tagName, target.id || target.className);\n                return;\n            }\n            \n            tapCount++;\n            \n            if (tapCount === 1) {\n                // First tap\n                firstTapTime = currentTime;\n                \n                // Set timeout to reset tap count if no second tap\n                tapTimeout = setTimeout(() => {\n                    tapCount = 0;\n                    firstTapTime = 0;\n                    console.log('Double-tap timeout - single tap detected');\n                }, this.doubleTapDelay);\n                \n                console.log('First tap detected, waiting for second tap...');\n                \n            } else if (tapCount === 2) {\n                // Second tap - check if within double-tap delay\n                const timeDiff = currentTime - firstTapTime;\n                \n                if (timeDiff <= this.doubleTapDelay) {\n                    // Double-tap detected!\n                    e.preventDefault(); // Prevent default zoom behavior\n                    e.stopPropagation(); // Stop event bubbling\n                    \n                    console.log(`Double-tap detected! Time difference: ${timeDiff}ms`);\n                    \n                    // Provide immediate feedback\n                    navigator.vibrate && navigator.vibrate(50); // Short vibration if available\n                    this.speak('Listening started');\n                    \n                    // Call the same function as voice command button\n                    setTimeout(() => {\n                        this.startVoiceCommand();\n                    }, 100); // Small delay to ensure speech starts first\n                    \n                    // Reset counters\n                    tapCount = 0;\n                    firstTapTime = 0;\n                } else {\n                    // Too slow, treat as new first tap\n                    tapCount = 1;\n                    firstTapTime = currentTime;\n                    \n                    tapTimeout = setTimeout(() => {\n                        tapCount = 0;\n                        firstTapTime = 0;\n                    }, this.doubleTapDelay);\n                    \n                    console.log('Second tap too slow, treating as new first tap');\n                }\n            }\n        }, { passive: false });\n        \n        // Also add touchstart to prevent default behaviors during double-tap\n        document.addEventListener('touchstart', (e) => {\n            // Only prevent default on non-UI elements during potential double-tap\n            const target = e.target;\n            const isUIElement = target.tagName === 'BUTTON' || \n                              target.tagName === 'SELECT' || \n                              target.tagName === 'INPUT' ||\n                              target.closest('button') || \n                              target.closest('select') ||\n                              target.closest('input') ||\n                              target.closest('.btn') ||\n                              target.id.includes('Btn') ||\n                              target.className.includes('btn');\n            \n            if (!isUIElement && tapCount === 1) {\n                // During potential double-tap sequence, prevent default behaviors\n                e.preventDefault();\n            }\n        }, { passive: false });\n        \n        console.log('Mobile double-tap gesture enabled for voice commands with improved detection');\n        \n        // Add visual hint for mobile users\n        this.addMobileHint();\n    }\n\n    /**\n     * Add visual hint for mobile double-tap feature\n     */\n    addMobileHint() {\n        // Create hint element\n        const hintElement = document.createElement('div');\n        hintElement.id = 'mobileHint';\n        hintElement.className = 'alert alert-info mobile-hint';\n        hintElement.style.cssText = `\n            position: fixed;\n            bottom: 20px;\n            left: 50%;\n            transform: translateX(-50%);\n            z-index: 1000;\n            background: rgba(0, 123, 255, 0.9);\n            color: white;\n            padding: 10px 20px;\n            border-radius: 25px;\n            font-size: 14px;\n            text-align: center;\n            animation: fadeInOut 4s ease-in-out;\n            pointer-events: none;\n        `;\n        hintElement.innerHTML = 'üí° Double-tap anywhere to start voice commands';\n        \n        // Add CSS animation\n        const style = document.createElement('style');\n        style.textContent = `\n            @keyframes fadeInOut {\n                0% { opacity: 0; transform: translateX(-50%) translateY(20px); }\n                15% { opacity: 1; transform: translateX(-50%) translateY(0); }\n                85% { opacity: 1; transform: translateX(-50%) translateY(0); }\n                100% { opacity: 0; transform: translateX(-50%) translateY(-20px); }\n            }\n        `;\n        document.head.appendChild(style);\n        \n        // Add to page\n        document.body.appendChild(hintElement);\n        \n        // Remove after animation\n        setTimeout(() => {\n            if (hintElement.parentNode) {\n                hintElement.parentNode.removeChild(hintElement);\n            }\n        }, 4000);\n    }\n    \n    /**\n     * Create navigation status element if it doesn't exist\n     */\n    createNavigationStatus() {\n        const navStatus = document.createElement('span');\n        navStatus.id = 'navigationStatus';\n        navStatus.className = 'badge bg-secondary';\n        navStatus.textContent = 'Ready';\n        return navStatus;\n    }\n\n    /**\n     * Setup all event listeners\n     */\n    setupEventListeners() {\n        // Add event listeners with null checks\n        if (this.elements.startBtn) {\n            this.elements.startBtn.addEventListener('click', () => this.startDetection());\n        }\n        if (this.elements.stopBtn) {\n            this.elements.stopBtn.addEventListener('click', () => this.stopDetection());\n        }\n        if (this.elements.voiceBtn) {\n            this.elements.voiceBtn.addEventListener('click', () => this.startVoiceCommand());\n        }\n        if (this.elements.locationBtn) {\n            this.elements.locationBtn.addEventListener('click', () => this.requestLocation());\n        }\n        if (this.elements.languageSelect) {\n            this.elements.languageSelect.addEventListener('change', (e) => this.changeLanguage(e.target.value));\n        }\n        if (this.elements.toneSelect) {\n            this.elements.toneSelect.addEventListener('change', (e) => this.changeTone(e.target.value));\n        }\n        \n        // Mobile double-tap gesture for voice commands\n        console.log('Checking mobile device for double-tap setup:', this.isMobileDevice);\n        if (this.isMobileDevice) {\n            this.setupMobileDoubleTap();\n        } else {\n            console.log('Desktop device - double-tap not enabled');\n        }\n        \n        // Keyboard shortcuts for accessibility and volume key detection\n        document.addEventListener('keydown', (e) => {\n            // Volume Up key detection (multiple key codes for different devices)\n            if (e.key === 'VolumeUp' || e.keyCode === 175 || e.keyCode === 174 || \n                e.code === 'VolumeUp' || e.code === 'AudioVolumeUp') {\n                e.preventDefault();\n                this.handleVolumeUpPress();\n                return;\n            }\n            \n            // Ctrl + key shortcuts\n            if (e.ctrlKey) {\n                switch(e.key) {\n                    case 's':\n                        e.preventDefault();\n                        if (this.isDetecting) {\n                            this.stopDetection();\n                        } else {\n                            this.startDetection();\n                        }\n                        break;\n                    case 'v':\n                        e.preventDefault();\n                        this.startVoiceCommand();\n                        break;\n                    case 'l':\n                        e.preventDefault();\n                        this.requestLocation();\n                        break;\n                }\n            }\n        });\n    }\n\n    /**\n     * Initialize speech recognition for voice commands\n     */\n    initSpeechRecognition() {\n        if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {\n            console.warn('Speech recognition not supported');\n            this.elements.voiceBtn.disabled = true;\n            this.updateStatus('Voice commands not supported. Use text input instead.', 'warning');\n            this.showTextFallback();\n            return;\n        }\n\n        // Initialize speech recognition\n        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n        \n        // Create recognition instance for voice commands\n        this.commandRecognition = new SpeechRecognition();\n        this.commandRecognition.continuous = false;\n        this.commandRecognition.interimResults = false;\n        this.commandRecognition.lang = this.currentLanguage;\n        \n        // Command recognition event handlers\n        this.commandRecognition.onstart = () => {\n            this.isListening = true;\n            this.speechDetected = false; // Track if actual speech was detected\n            this.updateStatus('üé§ Listening... Speak your command now', 'primary');\n            this.elements.voiceStatus.textContent = 'Listening';\n            this.elements.voiceStatus.className = 'badge bg-primary';\n            this.elements.voiceBtn.innerHTML = '<i class=\"fas fa-stop\"></i> Stop Listening';\n            console.log('Speech started successfully');\n            // Only speak feedback if this wasn't triggered by volume button to avoid conflicts\n            if (!this.volumeUpPressed) {\n                this.speak('Speak your command now', true);\n            }\n        };\n        \n        this.commandRecognition.onresult = (event) => {\n            // Clear any listening timeout since we got a result\n            if (this.currentListeningTimeout) {\n                clearTimeout(this.currentListeningTimeout);\n                this.currentListeningTimeout = null;\n            }\n            \n            const command = event.results[0][0].transcript.trim();\n            const confidence = event.results[0][0].confidence;\n            console.log('Voice command received:', command, 'Confidence:', confidence);\n            \n            // Mark that speech was detected\n            this.speechDetected = true;\n            \n            // Process all reasonable commands - many speech recognition engines return 0 confidence\n            if (command.length > 2) {\n                // Show command in UI\n                this.showRecognizedCommand(command);\n                \n                // Check if this is a navigation command first\n                if (this.isNavigationCommand(command)) {\n                    console.log('Navigation command detected:', command);\n                    this.processNavigationCommand(command);\n                } else if (command.toLowerCase().includes('start detection') || command.toLowerCase().includes('start object detection')) {\n                    // Handle start detection directly for faster response\n                    console.log('Direct start detection command:', command);\n                    this.fallbackCommandProcessing(command);\n                } else {\n                    // Process the command via Gemini for other commands\n                    this.processVoiceCommand(command);\n                }\n            } else {\n                console.log('Short command received, ignoring:', command, 'Length:', command.length);\n                this.updateStatus('Ready for voice commands. Press Voice Command button to try again.', 'info');\n            }\n        };\n        \n        this.commandRecognition.onerror = (event) => {\n            console.error('Command recognition error:', event.error);\n            this.isListening = false;\n            this.elements.voiceStatus.textContent = 'Ready';\n            this.elements.voiceStatus.className = 'badge bg-secondary';\n            this.elements.voiceBtn.innerHTML = '<i class=\"fas fa-microphone\"></i> Voice Command';\n            \n            // Handle different error types more gracefully - don't announce every error\n            if (event.error === 'not-allowed') {\n                this.updateStatus('Microphone access denied. Please allow microphone access.', 'warning');\n                this.showTextFallback();\n            } else if (event.error === 'no-speech') {\n                // For no-speech errors, just stay ready without error announcements\n                this.updateStatus('Ready for voice commands. Press Voice Command button to try again.', 'info');\n                console.log('No speech detected - staying ready for next command');\n            } else if (event.error === 'aborted') {\n                // Recognition was intentionally stopped, don't show error\n                console.log('Speech recognition aborted - this is normal');\n            } else {\n                // Other errors - just stay ready\n                this.updateStatus('Ready for voice commands. Press Voice Command button to try again.', 'info');\n                console.log('Speech recognition error handled:', event.error);\n            }\n        };\n        \n        this.commandRecognition.onend = () => {\n            this.isListening = false;\n            this.elements.voiceStatus.textContent = 'Ready';\n            this.elements.voiceStatus.className = 'badge bg-secondary';\n            this.elements.voiceBtn.innerHTML = '<i class=\"fas fa-microphone\"></i> Voice Command';\n            \n            // Clear any listening timeout\n            if (this.currentListeningTimeout) {\n                clearTimeout(this.currentListeningTimeout);\n                this.currentListeningTimeout = null;\n            }\n            \n            // Only show completion message if we're not in an error state\n            if (!this.updateStatus.lastWasError) {\n                this.updateStatus('Ready for voice commands. Say \"Hey BlindMate\" or press Volume Up.', 'success');\n            }\n            \n            // Restart continuous listening for wake words\n            setTimeout(() => {\n                this.startContinuousListening();\n            }, 1000);\n        };\n\n        // Add click handler for voice button\n        this.elements.voiceBtn.addEventListener('click', () => {\n            if (this.isListening) {\n                this.stopVoiceCommand();\n            } else {\n                this.startVoiceCommand();\n            }\n        });\n        \n        // Initialize continuous recognition for wake words separately\n        this.initContinuousListening();\n    }\n    \n    /**\n     * Initialize continuous listening for wake words\n     */\n    initContinuousListening() {\n        if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {\n            return;\n        }\n\n        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n        this.continuousRecognition = new SpeechRecognition();\n        this.continuousRecognition.continuous = true;\n        this.continuousRecognition.interimResults = true;\n        this.continuousRecognition.lang = this.currentLanguage;\n        \n        this.continuousRecognition.onresult = (event) => {\n            for (let i = event.resultIndex; i < event.results.length; i++) {\n                const result = event.results[i];\n                const command = result[0].transcript.toLowerCase().trim();\n                \n                console.log('Continuous listening heard:', command);\n                \n                // Check for wake words with better matching\n                if (this.wakeWords.some(wake => command.includes(wake))) {\n                    console.log('Wake word detected:', command);\n                    this.handleWakeWordDetected();\n                    break;\n                }\n            }\n        };\n        \n        this.continuousRecognition.onerror = (event) => {\n            console.log('Continuous recognition error:', event.error);\n            if (event.error !== 'aborted') {\n                // Restart continuous listening after a short delay\n                setTimeout(() => {\n                    if (this.isListeningForWakeWord) {\n                        this.startContinuousListening();\n                    }\n                }, 1000);\n            }\n        };\n        \n        this.continuousRecognition.onend = () => {\n            // Restart continuous listening if it should be active\n            if (this.isListeningForWakeWord && !this.isListening) {\n                setTimeout(() => {\n                    this.startContinuousListening();\n                }, 500);\n            }\n        };\n    }\n    \n    /**\n     * Start voice command\n     */\n    startVoiceCommand() {\n        if (!this.commandRecognition) {\n            this.updateStatus('Voice recognition not available.', 'warning');\n            this.showTextFallback();\n            return;\n        }\n\n        if (this.isListening) {\n            this.stopVoiceCommand();\n            return;\n        }\n\n        try {\n            // Stop continuous listening temporarily\n            this.stopContinuousListening();\n            \n            // Clear any existing timeouts\n            if (this.currentListeningTimeout) {\n                clearTimeout(this.currentListeningTimeout);\n                this.currentListeningTimeout = null;\n            }\n            \n            // Force stop any existing recognition first\n            try {\n                this.commandRecognition.stop();\n            } catch (e) {\n                // Ignore errors when stopping\n            }\n            \n            // Wait a moment then start fresh\n            setTimeout(() => {\n                try {\n                    if (!this.isListening) {  // Only start if not already listening\n                        this.commandRecognition.lang = this.currentLanguage;\n                        this.commandRecognition.start();\n                        console.log('Speech recognition started');\n                    }\n                } catch (error) {\n                    console.error('Speech start error:', error);\n                    this.updateStatus('Voice recognition temporarily unavailable. Please try again.', 'warning');\n                }\n            }, 200);\n        } catch (error) {\n            console.error('Error starting voice recognition:', error);\n            this.updateStatus('Voice recognition temporarily unavailable. Please try again.', 'warning');\n            \n            // Restart continuous listening\n            setTimeout(() => {\n                this.startContinuousListening();\n            }, 1000);\n        }\n    }\n    \n    /**\n     * Stop voice command\n     */\n    stopVoiceCommand() {\n        console.log('Stopping voice command, current state:', this.isListening);\n        \n        if (this.commandRecognition) {\n            try {\n                this.commandRecognition.stop();\n            } catch (error) {\n                console.log('Error stopping voice command:', error.message);\n            }\n        }\n        \n        // Clear any pending timeouts\n        if (this.currentListeningTimeout) {\n            clearTimeout(this.currentListeningTimeout);\n            this.currentListeningTimeout = null;\n        }\n        \n        if (this.volumeKeyTimeout) {\n            clearTimeout(this.volumeKeyTimeout);\n            this.volumeKeyTimeout = null;\n        }\n        \n        // Reset state immediately\n        this.isListening = false;\n        this.volumeUpPressed = false;\n        this.speechDetected = false;\n        \n        // Update UI\n        if (this.elements.voiceStatus) {\n            this.elements.voiceStatus.textContent = 'Ready';\n            this.elements.voiceStatus.className = 'badge bg-secondary';\n        }\n        if (this.elements.voiceBtn) {\n            this.elements.voiceBtn.innerHTML = '<i class=\"fas fa-microphone\"></i> Voice Command';\n        }\n    }\n    \n    /**\n     * Start continuous listening for wake words\n     */\n    startContinuousListening() {\n        if (this.continuousRecognition && this.isListeningForWakeWord && !this.isListening) {\n            try {\n                this.continuousRecognition.lang = this.currentLanguage;\n                this.continuousRecognition.start();\n            } catch (error) {\n                console.log('Continuous listening start error:', error.message);\n            }\n        }\n    }\n    \n    /**\n     * Stop continuous listening\n     */\n    stopContinuousListening() {\n        if (this.continuousRecognition) {\n            try {\n                this.continuousRecognition.stop();\n            } catch (error) {\n                console.log('Continuous listening stop error:', error.message);\n            }\n        }\n    }\n    \n    /**\n     * Handle wake word detection\n     */\n    handleWakeWordDetected() {\n        console.log('Wake word \"Hey BlindMate\" detected!');\n        this.updateStatus('üé§ Wake word detected! Listening for command...', 'success');\n        \n        // Stop continuous listening temporarily\n        this.stopContinuousListening();\n        \n        // Give audio feedback\n        this.speak('Yes, how can I help you?', true);\n        \n        // Start command listening after response\n        setTimeout(() => {\n            this.startVoiceCommand();\n        }, 1500);\n    }\n    \n    /**\n     * Handle volume up key press for voice activation\n     */\n    handleVolumeUpPress() {\n        console.log('Volume Up key pressed for voice activation');\n        \n        // Prevent multiple rapid presses\n        if (this.volumeKeyTimeout) {\n            clearTimeout(this.volumeKeyTimeout);\n        }\n        \n        // If already listening, stop\n        if (this.isListening) {\n            this.stopVoiceCommand();\n            this.speak('Voice command stopped', true);\n            return;\n        }\n        \n        // Add debouncing to prevent accidental volume button presses\n        this.volumeUpPressed = true;\n        \n        // Start voice command with improved feedback - don't say \"Voice command ready\" at the same time as starting recognition\n        this.updateStatus('üé§ Volume Up pressed - Voice command ready', 'info');\n        \n        // Start listening immediately without conflicting speech\n        this.volumeKeyTimeout = setTimeout(() => {\n            if (this.volumeUpPressed) {\n                this.startVoiceCommandWithTimeout();\n                this.volumeUpPressed = false;\n            }\n        }, 300); // Reduced delay to start faster\n    }\n    \n    /**\n     * Start voice command with automatic timeout to prevent hanging\n     */\n    startVoiceCommandWithTimeout() {\n        // Set a timeout to automatically stop listening if no speech detected\n        const listeningTimeout = setTimeout(() => {\n            if (this.isListening && !this.speechDetected) {\n                console.log('Voice command timeout - no speech detected, stopping silently');\n                this.stopVoiceCommand();\n                this.updateStatus('Ready for voice commands. Say \"Hey BlindMate\" or press Volume Up.', 'info');\n            }\n        }, 6000); // 6 seconds timeout\n        \n        // Store timeout ID to clear it if command succeeds\n        this.currentListeningTimeout = listeningTimeout;\n        \n        // Start normal voice command\n        this.startVoiceCommand();\n    }\n    \n    /**\n     * Check if a command is a navigation command\n     */\n    isNavigationCommand(command) {\n        const navigationKeywords = [\n            'take me to', 'go to', 'navigate to', 'direction to', 'directions to',\n            'route to', 'find route to', 'show route to', 'how to get to',\n            'where is', 'location of', 'find location', 'search for'\n        ];\n        \n        const lowercaseCommand = command.toLowerCase();\n        \n        // Exclude meaningless phrases from navigation detection\n        const meaninglessPhases = ['sorry', 'please try again', 'try again', 'didn\\'t understand'];\n        if (meaninglessPhases.some(phrase => lowercaseCommand.includes(phrase))) {\n            return false;\n        }\n        \n        return navigationKeywords.some(keyword => lowercaseCommand.includes(keyword));\n    }\n    \n    /**\n     * Process navigation commands directly\n     */\n    processNavigationCommand(command) {\n        console.log('Processing navigation command directly:', command);\n        \n        // Extract destination from command\n        let destination = this.extractDestination(command);\n        \n        if (destination) {\n            console.log('Direct navigation to:', destination);\n            // Use the enhanced navigation system via navigateToLocation\n            this.navigateToLocation(destination);\n        } else {\n            // If we can't extract destination with basic patterns, use Gemini AI\n            console.log('Could not extract destination, using Gemini AI processing');\n            this.processVoiceCommand(command);\n        }\n    }\n    \n    /**\n     * Extract destination from navigation command\n     */\n    extractDestination(command) {\n        const lowercaseCommand = command.toLowerCase();\n        \n        // Patterns to extract destination\n        const patterns = [\n            /(?:take me to|go to|navigate to|direction to|directions to|route to|find route to|show route to|how to get to)\\s+(.+)/i,\n            /(?:where is|location of|find location|search for)\\s+(.+)/i,\n            /(?:navigate|directions|route)\\s+(.+)/i\n        ];\n        \n        for (const pattern of patterns) {\n            const match = command.match(pattern);\n            if (match && match[1]) {\n                return match[1].trim();\n            }\n        }\n        \n        return null;\n    }\n    \n    /**\n     * Show text fallback input for when voice is not available\n     */\n    showTextFallback() {\n        if (document.getElementById('textCommandInput')) return; // Already shown\n        \n        const fallbackHtml = `\n            <div class=\"mt-3 p-3 border rounded bg-light\">\n                <h6>Voice not available? Use text instead:</h6>\n                <div class=\"input-group\">\n                    <input type=\"text\" id=\"textCommandInput\" class=\"form-control\" \n                           placeholder=\"Type your command (e.g., 'start detection', 'take me to library')\">\n                    <button class=\"btn btn-primary\" id=\"textCommandBtn\">\n                        <i class=\"fas fa-paper-plane\"></i> Send\n                    </button>\n                </div>\n                <small class=\"text-muted\">Commands: start detection, stop, where am i, take me to [place], enable location</small>\n            </div>\n        `;\n        \n        const controlsSection = document.querySelector('.col-md-6:last-child .card-body');\n        if (controlsSection) {\n            controlsSection.insertAdjacentHTML('beforeend', fallbackHtml);\n            \n            const textInput = document.getElementById('textCommandInput');\n            const textBtn = document.getElementById('textCommandBtn');\n            \n            const processTextCommand = () => {\n                const command = textInput.value.trim();\n                if (command) {\n                    this.showRecognizedCommand(command);\n                    this.processVoiceCommand(command);\n                    textInput.value = '';\n                }\n            };\n            \n            textBtn.addEventListener('click', processTextCommand);\n            textInput.addEventListener('keypress', (e) => {\n                if (e.key === 'Enter') {\n                    processTextCommand();\n                }\n            });\n        }\n    }\n    \n    /**\n     * Show the recognized command in UI\n     */\n    showRecognizedCommand(command) {\n        // Update the system status to show the command\n        this.updateStatus(`Command received: \"${command}\"`, 'info');\n        \n        // Show in dedicated command display\n        let commandDisplay = document.getElementById('lastCommand');\n        if (!commandDisplay) {\n            const statusArea = document.getElementById('systemStatus').parentElement;\n            statusArea.insertAdjacentHTML('afterend', `\n                <div class=\"alert alert-info mt-2\" id=\"lastCommand\" style=\"display: none;\">\n                    <strong>Last Command:</strong> <span id=\"commandText\"></span>\n                </div>\n            `);\n            commandDisplay = document.getElementById('lastCommand');\n        }\n        \n        document.getElementById('commandText').textContent = command;\n        commandDisplay.style.display = 'block';\n        \n        // Hide after 5 seconds\n        setTimeout(() => {\n            commandDisplay.style.display = 'none';\n        }, 5000);\n    }\n\n    /**\n     * Load TensorFlow.js Coco SSD model\n     */\n    async loadModel() {\n        try {\n            this.updateStatus('Loading AI detection model...', 'warning');\n            \n            // Check if TensorFlow.js is available\n            if (typeof tf === 'undefined') {\n                throw new Error('TensorFlow.js not loaded');\n            }\n            \n            // Set backend to CPU if WebGL is not available\n            if (!tf.ENV.getBool('WEBGL_VERSION')) {\n                console.warn('WebGL not available, falling back to CPU backend');\n                await tf.setBackend('cpu');\n            }\n            \n            // Ensure TensorFlow.js is ready\n            await tf.ready();\n            \n            // Check if COCO-SSD is available\n            if (typeof cocoSsd === 'undefined') {\n                throw new Error('COCO-SSD model not loaded');\n            }\n            \n            // Load COCO-SSD model\n            this.model = await cocoSsd.load();\n            \n            this.updateStatus('AI model loaded successfully!', 'success');\n            \n            // Hide loading overlay\n            const loadingOverlay = document.getElementById('loadingOverlay');\n            if (loadingOverlay) {\n                loadingOverlay.style.display = 'none';\n                console.log('Loading overlay hidden successfully');\n            } else {\n                console.warn('Loading overlay element not found');\n            }\n            \n            console.log('COCO-SSD model loaded successfully');\n            \n        } catch (error) {\n            console.error('Error loading model:', error);\n            this.updateStatus('Object detection disabled. Voice commands and navigation still available.', 'warning');\n            \n            // Hide loading overlay even on error\n            const loadingOverlay = document.getElementById('loadingOverlay');\n            if (loadingOverlay) {\n                loadingOverlay.style.display = 'none';\n            }\n            \n            // Don't throw error - allow app to continue without object detection\n            console.log('Continuing without object detection...');\n        }\n    }\n\n    /**\n     * Start voice interaction flow\n     */\n    startVoiceInteraction() {\n        const greeting = 'Hello! I am BlindMate, your AI assistant. Say \"Hey BlindMate\" or press Volume Up anytime to give me voice commands.';\n        this.speak(greeting, true); // High priority\n        \n        // Start continuous listening for wake word after greeting\n        setTimeout(() => {\n            this.startContinuousListening();\n            this.updateStatus('üëÇ Always listening for \"Hey BlindMate\" or Volume Up key', 'info');\n        }, 4000);\n    }\n    \n    /**\n     * Setup voice-guided permission flow\n     */\n    setupVoicePermissionFlow() {\n        if (this.recognition && !this.isListening) {\n            this.recognition.continuous = false; // Short responses for permissions\n            this.recognition.interimResults = false;\n            \n            this.recognition.onresult = (event) => {\n                const command = event.results[event.results.length - 1][0].transcript.toLowerCase().trim();\n                console.log('Permission flow - heard:', command);\n                \n                if (command.includes('yes') || command.includes('‡§π‡§æ‡§Å') || command.includes('‡¶ì‡¶Ø‡¶º‡¶æ‡¶á') || command.includes('‡ÆÜ‡ÆÆ‡Øç')) {\n                    this.handlePermissionYes();\n                } else if (command.includes('no') || command.includes('‡§®‡§π‡•Ä‡§Ç') || command.includes('‡¶®‡¶æ') || command.includes('‡Æá‡¶≤‡Øç‡Æ≤‡Øà')) {\n                    this.handlePermissionNo();\n                }\n            };\n            \n            this.recognition.onerror = (event) => {\n                console.log('Permission recognition error:', event.error);\n                this.isListening = false;\n            };\n            \n            this.recognition.onend = () => {\n                this.isListening = false;\n            };\n            \n            try {\n                this.recognition.start();\n                this.isListening = true;\n            } catch (error) {\n                console.log('Could not start permission recognition:', error);\n            }\n        }\n    }\n    \n    /**\n     * Handle \"yes\" response during permission flow\n     */\n    async handlePermissionYes() {\n        if (!this.stream) {\n            // First \"yes\" - start detection\n            this.speak('Starting camera detection now.', true);\n            await this.startDetection();\n            \n            // Ask for location\n            setTimeout(() => {\n                this.speak('Would you like to enable location for navigation?', true);\n            }, 2000);\n        } else if (!this.userLocation) {\n            // Second \"yes\" - enable location\n            this.speak('Enabling location services.', true);\n            await this.requestLocation();\n            this.finalizeSetup();\n        }\n    }\n    \n    /**\n     * Handle \"no\" response during permission flow\n     */\n    handlePermissionNo() {\n        this.speak('Okay, you can enable features later using voice commands or buttons.', true);\n        this.finalizeSetup();\n    }\n    \n    /**\n     * Finalize setup and start continuous listening\n     */\n    finalizeSetup() {\n        setTimeout(() => {\n            this.speak('Setup complete. Say \"Hey BlindMate\" followed by your command to interact with me.', true);\n            this.startContinuousListening();\n        }, 2000);\n    }\n    \n    /**\n     * Start continuous listening for wake word\n     */\n    startContinuousListening() {\n        if (!this.recognition || this.continuousRecognition) return;\n        \n        try {\n            this.continuousRecognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();\n            this.continuousRecognition.continuous = true;\n            this.continuousRecognition.interimResults = false;\n            this.continuousRecognition.lang = this.currentLanguage;\n            \n            this.continuousRecognition.onresult = (event) => {\n                const command = event.results[event.results.length - 1][0].transcript.toLowerCase().trim();\n                console.log('Continuous listening heard:', command);\n                \n                // Check for wake word\n                const hasWakeWord = this.wakeWords.some(wake => command.includes(wake));\n                \n                if (hasWakeWord) {\n                    console.log('Wake word detected in command:', command);\n                    // Extract command after wake word\n                    const commandAfterWake = command.split(/hey\\s*blind\\s*mate\\s*/i)[1]?.trim();\n                    if (commandAfterWake) {\n                        console.log('Processing command after wake word:', commandAfterWake);\n                        this.speak('Yes, how can I help?', true);\n                        this.processVoiceCommand(commandAfterWake);\n                    } else {\n                        this.speak('Yes, I am listening. What can I do for you?', true);\n                    }\n                }\n            };\n            \n            this.continuousRecognition.onerror = (event) => {\n                console.log('Continuous recognition error:', event.error);\n                // Only restart if it's not already running\n                if (event.error !== 'aborted') {\n                    setTimeout(() => {\n                        if (this.continuousRecognition && !this.isListening) {\n                            try {\n                                this.continuousRecognition.start();\n                            } catch (e) {\n                                console.log('Could not restart continuous recognition:', e);\n                            }\n                        }\n                    }, 2000);\n                }\n            };\n            \n            this.continuousRecognition.onend = () => {\n                // Only restart if we should be listening\n                if (this.continuousRecognition && !this.isListening) {\n                    setTimeout(() => {\n                        try {\n                            this.continuousRecognition.start();\n                        } catch (e) {\n                            console.log('Could not restart continuous recognition:', e);\n                        }\n                    }, 1000);\n                }\n            };\n            \n            this.continuousRecognition.start();\n            \n        } catch (error) {\n            console.log('Could not start continuous listening:', error);\n        }\n    }\n\n    /**\n     * Start object detection\n     */\n    async startDetection() {\n        try {\n            if (!this.model) {\n                this.speak('AI model is not ready. Please wait.');\n                return;\n            }\n\n            this.updateStatus('Starting camera...', 'warning');\n            \n            // Request camera access\n            this.stream = await navigator.mediaDevices.getUserMedia({\n                video: { \n                    width: { ideal: 640 }, \n                    height: { ideal: 480 },\n                    facingMode: 'environment' // Use back camera on mobile\n                }\n            });\n            \n            this.video.srcObject = this.stream;\n            \n            // Wait for video to be ready\n            await new Promise((resolve) => {\n                this.video.onloadedmetadata = () => {\n                    this.video.play();\n                    resolve();\n                };\n            });\n            \n            // Setup canvas dimensions\n            this.canvas.width = this.video.videoWidth;\n            this.canvas.height = this.video.videoHeight;\n            \n            this.isDetecting = true;\n            this.updateStatus('Detection active - Scanning for objects...', 'success');\n            this.elements.detectionStatus.textContent = 'Active';\n            this.elements.detectionStatus.className = 'badge bg-success';\n            \n            // Show detection indicator\n            this.elements.detectionIndicator.style.display = 'block';\n            this.elements.detectionIndicator.classList.add('active');\n            \n            this.elements.startBtn.disabled = true;\n            this.elements.stopBtn.disabled = false;\n            \n            this.speak('Object detection started. I will alert you about any obstacles or objects I detect.');\n            \n            // Start detection loop\n            this.detectObjects();\n            \n        } catch (error) {\n            console.error('Error starting detection:', error);\n            this.updateStatus('Camera unavailable. Voice commands and navigation are still active.', 'warning');\n            this.speak('Camera is not available, but voice commands and navigation are ready to use.');\n        }\n    }\n\n    /**\n     * Stop object detection\n     */\n    stopDetection() {\n        this.isDetecting = false;\n        \n        // Clean up speech delay timer\n        if (this.speechDelayTimer) {\n            clearTimeout(this.speechDelayTimer);\n            this.speechDelayTimer = null;\n        }\n        this.pendingAnnouncement = null;\n        this.isAnnouncementDelayed = false;\n        \n        if (this.stream) {\n            this.stream.getTracks().forEach(track => track.stop());\n            this.stream = null;\n        }\n        \n        this.video.srcObject = null;\n        this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);\n        \n        this.updateStatus('Detection stopped.', 'secondary');\n        this.elements.detectionStatus.textContent = 'Inactive';\n        this.elements.detectionStatus.className = 'badge bg-secondary';\n        \n        // Hide detection indicator\n        this.elements.detectionIndicator.style.display = 'none';\n        this.elements.detectionIndicator.classList.remove('active');\n        \n        this.elements.startBtn.disabled = false;\n        this.elements.stopBtn.disabled = true;\n        \n        this.speak('Object detection stopped.');\n    }\n\n    /**\n     * Main object detection loop\n     */\n    async detectObjects() {\n        if (!this.isDetecting || !this.model) {\n            return;\n        }\n\n        try {\n            // Perform detection\n            const predictions = await this.model.detect(this.video);\n            \n            // Clear previous drawings\n            this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);\n            \n            // Filter predictions by confidence threshold\n            const validPredictions = predictions.filter(prediction => \n                prediction.score >= this.detectionThreshold\n            );\n            \n            if (validPredictions.length > 0) {\n                this.drawPredictions(validPredictions);\n                \n                // Update object tracking and announce with smart system\n                this.updateObjectTracking(validPredictions);\n                this.announceDetectionsSmart(validPredictions);\n            } else {\n                // No objects detected, update tracking for disappearances\n                this.updateObjectTracking([]);\n            }\n            \n            // Continue detection loop\n            requestAnimationFrame(() => this.detectObjects());\n            \n        } catch (error) {\n            console.error('Detection error:', error);\n            // Continue detection even if one frame fails\n            setTimeout(() => this.detectObjects(), 100);\n        }\n    }\n\n    /**\n     * Draw bounding boxes and labels on canvas with improved styling\n     */\n    drawPredictions(predictions) {\n        // Clear previous drawings\n        this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);\n        \n        predictions.forEach((prediction, index) => {\n            const [x, y, width, height] = prediction.bbox;\n            const confidence = Math.round(prediction.score * 100);\n            const label = `${prediction.class} ${confidence}%`;\n            \n            // Color coding for different object types\n            let boxColor = '#00ff00'; // Default green\n            if (prediction.class === 'person') boxColor = '#ff6b6b'; // Red for people\n            else if (prediction.class.includes('vehicle') || prediction.class === 'car' || prediction.class === 'truck') boxColor = '#ffa500'; // Orange for vehicles\n            else if (prediction.class === 'chair' || prediction.class === 'couch') boxColor = '#4ecdc4'; // Teal for furniture\n            \n            // Draw bounding box with shadow for better visibility\n            this.ctx.shadowColor = 'rgba(0, 0, 0, 0.8)';\n            this.ctx.shadowBlur = 3;\n            this.ctx.strokeStyle = boxColor;\n            this.ctx.lineWidth = 3;\n            this.ctx.strokeRect(x, y, width, height);\n            \n            // Reset shadow for text\n            this.ctx.shadowBlur = 0;\n            \n            // Measure text to create proper background\n            this.ctx.font = 'bold 16px Arial';\n            const textMetrics = this.ctx.measureText(label);\n            const textWidth = textMetrics.width + 10;\n            const textHeight = 25;\n            \n            // Draw label background with some padding\n            this.ctx.fillStyle = boxColor;\n            this.ctx.fillRect(x, y - textHeight, textWidth, textHeight);\n            \n            // Draw label text\n            this.ctx.fillStyle = '#000000';\n            this.ctx.fillText(label, x + 5, y - 7);\n            \n            // Add distance indicator\n            const distance = this.estimateDistance(prediction.bbox);\n            this.ctx.font = 'bold 12px Arial';\n            this.ctx.fillStyle = '#ffffff';\n            this.ctx.fillText(distance, x + 5, y + height - 5);\n        });\n    }\n\n    /**\n     * Announce detected objects via speech with priority system\n     */\n    /**\n     * Update object tracking system for smart announcements\n     */\n    updateObjectTracking(predictions) {\n        const now = Date.now();\n        const currentDetectedObjects = new Set();\n        \n        // Extract object names from current predictions\n        predictions.forEach(prediction => {\n            currentDetectedObjects.add(prediction.class);\n        });\n        \n        // Update last seen time for currently detected objects\n        for (const objectName of currentDetectedObjects) {\n            this.objectLastSeen.set(objectName, now);\n            \n            // Remove from disappearance tracking if it reappeared\n            if (this.objectDisappearanceTime.has(objectName)) {\n                this.objectDisappearanceTime.delete(objectName);\n            }\n        }\n        \n        // Check for disappeared objects and mark their disappearance time\n        for (const [objectName, lastSeenTime] of this.objectLastSeen.entries()) {\n            if (!currentDetectedObjects.has(objectName) && !this.objectDisappearanceTime.has(objectName)) {\n                // Object just disappeared, mark the time\n                this.objectDisappearanceTime.set(objectName, now);\n            }\n        }\n        \n        // Clean up objects that have been gone for longer than cooldown period\n        for (const [objectName, disappearanceTime] of this.objectDisappearanceTime.entries()) {\n            if (now - disappearanceTime > this.cooldownPeriod) {\n                // Reset announcement count for objects that have been gone long enough\n                this.objectAnnouncementCount.delete(objectName);\n                this.objectLastSeen.delete(objectName);\n                this.objectDisappearanceTime.delete(objectName);\n            }\n        }\n    }\n\n    /**\n     * Smart announcement system with 3-announcement limit and cooldown\n     */\n    announceDetectionsSmart(predictions) {\n        const now = Date.now();\n        \n        // Respect global announcement cooldown\n        if (now - this.lastAnnouncement < this.announcementInterval) {\n            return;\n        }\n        \n        // Priority objects (most important for navigation)\n        const priorityObjects = ['person', 'chair', 'car', 'truck', 'bus', 'bicycle', 'motorcycle'];\n        \n        // Filter predictions that can be announced based on smart tracking\n        const announcablePredictions = predictions.filter(prediction => {\n            const announcementCount = this.objectAnnouncementCount.get(prediction.class) || 0;\n            \n            if (announcementCount >= this.maxAnnouncements) {\n                return false; // Already announced 3 times\n            }\n            \n            // Check if object was missing and came back (reset scenario)\n            const disappearanceTime = this.objectDisappearanceTime.get(prediction.class);\n            if (disappearanceTime && (now - disappearanceTime) < this.cooldownPeriod) {\n                return false; // Object reappeared too quickly, don't announce\n            }\n            \n            return true;\n        });\n        \n        if (announcablePredictions.length === 0) {\n            // Debug: Show why objects weren't announced\n            predictions.forEach(prediction => {\n                const count = this.objectAnnouncementCount.get(prediction.class) || 0;\n                const disappearanceTime = this.objectDisappearanceTime.get(prediction.class);\n                const timeSinceDisappearance = disappearanceTime ? (now - disappearanceTime) : null;\n                \n                if (count >= this.maxAnnouncements) {\n                    console.log(`${prediction.class}: Max announcements reached (${count}/${this.maxAnnouncements})`);\n                } else if (timeSinceDisappearance !== null && timeSinceDisappearance < this.cooldownPeriod) {\n                    console.log(`${prediction.class}: In cooldown (${Math.round(timeSinceDisappearance/1000)}s/${Math.round(this.cooldownPeriod/1000)}s)`);\n                }\n            });\n            return; // No objects to announce\n        }\n        \n        // Sort predictions by priority and distance\n        const sortedPredictions = announcablePredictions.sort((a, b) => {\n            const aPriority = priorityObjects.includes(a.class) ? 1 : 0;\n            const bPriority = priorityObjects.includes(b.class) ? 1 : 0;\n            \n            if (aPriority !== bPriority) {\n                return bPriority - aPriority; // Higher priority first\n            }\n            \n            // If same priority, sort by size (closer objects are larger)\n            const aSize = a.bbox[2] * a.bbox[3];\n            const bSize = b.bbox[2] * b.bbox[3];\n            return bSize - aSize;\n        });\n        \n        // Take only the most important objects (max 2)\n        const importantObjects = sortedPredictions.slice(0, 2);\n        \n        if (importantObjects.length > 0) {\n            // Increment announcement count for announced objects\n            importantObjects.forEach(prediction => {\n                const currentCount = this.objectAnnouncementCount.get(prediction.class) || 0;\n                this.objectAnnouncementCount.set(prediction.class, currentCount + 1);\n                \n                // Debug logging for smart announcement system\n                console.log(`Smart Announcement: ${prediction.class} (count: ${currentCount + 1}/${this.maxAnnouncements})`);\n            });\n            \n            const objectsWithDistance = importantObjects.map(prediction => {\n                const distance = this.estimateDistance(prediction.bbox);\n                const position = this.getRelativePosition(prediction.bbox);\n                return { \n                    name: prediction.class, \n                    distance: distance,\n                    position: position,\n                    confidence: Math.round(prediction.score * 100)\n                };\n            });\n            \n            // Create contextual announcement\n            let announcement = '';\n            objectsWithDistance.forEach((obj, index) => {\n                if (index > 0) announcement += '. Also, ';\n                \n                // More natural language\n                if (obj.name === 'person') {\n                    announcement += `person ${obj.position}, ${obj.distance}`;\n                } else {\n                    announcement += `${obj.name} ${obj.position}, ${obj.distance}`;\n                }\n            });\n            \n            this.speak(announcement, false, true); // Mark as object announcement for special delay handling\n            this.lastAnnouncement = now;\n        }\n    }\n\n    /**\n     * Legacy announcement method for backwards compatibility\n     */\n    announceDetections(predictions) {\n        // Redirect to smart announcement system\n        this.announceDetectionsSmart(predictions);\n    }\n    \n    /**\n     * Get relative position of object (left, center, right)\n     */\n    getRelativePosition(bbox) {\n        const [x, y, width, height] = bbox;\n        const centerX = x + width / 2;\n        const canvasCenter = this.canvas.width / 2;\n        const threshold = this.canvas.width * 0.25; // 25% threshold\n        \n        if (centerX < canvasCenter - threshold) {\n            return 'on your left';\n        } else if (centerX > canvasCenter + threshold) {\n            return 'on your right';\n        } else {\n            return 'ahead of you';\n        }\n    }\n\n    /**\n     * Estimate distance based on bounding box size (simplified)\n     */\n    estimateDistance(bbox) {\n        const [x, y, width, height] = bbox;\n        const area = width * height;\n        const videoArea = this.video.videoWidth * this.video.videoHeight;\n        const relativeSize = area / videoArea;\n        \n        if (relativeSize > 0.3) return 'very close';\n        if (relativeSize > 0.15) return '1 meter away';\n        if (relativeSize > 0.05) return '2 meters away';\n        return 'far away';\n    }\n\n    /**\n     * Process voice commands via Gemini API\n     */\n    async processVoiceCommand(command) {\n        console.log('Processing voice command:', command);\n        \n        // Filter out common meaningless phrases that might trigger false processing\n        const meaninglessPatterns = [\n            /^(um|uh|ah|er|hm|hmm|yes|yeah|no|okay|ok)$/i,\n            /^sorry.*didn'?t.*understand/i,\n            /^please try again/i,\n            /^try again/i,\n            /^what$/i,\n            /^\\s*$/,  // Empty or whitespace only\n            /^.{1,2}$/  // Very short commands (1-2 characters)\n        ];\n        \n        const isEmptyCommand = meaninglessPatterns.some(pattern => pattern.test(command.trim()));\n        \n        if (isEmptyCommand) {\n            console.log('Filtering out meaningless command:', command);\n            this.updateStatus('Ready for voice commands. Say \"Hey BlindMate\" or press Volume Up.', 'info');\n            return;\n        }\n        \n        try {\n            this.updateStatus('Processing your command...', 'primary');\n            \n            // Send command to Gemini API for processing\n            const response = await fetch('/api/process-command', {\n                method: 'POST',\n                headers: {\n                    'Content-Type': 'application/json',\n                },\n                body: JSON.stringify({\n                    command: command,\n                    language: this.currentLanguage,\n                    tone: this.currentTone\n                })\n            });\n            \n            if (!response.ok) {\n                throw new Error(`HTTP error! status: ${response.status}`);\n            }\n            \n            const result = await response.json();\n            console.log('Gemini response:', result);\n            \n            // Execute the action based on Gemini's response\n            console.log('About to execute action:', result.action, 'with destination:', result.destination);\n            await this.executeAction(result);\n            \n        } catch (error) {\n            console.error('Error processing command:', error);\n            \n            // Fallback to basic command processing without announcement\n            console.log('Using fallback processing for command:', command);\n            await this.fallbackCommandProcessing(command);\n        }\n    }\n    \n    /**\n     * Fallback command processing when Gemini is unavailable\n     */\n    async fallbackCommandProcessing(command) {\n        const cmd = command.toLowerCase();\n        \n        if ((cmd.includes('start') && cmd.includes('detection')) || cmd === 'start detection') {\n            if (!this.isDetecting) {\n                this.speak('Starting object detection', true);\n                await this.startDetection();\n                this.updateStatus('Object detection started via voice command', 'success');\n            } else {\n                this.speak('Detection is already running', true);\n            }\n        } else if (cmd.includes('stop') && !cmd.includes('navigation')) {\n            if (this.isDetecting) {\n                this.speak('Stopping detection', true);\n                this.stopDetection();\n                this.updateStatus('Object detection stopped via voice command', 'success');\n            } else {\n                this.speak('Detection is not currently running', true);\n            }\n        } else if (cmd.includes('location') || cmd.includes('where am i')) {\n            this.speak('Enabling location services', true);\n            this.requestLocation();\n        } else if (cmd.includes('take me') || cmd.includes('navigate') || cmd.includes('go to')) {\n            // Extract destination\n            let destination = cmd.replace(/take me to|navigate to|go to/g, '').trim();\n            \n            // Handle common phrase variations\n            if (cmd.includes('take me to the')) {\n                destination = cmd.replace(/take me to the/g, '').trim();\n            }\n            \n            if (destination) {\n                console.log('Navigation command detected:', cmd, 'Destination:', destination);\n                this.speak(`Navigating to ${destination}`, true);\n                await this.navigateToLocation(destination);\n            } else {\n                this.speak('Where would you like to go? Please say the name of any place, landmark, or address.', true);\n            }\n        } else if (cmd.includes('preview') || cmd.includes('route to')) {\n            // Extract destination for route preview\n            let destination = cmd.replace(/preview route to|route to|preview/g, '').trim();\n            if (destination) {\n                console.log('Preview command detected:', cmd, 'Destination:', destination);\n                await this.previewRoute(destination);\n            } else {\n                this.speak('Which location would you like to preview?', true);\n            }\n        } else if (cmd.includes('stop navigation') || cmd.includes('cancel navigation')) {\n            this.stopNavigation();\n        } else if (cmd.includes('language') && cmd.includes('hindi')) {\n            this.changeLanguage('hi-IN');\n        } else if (cmd.includes('language') && cmd.includes('english')) {\n            this.changeLanguage('en-IN');\n        } else if (cmd.includes('tutorial') || cmd.includes('help') || cmd.includes('guide') || cmd.includes('learn')) {\n            this.speak('Starting BlindMate tutorial. This will help you learn all the features.', true);\n            setTimeout(() => {\n                window.location.href = '/tutorial';\n            }, 2000);\n        } else {\n            this.speak('Command not recognized. Try saying start detection, navigate to a place, or tutorial for help.', true);\n        }\n    }\n\n    /**\n     * Execute actions based on Gemini response\n     */\n    async executeAction(result) {\n        console.log('Executing action:', result);\n        \n        if (!result.action) {\n            this.speak('I could not understand that command.');\n            return;\n        }\n        \n        // Update action status\n        this.updateActionStatus(result.response || 'Processing command...', 'info');\n        \n        // Execute the requested action\n        switch (result.action) {\n            case 'silent':\n                // Do nothing for meaningless commands - prevents false error messages\n                this.updateStatus('Ready for voice commands. Say \"Hey BlindMate\" or press Volume Up.', 'info');\n                return;\n                \n            case 'start_detection':\n                if (!this.isDetecting) {\n                    await this.startDetection();\n                    this.updateStatus('Object detection started via voice command', 'success');\n                } else {\n                    this.speak('Detection is already running', true);\n                }\n                break;\n                \n            case 'stop_detection':\n            case 'stop':\n                if (this.isDetecting) {\n                    this.stopDetection();\n                    this.updateStatus('Object detection stopped via voice command', 'success');\n                } else {\n                    this.speak('Detection is not currently running', true);\n                }\n                break;\n                \n            case 'navigate':\n                console.log('Processing navigate action with destination:', result.destination);\n                this.speak(result.response || 'Starting navigation...', true);\n                if (result.destination) {\n                    console.log('Calling navigateToLocation with:', result.destination);\n                    await this.navigateToLocation(result.destination);\n                } else {\n                    this.speak('I need a destination to navigate to. Please say the name of any place, landmark, or address.', true);\n                }\n                break;\n                \n            case 'show_map':\n                console.log('Showing navigation map');\n                this.speak(result.response || 'Showing navigation map...', true);\n                if (window.blindMateNavigation && window.blindMateNavigation.showNavigationMap) {\n                    window.blindMateNavigation.showNavigationMap();\n                } else {\n                    this.speak('Navigation map is not available. Please start navigation first.', true);\n                }\n                break;\n                \n            case 'emergency_stop':\n                console.log('Emergency stop navigation');\n                this.speak(result.response || 'Stopping navigation immediately...', true);\n                if (window.blindMateNavigation && window.blindMateNavigation.emergencyStop) {\n                    window.blindMateNavigation.emergencyStop();\n                } else {\n                    this.speak('Navigation is not currently active.', true);\n                }\n                break;\n                \n            case 'test_voice':\n                console.log('Testing voice recognition');\n                this.speak(result.response || 'Testing voice recognition...', true);\n                if (window.blindMateNavigation && window.blindMateNavigation.testVoiceRecognition) {\n                    window.blindMateNavigation.testVoiceRecognition();\n                } else {\n                    // Fallback test using main app's voice system\n                    this.testVoiceRecognitionFallback();\n                }\n                break;\n                \n            case 'toggle_obstacle_alerts':\n                console.log('Toggling obstacle alerts');\n                this.speak(result.response || 'Toggling obstacle alerts...', true);\n                if (window.blindMateNavigation && window.blindMateNavigation.toggleObstacleAlerts) {\n                    window.blindMateNavigation.toggleObstacleAlerts();\n                } else {\n                    this.speak('Obstacle alert system is not available.', true);\n                }\n                break;\n                \n            case 'preview_route':\n                console.log('Gemini preview action:', result.destination);\n                if (result.destination) {\n                    await this.previewRoute(result.destination);\n                } else {\n                    this.speak('I need a destination to preview the route', true);\n                }\n                break;\n                \n            case 'stop_navigation':\n                console.log('Gemini stop navigation action');\n                this.stopNavigation();\n                break;\n                \n            case 'enable_location':\n                await this.requestLocation();\n                break;\n                \n            case 'change_language':\n                if (result.language) {\n                    this.changeLanguage(result.language);\n                } else {\n                    this.speak('Language not supported', true);\n                }\n                break;\n                \n            case 'change_tone':\n                if (result.tone) {\n                    this.changeTone(result.tone);\n                } else {\n                    this.speak('Tone not supported', true);\n                }\n                break;\n                \n            case 'get_location':\n                if (this.userLocation) {\n                    this.speak(`You are currently at latitude ${this.userLocation.latitude.toFixed(4)}, longitude ${this.userLocation.longitude.toFixed(4)}`, true);\n                } else {\n                    this.speak('Location not available. Please enable location services first.', true);\n                }\n                break;\n                \n            default:\n                console.log('Unknown action:', action);\n                if (!response) {\n                    this.speak('I understood your command but could not perform the action.', true);\n                }\n        }\n    }\n    \n    /**\n     * Fallback voice recognition test using main app system\n     */\n    testVoiceRecognitionFallback() {\n        console.log('Testing voice recognition via main app fallback');\n        \n        this.speak('Voice recognition test starting. Please say something after the prompt.', true);\n        \n        setTimeout(() => {\n            if (!this.commandRecognition) {\n                this.speak('Voice recognition is not available on this device.', true);\n                return;\n            }\n            \n            try {\n                this.commandRecognition.onresult = (event) => {\n                    const transcript = event.results[0][0].transcript;\n                    console.log('Voice test result:', transcript);\n                    this.speak(`Voice recognition working perfectly. I heard: ${transcript}`, true);\n                    \n                    // Restore normal command processing\n                    this.setupVoiceRecognition();\n                };\n                \n                this.commandRecognition.start();\n                this.speak('Now listening for your test voice command.', true);\n                \n            } catch (error) {\n                console.error('Voice test failed:', error);\n                this.speak('Voice recognition test failed. Please check your microphone permissions.', true);\n            }\n        }, 2000);\n    }\n\n    /**\n     * Navigate to any worldwide destination using Google APIs\n     */\n    async navigateToLocation(destination) {\n        console.log('navigateToLocation called with:', destination);\n        \n        if (!this.userLocation) {\n            this.speak('Location access is required for navigation. Please enable location first.', true);\n            await this.requestLocation();\n            if (!this.userLocation) {\n                return;\n            }\n        }\n        \n        try {\n            this.updateStatus(`Getting directions to ${destination}...`, 'primary');\n            this.speak(`Getting directions to ${destination}`, true);\n            \n            console.log('User location:', this.userLocation);\n            console.log('Destination:', destination);\n            \n            // Use the enhanced navigation system that handles geocoding + directions\n            if (window.blindMateNavigation && typeof window.blindMateNavigation.startNavigation === 'function') {\n                console.log('Using enhanced navigation system');\n                window.blindMateNavigation.currentDestination = destination;\n                await window.blindMateNavigation.startNavigation(destination);\n            } else {\n                console.log('Enhanced navigation not available, using fallback');\n                // Fallback to direct API call\n                const response = await fetch('/api/directions', {\n                    method: 'POST',\n                    headers: { 'Content-Type': 'application/json' },\n                    body: JSON.stringify({\n                        origin: `${this.userLocation.latitude},${this.userLocation.longitude}`,\n                        destination: destination\n                    })\n                });\n                \n                const data = await response.json();\n                \n                if (data.success) {\n                    this.currentRoute = data;\n                    this.currentStepIndex = 0;\n                    this.isNavigating = true;\n                    \n                    // Update navigation status\n                    if (this.elements.navigationStatus) {\n                        this.elements.navigationStatus.textContent = 'Navigating';\n                        this.elements.navigationStatus.className = 'badge bg-success';\n                    }\n                    \n                    // Speak route overview\n                    await this.speakRouteOverview(data, destination);\n                    \n                    // Start position tracking for rerouting\n                    this.startLocationTracking();\n                    \n                    this.updateStatus(`Navigating to ${destination}`, 'success');\n                } else {\n                    this.speak(data.message || `Could not get directions to ${destination}. Please try again.`, true);\n                }\n            }\n            \n        } catch (error) {\n            console.error('Navigation error:', error);\n            this.speak(`Sorry, I couldn't get directions to ${destination}. Please try again.`, true);\n        }\n    }\n    \n    /**\n     * Preview route to any destination without starting navigation\n     */\n    async previewRoute(destination) {\n        console.log('previewRoute called with:', destination);\n        \n        if (!this.userLocation) {\n            this.speak('Location access is required for route preview. Please enable location first.', true);\n            await this.requestLocation();\n            if (!this.userLocation) {\n                return;\n            }\n        }\n        \n        try {\n            this.updateStatus(`Previewing route to ${destination}...`, 'primary');\n            \n            // Use the enhanced navigation system for route preview\n            const response = await fetch('/api/directions', {\n                method: 'POST',\n                headers: { 'Content-Type': 'application/json' },\n                body: JSON.stringify({\n                    origin: `${this.userLocation.latitude},${this.userLocation.longitude}`,\n                    destination: destination\n                })\n            });\n            \n            const data = await response.json();\n            \n            if (data.success) {\n                await this.speakRoutePreview(data, destination);\n                this.updateStatus(`Route preview completed for ${destination}`, 'success');\n            } else {\n                this.speak(data.message || `Could not get route preview to ${destination}. Please try again.`, true);\n            }\n            \n        } catch (error) {\n            console.error('Route preview error:', error);\n            this.speak(`Sorry, I couldn't preview the route to ${destination}`, true);\n        }\n    }\n    \n    /**\n     * Get directions from Google Maps API\n     */\n    async getDirections(originLat, originLng, destLat, destLng) {\n        try {\n            // Use backend proxy to avoid exposing API key\n            const response = await fetch('/api/directions', {\n                method: 'POST',\n                headers: {\n                    'Content-Type': 'application/json',\n                },\n                body: JSON.stringify({\n                    origin: `${originLat},${originLng}`,\n                    destination: `${destLat},${destLng}`,\n                    mode: 'walking'\n                })\n            });\n            \n            if (!response.ok) {\n                throw new Error(`HTTP error! status: ${response.status}`);\n            }\n            \n            const data = await response.json();\n            \n            if (data.status === 'OK' && data.routes && data.routes.length > 0) {\n                return data.routes[0];\n            } else {\n                throw new Error('No routes found');\n            }\n            \n        } catch (error) {\n            console.error('Directions API error:', error);\n            // Fallback: calculate straight-line distance and basic directions\n            return this.getFallbackDirections(originLat, originLng, destLat, destLng);\n        }\n    }\n    \n    /**\n     * Fallback directions when API is unavailable\n     */\n    getFallbackDirections(originLat, originLng, destLat, destLng) {\n        const distance = this.calculateDistance(originLat, originLng, destLat, destLng);\n        const bearing = this.calculateBearing(originLat, originLng, destLat, destLng);\n        const direction = this.getDirectionFromBearing(bearing);\n        \n        return {\n            legs: [{\n                distance: { text: `${Math.round(distance)} meters`, value: distance },\n                duration: { text: `${Math.round(distance / 1.4)} minutes`, value: Math.round(distance / 1.4) * 60 },\n                steps: [{\n                    distance: { text: `${Math.round(distance)} meters`, value: distance },\n                    duration: { text: `${Math.round(distance / 1.4)} minutes`, value: Math.round(distance / 1.4) * 60 },\n                    html_instructions: `Walk ${direction} for ${Math.round(distance)} meters`,\n                    start_location: { lat: originLat, lng: originLng },\n                    end_location: { lat: destLat, lng: destLng }\n                }]\n            }]\n        };\n    }\n    \n    /**\n     * Speak route overview when starting navigation\n     */\n    async speakRouteOverview(route, destinationName) {\n        const routeData = route.route || route;\n        const totalDistance = routeData.distance;\n        const totalTime = routeData.duration;\n        \n        this.speak(`Navigation started to ${destinationName}. You are ${totalDistance} away. Estimated walking time: ${totalTime}`, true);\n        \n        // Speak first 2-3 steps\n        const steps = routeData.steps ? routeData.steps.slice(0, 2) : [];\n        for (let i = 0; i < steps.length; i++) {\n            const step = steps[i];\n            const instruction = step.instruction || this.cleanHtmlInstructions(step.html_instructions || step.instruction);\n            \n            setTimeout(() => {\n                this.speak(`Step ${i + 1}: ${instruction}`, true);\n            }, (i + 1) * 3000);\n        }\n    }\n    \n    /**\n     * Speak route preview (first few steps only)\n     */\n    async speakRoutePreview(route, destinationName) {\n        const leg = route.legs[0];\n        const totalDistance = leg.distance.text;\n        const totalTime = leg.duration.text;\n        \n        this.speak(`Route preview to ${destinationName}: ${totalDistance}, about ${totalTime} walking`, true);\n        \n        setTimeout(() => {\n            if (leg.steps.length > 0) {\n                const firstStep = this.cleanHtmlInstructions(leg.steps[0].html_instructions);\n                this.speak(`First step: ${firstStep}`, true);\n            }\n        }, 2000);\n        \n        if (leg.steps.length > 1) {\n            setTimeout(() => {\n                const secondStep = this.cleanHtmlInstructions(leg.steps[1].html_instructions);\n                this.speak(`Then: ${secondStep}`, true);\n            }, 4000);\n        }\n    }\n    \n    /**\n     * Clean HTML instructions from Google Maps API\n     */\n    cleanHtmlInstructions(htmlInstructions) {\n        return htmlInstructions\n            .replace(/<[^>]*>/g, '') // Remove HTML tags\n            .replace(/&nbsp;/g, ' ') // Replace non-breaking spaces\n            .replace(/&amp;/g, '&') // Replace HTML entities\n            .trim();\n    }\n    \n    /**\n     * Start location tracking for rerouting\n     */\n    startLocationTracking() {\n        if (!navigator.geolocation) {\n            console.warn('Geolocation not supported for tracking');\n            return;\n        }\n        \n        // Watch position every 5 seconds\n        this.locationWatcher = navigator.geolocation.watchPosition(\n            (position) => {\n                this.checkRouteDeviation(position.coords.latitude, position.coords.longitude);\n            },\n            (error) => {\n                console.error('Location tracking error:', error);\n            },\n            {\n                enableHighAccuracy: true,\n                timeout: 10000,\n                maximumAge: 5000\n            }\n        );\n    }\n    \n    /**\n     * Check if user has deviated from the route\n     */\n    checkRouteDeviation(currentLat, currentLng) {\n        if (!this.isNavigating || !this.currentRoute) return;\n        \n        const currentStep = this.currentRoute.legs[0].steps[this.currentStepIndex];\n        if (!currentStep) return;\n        \n        // Calculate distance to expected route point\n        const expectedLat = currentStep.start_location.lat;\n        const expectedLng = currentStep.start_location.lng;\n        const deviation = this.calculateDistance(currentLat, currentLng, expectedLat, expectedLng);\n        \n        // If user is too far off track, reroute\n        if (deviation > this.routeDeviationThreshold) {\n            this.speak('You have moved off the path. Recalculating route...', true);\n            this.reroute(currentLat, currentLng);\n        }\n    }\n    \n    /**\n     * Reroute from current position\n     */\n    async reroute(currentLat, currentLng) {\n        if (!this.isNavigating) return;\n        \n        // Find the destination from current route\n        const originalDestination = this.currentRoute.legs[0].end_location;\n        \n        try {\n            const newRoute = await this.getDirections(\n                currentLat, currentLng,\n                originalDestination.lat, originalDestination.lng\n            );\n            \n            if (newRoute) {\n                this.currentRoute = newRoute;\n                this.currentStepIndex = 0;\n                \n                const leg = newRoute.legs[0];\n                this.speak(`New route calculated. ${leg.distance.text} remaining.`, true);\n                \n                // Speak next instruction\n                if (leg.steps.length > 0) {\n                    setTimeout(() => {\n                        const instruction = this.cleanHtmlInstructions(leg.steps[0].html_instructions);\n                        this.speak(instruction, true);\n                    }, 2000);\n                }\n            }\n        } catch (error) {\n            console.error('Rerouting error:', error);\n            this.speak('Could not recalculate route. Please use your navigation app.', true);\n        }\n    }\n    \n    /**\n     * Stop navigation and location tracking\n     */\n    stopNavigation() {\n        this.isNavigating = false;\n        this.currentRoute = null;\n        this.currentStepIndex = 0;\n        \n        if (this.locationWatcher) {\n            navigator.geolocation.clearWatch(this.locationWatcher);\n            this.locationWatcher = null;\n        }\n        \n        this.speak('Navigation stopped', true);\n        this.updateStatus('Navigation stopped', 'info');\n        \n        // Update navigation status\n        this.elements.navigationStatus.textContent = 'Ready';\n        this.elements.navigationStatus.className = 'badge bg-secondary';\n    }\n    \n    /**\n     * Calculate distance between two points in meters\n     */\n    calculateDistance(lat1, lng1, lat2, lng2) {\n        const R = 6371e3; // Earth's radius in meters\n        const œÜ1 = lat1 * Math.PI / 180;\n        const œÜ2 = lat2 * Math.PI / 180;\n        const ŒîœÜ = (lat2 - lat1) * Math.PI / 180;\n        const ŒîŒª = (lng2 - lng1) * Math.PI / 180;\n        \n        const a = Math.sin(ŒîœÜ/2) * Math.sin(ŒîœÜ/2) +\n                Math.cos(œÜ1) * Math.cos(œÜ2) *\n                Math.sin(ŒîŒª/2) * Math.sin(ŒîŒª/2);\n        const c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1-a));\n        \n        return R * c;\n    }\n    \n    /**\n     * Calculate bearing between two points\n     */\n    calculateBearing(lat1, lng1, lat2, lng2) {\n        const œÜ1 = lat1 * Math.PI / 180;\n        const œÜ2 = lat2 * Math.PI / 180;\n        const ŒîŒª = (lng2 - lng1) * Math.PI / 180;\n        \n        const y = Math.sin(ŒîŒª) * Math.cos(œÜ2);\n        const x = Math.cos(œÜ1) * Math.sin(œÜ2) - Math.sin(œÜ1) * Math.cos(œÜ2) * Math.cos(ŒîŒª);\n        \n        const Œ∏ = Math.atan2(y, x);\n        return (Œ∏ * 180 / Math.PI + 360) % 360;\n    }\n    \n    /**\n     * Get direction name from bearing\n     */\n    getDirectionFromBearing(bearing) {\n        const directions = ['north', 'northeast', 'east', 'southeast', 'south', 'southwest', 'west', 'northwest'];\n        const index = Math.round(bearing / 45) % 8;\n        return directions[index];\n    }\n\n    /**\n     * Provide basic navigation assistance\n     */\n    provideNavigationGuidance(destination) {\n        const guidance = [\n            `I'm helping you navigate to ${destination}.`,\n            \"Since I opened navigation in your maps app, please follow the turn-by-turn directions there.\",\n            \"You can still use voice commands with me:\",\n            \"Say 'start detection' to scan for obstacles while walking.\",\n            \"Say 'stop' to pause any features.\",\n            \"Stay safe and be aware of your surroundings.\"\n        ];\n        \n        guidance.forEach((message, index) => {\n            setTimeout(() => this.speak(message), index * 3000);\n        });\n    }\n\n    /**\n     * Request user location\n     */\n    async requestLocation() {\n        try {\n            this.updateStatus('Requesting location access...', 'warning');\n            \n            const position = await new Promise((resolve, reject) => {\n                navigator.geolocation.getCurrentPosition(resolve, reject, {\n                    enableHighAccuracy: true,\n                    timeout: 10000,\n                    maximumAge: 60000\n                });\n            });\n            \n            this.userLocation = {\n                latitude: position.coords.latitude,\n                longitude: position.coords.longitude\n            };\n            \n            this.updateStatus('Location access granted.', 'success');\n            this.elements.locationBtn.className = 'btn btn-success btn-lg';\n            this.elements.locationBtn.innerHTML = '<i class=\"fas fa-check\" aria-hidden=\"true\"></i> Location Enabled';\n            \n            this.speak('Location access granted. I can now provide navigation assistance.');\n            \n        } catch (error) {\n            console.error('Location error:', error);\n            this.updateStatus('Location access denied.', 'danger');\n            this.speak('Location access is required for navigation features. Please enable location in your browser settings.');\n        }\n    }\n\n    /**\n     * Change application language\n     */\n    changeLanguage(langCode) {\n        console.log('Changing language to:', langCode);\n        \n        this.currentLanguage = langCode;\n        this.elements.languageSelect.value = langCode;\n        \n        // Update recognition language\n        if (this.commandRecognition) {\n            this.commandRecognition.lang = langCode;\n        }\n        if (this.continuousRecognition) {\n            this.continuousRecognition.lang = langCode;\n        }\n        \n        // Update language preference on server\n        this.updateServerPreferences();\n        \n        this.speak(`Language changed to ${this.getLanguageName(langCode)}`);\n    }\n\n    /**\n     * Change voice tone\n     */\n    changeTone(tone) {\n        console.log('Changing tone to:', tone);\n        \n        this.currentTone = tone;\n        this.elements.toneSelect.value = tone;\n        \n        // Update tone preference on server\n        this.updateServerPreferences();\n        \n        // Speak confirmation with new tone\n        this.speak(`Voice tone changed to ${tone}`, true);\n    }\n\n    /**\n     * Update preferences on server\n     */\n    async updateServerPreferences() {\n        try {\n            await fetch('/api/preferences', {\n                method: 'POST',\n                headers: { 'Content-Type': 'application/json' },\n                body: JSON.stringify({\n                    language: this.currentLanguage,\n                    tone: this.currentTone\n                })\n            });\n        } catch (error) {\n            console.error('Error updating preferences:', error);\n        }\n    }\n\n    /**\n     * Load preferences from server\n     */\n    async loadServerPreferences() {\n        try {\n            const response = await fetch('/api/preferences');\n            const preferences = await response.json();\n            \n            if (preferences.language) {\n                this.currentLanguage = preferences.language;\n                this.elements.languageSelect.value = preferences.language;\n            }\n            \n            if (preferences.tone) {\n                this.currentTone = preferences.tone;\n                this.elements.toneSelect.value = preferences.tone;\n            }\n        } catch (error) {\n            console.error('Error loading preferences:', error);\n        }\n    }\n\n    /**\n     * Get display name for language code\n     */\n    getLanguageName(langCode) {\n        const languageNames = {\n            'en-IN': 'English',\n            'hi-IN': 'Hindi',\n            'ta-IN': 'Tamil',\n            'te-IN': 'Telugu',\n            'bn-IN': 'Bengali',\n            'mr-IN': 'Marathi',\n            'gu-IN': 'Gujarati',\n            'es-ES': 'Spanish',\n            'fr-FR': 'French',\n            'de-DE': 'German',\n            'it-IT': 'Italian',\n            'pt-PT': 'Portuguese',\n            'ja-JP': 'Japanese',\n            'zh-CN': 'Chinese',\n            'ar-SA': 'Arabic'\n        };\n        return languageNames[langCode] || langCode;\n    }\n\n    /**\n     * Get tone-specific voice settings\n     */\n    getToneSettings(tone) {\n        const toneSettings = {\n            'friendly': { rate: 0.9, pitch: 1.1, volume: 0.8 },\n            'formal': { rate: 0.7, pitch: 0.9, volume: 0.8 },\n            'energetic': { rate: 1.1, pitch: 1.2, volume: 0.9 },\n            'calm': { rate: 0.6, pitch: 0.8, volume: 0.7 },\n            'robotic': { rate: 0.8, pitch: 0.7, volume: 0.8 }\n        };\n        return toneSettings[tone] || toneSettings['friendly'];\n    }\n\n    /**\n     * Find appropriate voice for tone\n     */\n    findVoiceForTone(voices, language, tone) {\n        // Try to find voices that match tone characteristics\n        const langVoices = voices.filter(v => v.lang === language || v.lang.startsWith(language.split('-')[0]));\n        \n        if (langVoices.length === 0) return null;\n        \n        // Different tone preferences for voice selection\n        switch (tone) {\n            case 'formal':\n                return langVoices.find(v => v.name.toLowerCase().includes('professional') || \n                                          v.name.toLowerCase().includes('formal')) || langVoices[0];\n            case 'energetic':\n                return langVoices.find(v => v.name.toLowerCase().includes('young') || \n                                          v.name.toLowerCase().includes('bright')) || langVoices[0];\n            case 'calm':\n                return langVoices.find(v => v.name.toLowerCase().includes('calm') || \n                                          v.name.toLowerCase().includes('soft')) || langVoices[0];\n            case 'robotic':\n                return langVoices.find(v => v.name.toLowerCase().includes('robotic') || \n                                          v.name.toLowerCase().includes('computer')) || langVoices[0];\n            default:\n                return langVoices[0];\n        }\n    }\n\n    /**\n     * Text-to-speech function with queue management and cooldown\n     */\n    speak(text, priority = false, isObjectAnnouncement = false) {\n        if (!this.synth || !text) {\n            return;\n        }\n\n        const now = Date.now();\n        \n        // Handle object announcements with special delay logic\n        if (isObjectAnnouncement && !priority) {\n            this._handleObjectAnnouncement(text, now);\n            return;\n        }\n        \n        // If high priority or enough time has passed since last speech\n        if (priority || (now - this.lastSpeechTime > this.speechCooldown && !this.isSpeaking)) {\n            this._speakNow(text);\n        } else if (!priority) {\n            // Add to queue for non-priority speech\n            this.speechQueue.push(text);\n            if (!this.isSpeaking) {\n                this._processNextSpeech();\n            }\n        }\n    }\n    \n    /**\n     * Handle object announcements with special delay logic\n     */\n    _handleObjectAnnouncement(text, now) {\n        // Cancel any pending announcement\n        if (this.speechDelayTimer) {\n            clearTimeout(this.speechDelayTimer);\n            this.speechDelayTimer = null;\n        }\n        \n        // Store the pending announcement\n        this.pendingAnnouncement = text;\n        \n        // Calculate delay needed\n        const timeSinceLastSpeech = now - this.lastSpeechTime;\n        const minimumDelay = this.minObjectAnnouncementDelay;\n        \n        if (this.isSpeaking || timeSinceLastSpeech < minimumDelay) {\n            // Need to delay announcement\n            const delayNeeded = this.isSpeaking ? \n                minimumDelay : // Wait full delay if currently speaking\n                minimumDelay - timeSinceLastSpeech; // Wait remaining time\n                \n            this.isAnnouncementDelayed = true;\n            \n            console.log(`Object announcement delayed by ${delayNeeded}ms for clarity`);\n            \n            this.speechDelayTimer = setTimeout(() => {\n                if (this.pendingAnnouncement) {\n                    this._speakNow(this.pendingAnnouncement);\n                    this.pendingAnnouncement = null;\n                    this.isAnnouncementDelayed = false;\n                }\n            }, delayNeeded);\n        } else {\n            // Can announce immediately\n            this._speakNow(text);\n            this.pendingAnnouncement = null;\n        }\n    }\n\n    /**\n     * Internal function to speak immediately\n     */\n    _speakNow(text) {\n        try {\n            // Cancel any ongoing speech immediately to prevent overlaps\n            this.synth.cancel();\n            \n            // Small delay to ensure cancellation is processed\n            setTimeout(() => {\n                this.isSpeaking = true;\n                this.lastSpeechTime = Date.now();\n                \n                const utterance = new SpeechSynthesisUtterance(text);\n                utterance.lang = this.currentLanguage;\n                \n                // Apply tone-specific voice settings\n                const toneSettings = this.getToneSettings(this.currentTone);\n                utterance.rate = toneSettings.rate;\n                utterance.pitch = toneSettings.pitch;\n                utterance.volume = toneSettings.volume;\n                \n                // Find appropriate voice based on language and tone\n                const voices = this.synth.getVoices();\n                if (voices.length > 0) {\n                    let voice = this.findVoiceForTone(voices, this.currentLanguage, this.currentTone);\n                    \n                    if (!voice) {\n                        voice = voices.find(v => v.lang === this.currentLanguage) || \n                               voices.find(v => v.lang.startsWith(this.currentLanguage.split('-')[0])) ||\n                               voices.find(v => v.default);\n                    }\n                    \n                    if (voice) {\n                        utterance.voice = voice;\n                    }\n                }\n                \n                utterance.onstart = () => {\n                    console.log('Speech started successfully');\n                };\n                \n                utterance.onend = () => {\n                    console.log('Speech ended normally');\n                    this.isSpeaking = false;\n                    // Longer delay before next speech for better clarity\n                    setTimeout(() => this._processNextSpeech(), 750);\n                };\n                \n                utterance.onerror = (event) => {\n                    console.warn('Speech error:', event);\n                    this.isSpeaking = false;\n                    setTimeout(() => this._processNextSpeech(), 750);\n                };\n                \n                this.synth.speak(utterance);\n                \n            }, 50); // Small delay to ensure proper cancellation\n            \n        } catch (error) {\n            this.isSpeaking = false;\n            console.warn('Speech synthesis error:', error);\n        }\n    }\n    \n    /**\n     * Process next item in speech queue\n     */\n    _processNextSpeech() {\n        if (this.speechQueue.length > 0 && !this.isSpeaking) {\n            const text = this.speechQueue.shift();\n            this._speakNow(text);\n        }\n    }\n\n    /**\n     * Check if this is a first-time user and offer tutorial\n     */\n    checkFirstTimeUser() {\n        const hasCompletedTutorial = localStorage.getItem('blindmate_tutorial_completed');\n        const hasUsedApp = localStorage.getItem('blindmate_first_use');\n        \n        if (!hasCompletedTutorial && !hasUsedApp) {\n            // Mark that the user has seen the app\n            localStorage.setItem('blindmate_first_use', 'true');\n            \n            // Wait a moment for the interface to load, then offer tutorial\n            setTimeout(() => {\n                this.speak('Welcome to BlindMate! This is your first time using the app. Would you like to start with a guided tutorial to learn all the features? You can also access the tutorial anytime by saying \"start tutorial\" or clicking the tutorial button.');\n                \n                // Show tutorial button prominently\n                const tutorialButton = document.getElementById('tutorialButton');\n                if (tutorialButton) {\n                    tutorialButton.classList.add('btn-warning');\n                    tutorialButton.innerHTML = '<i class=\"fas fa-graduation-cap\"></i> Recommended: Start Tutorial';\n                }\n            }, 2000);\n        }\n    }\n\n    /**\n     * Update system status display\n     */\n    updateStatus(message, type = 'info') {\n        if (this.elements && this.elements.systemStatus) {\n            this.elements.systemStatus.textContent = message;\n            this.elements.systemStatus.className = `alert alert-${type}`;\n            \n            // Auto-clear success and warning messages\n            if (type === 'success' || type === 'warning') {\n                setTimeout(() => {\n                    if (this.elements.systemStatus && this.elements.systemStatus.textContent === message) {\n                        this.updateStatus('System ready', 'info');\n                    }\n                }, 5000);\n            }\n        } else {\n            console.log('Status update:', message, type);\n        }\n    }\n}\n\n// Initialize the application when the page loads\ndocument.addEventListener('DOMContentLoaded', () => {\n    window.blindMate = new BlindMate();\n});\n\n// Handle page visibility changes to pause/resume detection\ndocument.addEventListener('visibilitychange', () => {\n    if (window.blindMate) {\n        if (document.hidden && window.blindMate.isDetecting) {\n            // Pause detection when page is hidden\n            window.blindMate.isDetecting = false;\n        } else if (!document.hidden && window.blindMate.stream) {\n            // Resume detection when page becomes visible\n            window.blindMate.isDetecting = true;\n            window.blindMate.detectObjects();\n        }\n    }\n});\n","size_bytes":114521},"LOCAL_SETUP_GUIDE.md":{"content":"# BlindMate - Local Development Setup Guide\n\n## üìÅ Project Structure\n\n```\nblindmate/\n‚îú‚îÄ‚îÄ app.py                          # Main Flask application\n‚îú‚îÄ‚îÄ main.py                         # Application entry point\n‚îú‚îÄ‚îÄ gemini_service.py              # Google Gemini AI service\n‚îú‚îÄ‚îÄ requirements-local.txt         # Python dependencies\n‚îú‚îÄ‚îÄ setup-local.py                 # Automated setup script\n‚îú‚îÄ‚îÄ LOCAL_SETUP_GUIDE.md          # This guide\n‚îú‚îÄ‚îÄ pyproject.toml                 # Project configuration\n‚îú‚îÄ‚îÄ .env.template                  # Environment variables template\n‚îú‚îÄ‚îÄ templates/                     # HTML templates\n‚îÇ   ‚îú‚îÄ‚îÄ index.html                # Main application page\n‚îÇ   ‚îú‚îÄ‚îÄ navigation.html           # Navigation page\n‚îÇ   ‚îú‚îÄ‚îÄ onboarding.html          # Tutorial/onboarding page\n‚îÇ   ‚îî‚îÄ‚îÄ simple_navigation.html   # Simple navigation interface\n‚îú‚îÄ‚îÄ static/                       # Static files (CSS, JS, assets)\n‚îÇ   ‚îú‚îÄ‚îÄ css/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ styles.css           # Main stylesheet\n‚îÇ   ‚îú‚îÄ‚îÄ js/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.js               # Main application JavaScript\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ navigation.js        # Navigation system\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ onboarding.js        # Tutorial JavaScript\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sw.js                # Service worker for PWA\n‚îÇ   ‚îî‚îÄ‚îÄ assets/                  # Images, icons, etc.\n‚îî‚îÄ‚îÄ docs/                        # Documentation\n    ‚îú‚îÄ‚îÄ README.md                # Project documentation\n    ‚îî‚îÄ‚îÄ replit.md               # Development notes\n```\n\n## üöÄ Quick Start (Automated Setup)\n\n### Step 1: Prerequisites\n- Python 3.11 or higher installed\n- Git (optional, for cloning)\n- Modern web browser (Chrome, Firefox, Edge)\n\n### Step 2: Download the Project\n```bash\n# If using Git\ngit clone <repository-url>\ncd blindmate\n\n# OR manually download and extract the project files\n```\n\n### Step 3: Run Automated Setup\n```bash\npython setup-local.py\n```\n\nThis script will:\n- ‚úÖ Check Python version compatibility\n- ‚úÖ Create virtual environment (`venv/`)\n- ‚úÖ Install all dependencies\n- ‚úÖ Create environment template file\n\n### Step 4: Configure Environment\n```bash\n# Copy the template\ncp .env.template .env\n\n# Edit .env with your API keys\nnano .env  # or use any text editor\n```\n\nAdd your API keys to `.env`:\n```bash\n# Database (SQLite for local development)\nDATABASE_URL=sqlite:///blindmate.db\n\n# Google Gemini AI (required)\nGEMINI_API_KEY=your_gemini_api_key_here\n\n# Session security\nSESSION_SECRET=your_random_secret_key_here\n\n# Google Maps (optional but recommended)\nGOOGLE_MAPS_API_KEY=your_google_maps_api_key_here\n```\n\n### Step 5: Activate Environment and Run\n```bash\n# Windows\nvenv\\Scripts\\activate\n\n# macOS/Linux\nsource venv/bin/activate\n\n# Run the application\npython main.py\n```\n\n### Step 6: Access the Application\nOpen your browser and go to: `http://localhost:5000`\n\n---\n\n## üõ†Ô∏è Manual Setup (Alternative)\n\n### Step 1: Create Virtual Environment\n```bash\npython -m venv venv\n```\n\n### Step 2: Activate Virtual Environment\n```bash\n# Windows\nvenv\\Scripts\\activate\n\n# macOS/Linux\nsource venv/bin/activate\n```\n\n### Step 3: Install Dependencies\n```bash\npip install -r requirements-local.txt\n```\n\n### Step 4: Set Environment Variables\nCreate a `.env` file:\n```bash\nDATABASE_URL=sqlite:///blindmate.db\nGEMINI_API_KEY=your_actual_api_key\nSESSION_SECRET=your_secret_key\nGOOGLE_MAPS_API_KEY=your_maps_key\n```\n\n### Step 5: Run the Application\n```bash\npython main.py\n```\n\n---\n\n## üîë Getting API Keys\n\n### Google Gemini API Key (Required)\n1. Visit [Google AI Studio](https://aistudio.google.com)\n2. Sign in with your Google account\n3. Click \"Create API Key\"\n4. Copy the key and add it to your `.env` file\n\n### Google Maps API Key (Optional)\n1. Go to [Google Cloud Console](https://console.cloud.google.com)\n2. Create a new project or select existing\n3. Enable \"Maps JavaScript API\" and \"Directions API\"\n4. Create credentials (API Key)\n5. Restrict the key to your domain for security\n6. Add it to your `.env` file\n\n---\n\n## üìã System Requirements\n\n### Minimum Requirements\n- **Python**: 3.11 or higher\n- **RAM**: 4GB minimum, 8GB recommended\n- **Disk Space**: 500MB for dependencies\n- **Browser**: Chrome 80+, Firefox 75+, Edge 80+, Safari 13+\n- **Internet**: Required for AI services and maps\n\n### Browser Permissions Required\n- üì∑ **Camera Access**: For object detection\n- üé§ **Microphone Access**: For voice commands\n- üìç **Location Access**: For navigation features\n\n---\n\n## üß™ Testing the Installation\n\n### 1. Basic Functionality Test\n```bash\n# Check if the server starts\npython main.py\n\n# Should see output like:\n# * Running on http://127.0.0.1:5000\n```\n\n### 2. Web Interface Test\n1. Open `http://localhost:5000`\n2. Allow camera and microphone permissions\n3. You should see the BlindMate interface\n\n### 3. Voice Command Test\n1. Click \"Start Detection\" or say \"Hey BlindMate\"\n2. Try saying: \"Start detection\"\n3. Camera should activate and detect objects\n\n### 4. Navigation Test\n1. Say: \"Take me to Central Park\"\n2. Should ask for confirmation and start navigation\n\n---\n\n## üö® Troubleshooting\n\n### Common Issues\n\n**‚ùå \"Python not found\"**\n```bash\n# Solution: Install Python 3.11+ from python.org\n# Verify with: python --version\n```\n\n**‚ùå \"Permission denied\" errors**\n```bash\n# Windows: Run as administrator\n# macOS/Linux: Use sudo for system packages\n```\n\n**‚ùå \"Module not found\" errors**\n```bash\n# Make sure virtual environment is activated\nsource venv/bin/activate  # macOS/Linux\nvenv\\Scripts\\activate     # Windows\n\n# Reinstall dependencies\npip install -r requirements-local.txt\n```\n\n**‚ùå \"Camera not working\"**\n- Check browser permissions (chrome://settings/content/camera)\n- Close other applications using the camera\n- Try a different browser\n\n**‚ùå \"Voice commands not responding\"**\n- Check microphone permissions\n- Ensure you're speaking clearly\n- Verify GEMINI_API_KEY is set correctly\n\n**‚ùå \"Navigation not working\"**\n- Add GOOGLE_MAPS_API_KEY to .env\n- Enable location permissions in browser\n- Check internet connection\n\n### Log Debugging\n```bash\n# Check application logs\ntail -f app.log\n\n# Check browser console (F12 -> Console tab)\n# Look for error messages in red\n```\n\n---\n\n## üåê Production Deployment\n\n### Option 1: Replit (Recommended)\n1. Import project to Replit\n2. Add secrets in Replit Secrets tab\n3. Run with the existing workflow\n\n### Option 2: Local Server\n```bash\n# Install production server\npip install gunicorn\n\n# Run with gunicorn\ngunicorn --bind 0.0.0.0:5000 --workers 4 main:app\n```\n\n### Option 3: Docker (Advanced)\n```bash\n# Create Dockerfile if needed\ndocker build -t blindmate .\ndocker run -p 5000:5000 blindmate\n```\n\n---\n\n## üìû Support\n\n### Getting Help\n1. **Check this guide first** - most issues are covered here\n2. **Browser Console** - Press F12 and check for error messages\n3. **Test with different browsers** - Chrome works best\n4. **Verify API keys** - Make sure they're valid and not expired\n\n### Performance Tips\n- **Use Chrome** for best performance\n- **Good lighting** improves object detection\n- **Stable internet** required for AI features\n- **Close other camera apps** to avoid conflicts\n\n---\n\n## ‚úÖ Success Checklist\n\nAfter setup, you should be able to:\n- [ ] Access http://localhost:5000 without errors\n- [ ] See camera feed in the main interface\n- [ ] Grant camera and microphone permissions\n- [ ] Hear \"Navigation system ready\" voice message\n- [ ] Say \"Start detection\" and see object detection\n- [ ] Say \"Take me to [location]\" and get navigation\n- [ ] Switch between different pages (tutorial, navigation)\n\n---\n\n**üéâ Congratulations! BlindMate is now running locally on your machine.**\n\nFor more information, check the documentation in the `docs/` folder.","size_bytes":7804},"QUICK_START.md":{"content":"# üöÄ BlindMate - Quick Start Guide\n\n## One-Command Setup\n\n```bash\npython setup-local.py\n```\n\nThat's it! The script handles everything automatically.\n\n## Manual Setup (3 steps)\n\n```bash\n# 1. Create virtual environment\npython -m venv venv\n\n# 2. Activate and install\nsource venv/bin/activate  # Linux/Mac\n# OR\nvenv\\Scripts\\activate     # Windows\n\npip install -r requirements-local.txt\n\n# 3. Set up environment\ncp .env.template .env\n# Edit .env with your API keys\n```\n\n## Get API Keys\n\n1. **Gemini AI** (required): https://aistudio.google.com\n2. **Google Maps** (optional): https://console.cloud.google.com\n\n## Run the App\n\n```bash\npython main.py\n```\n\nOpen: http://localhost:5000\n\n## Need Help?\n\nSee `LOCAL_SETUP_GUIDE.md` for detailed instructions.","size_bytes":747},"setup-local.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nBlindMate Local Setup Script\nThis script helps you set up the BlindMate application for local development.\n\"\"\"\n\nimport os\nimport sys\nimport subprocess\nimport platform\n\ndef run_command(command, description):\n    \"\"\"Run a command and handle errors\"\"\"\n    print(f\"Running: {description}\")\n    try:\n        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n        if result.returncode == 0:\n            print(f\"‚úì {description} completed successfully\")\n            return True\n        else:\n            print(f\"‚úó {description} failed: {result.stderr}\")\n            return False\n    except Exception as e:\n        print(f\"‚úó Error running {description}: {e}\")\n        return False\n\ndef check_python_version():\n    \"\"\"Check if Python version is compatible\"\"\"\n    version = sys.version_info\n    if version.major == 3 and version.minor >= 11:\n        print(f\"‚úì Python {version.major}.{version.minor}.{version.micro} is compatible\")\n        return True\n    else:\n        print(f\"‚úó Python {version.major}.{version.minor}.{version.micro} is not compatible. Please use Python 3.11 or higher.\")\n        return False\n\ndef setup_virtual_environment():\n    \"\"\"Create and activate virtual environment\"\"\"\n    print(\"\\n=== Setting up Virtual Environment ===\")\n    \n    if not run_command(\"python -m venv venv\", \"Creating virtual environment\"):\n        return False\n    \n    # Activation command differs by platform\n    if platform.system() == \"Windows\":\n        activate_cmd = \"venv\\\\Scripts\\\\activate\"\n        pip_cmd = \"venv\\\\Scripts\\\\pip\"\n    else:\n        activate_cmd = \"source venv/bin/activate\"\n        pip_cmd = \"venv/bin/pip\"\n    \n    print(f\"Virtual environment created. To activate it, run: {activate_cmd}\")\n    \n    # Install dependencies\n    if not run_command(f\"{pip_cmd} install -r requirements-local.txt\", \"Installing dependencies\"):\n        return False\n    \n    return True\n\ndef create_env_template():\n    \"\"\"Create a template .env file\"\"\"\n    print(\"\\n=== Creating Environment Template ===\")\n    \n    env_template = \"\"\"# BlindMate Environment Variables\n# Copy this file to .env and fill in your actual values\n\n# Database URL (use SQLite for local development or your PostgreSQL URL)\nDATABASE_URL=sqlite:///blindmate.db\n\n# Google Gemini API Key (get from Google AI Studio)\nGEMINI_API_KEY=your_gemini_api_key_here\n\n# Session secret for Flask (generate a random string)\nSESSION_SECRET=your_secret_session_key_here\n\n# Optional: Google Maps API Key for enhanced navigation\nGOOGLE_MAPS_API_KEY=your_google_maps_api_key_here\n\"\"\"\n    \n    try:\n        with open('.env.template', 'w') as f:\n            f.write(env_template)\n        print(\"‚úì Created .env.template file\")\n        print(\"  Please copy it to .env and fill in your actual API keys\")\n        return True\n    except Exception as e:\n        print(f\"‚úó Failed to create .env.template: {e}\")\n        return False\n\ndef main():\n    \"\"\"Main setup function\"\"\"\n    print(\"BlindMate Local Development Setup\")\n    print(\"=\" * 40)\n    \n    # Check Python version\n    if not check_python_version():\n        sys.exit(1)\n    \n    # Setup virtual environment\n    if not setup_virtual_environment():\n        print(\"\\n‚úó Setup failed during virtual environment creation\")\n        sys.exit(1)\n    \n    # Create environment template\n    create_env_template()\n    \n    print(\"\\n\" + \"=\" * 40)\n    print(\"‚úì Setup completed successfully!\")\n    print(\"\\nNext steps:\")\n    print(\"1. Activate the virtual environment:\")\n    if platform.system() == \"Windows\":\n        print(\"   venv\\\\Scripts\\\\activate\")\n    else:\n        print(\"   source venv/bin/activate\")\n    print(\"2. Copy .env.template to .env and fill in your API keys\")\n    print(\"3. Run the application: python main.py\")\n    print(\"\\nFor more information, see the README.md file\")\n\nif __name__ == \"__main__\":\n    main()","size_bytes":3872},"docs/README.md":{"content":"# BlindMate - AI Assistant for Visually Impaired Users\n\nBlindMate is a comprehensive web application designed specifically for visually impaired users. It provides real-time object detection, voice-activated navigation, and multilingual AI assistance to help users navigate their environment safely and independently.\n\n## üåü Features\n\n### üîç Real-time Object Detection\n- **TensorFlow.js COCO-SSD Model**: Detects objects like people, chairs, vehicles, stairs, and more\n- **Live Webcam Feed**: Continuous monitoring of the user's environment\n- **Visual Feedback**: Bounding boxes and labels overlaid on detected objects\n- **Audio Alerts**: Text-to-speech announcements with distance estimation\n- **Smart Throttling**: Prevents audio overload with intelligent announcement timing\n\n### üó£Ô∏è Voice-Activated AI Assistant\n- **Gemini Pro Integration**: Advanced natural language processing for command interpretation\n- **Continuous Listening**: Web Speech API for hands-free operation\n- **Smart Commands**: \n  - \"Start detection\" / \"Stop detection\"\n  - \"Take me to [location]\"\n  - \"Enable location\"\n  - \"Change language to [language]\"\n- **Structured Responses**: JSON-based command processing for reliable action execution\n\n### üåç Multilingual Support\n- **7 Indian Languages**: English, Hindi, Tamil, Telugu, Bengali, Marathi, Gujarati\n- **Voice Recognition**: Speech-to-text in user's preferred language\n- **Localized Responses**: AI responses and system messages in selected language\n- **Easy Language Switching**: Voice commands or manual selection\n\n### üß≠ Smart Navigation\n- **Google Maps Integration**: Turn-by-turn directions using Google Maps API\n- **Voice-Guided Directions**: Spoken navigation instructions\n- **Walking-Optimized Routes**: Pedestrian-friendly path calculation\n- **Location-Based Services**: Automatic current location detection\n\n### ‚ôø Accessibility-First Design\n- **Large Fonts & High Contrast**: Optimized for users with partial vision\n- **Keyboard Navigation**: Full keyboard accessibility with focus indicators\n- **Screen Reader Compatible**: Semantic HTML and ARIA labels\n- **Mobile-First Responsive**: Works seamlessly on smartphones and tablets\n- **Voice-First Interface**: Minimal visual interaction required\n\n## üõ†Ô∏è Technology Stack\n\n### Frontend\n- **HTML5**: Semantic markup for accessibility\n- **CSS3**: Responsive design with Bootstrap framework\n- **Vanilla JavaScript**: ES6+ with modern web APIs\n- **TensorFlow.js**: Machine learning in the browser\n- **Web Speech API**: Voice recognition and synthesis\n- **Google Maps API**: Navigation and directions\n\n### Backend\n- **Flask**: Lightweight Python web framework\n- **Google Generative AI**: Gemini Pro for command processing\n- **Flask-CORS**: Cross-origin resource sharing\n- **Python 3.8+**: Modern Python features\n\n## üöÄ Quick Start\n\n### Prerequisites\n- Python 3.11 or higher\n- Modern web browser with camera and microphone access\n- Internet connection for AI services\n\n### Option 1: Replit Deployment (Recommended)\n\n1. **Get the required API key**:\n   - Visit [Google AI Studio](https://aistudio.google.com)\n   - Sign in with your Google account\n   - Create a new API key (free)\n   - Keep this key safe - you'll need it in step 3\n\n2. **Set up the project in Replit**:\n   - The project is already configured and ready to run\n   - All dependencies are pre-installed\n\n3. **Add your API key**:\n   - Go to the Secrets tab in your Replit project\n   - Add a new secret with key: `GEMINI_API_KEY`\n   - Paste your API key as the value\n\n4. **Run the application**:\n   ```bash\n   python main.py\n   ```\n\n### Option 2: Local Development\n\n1. **Quick Setup (Automated)**:\n   ```bash\n   python setup-local.py\n   ```\n   This script will:\n   - Check Python version compatibility\n   - Create a virtual environment\n   - Install all dependencies from `requirements-local.txt`\n   - Create environment template file\n\n2. **Manual Setup**:\n   ```bash\n   # Clone or download the project\n   git clone <repository-url>\n   cd blindmate\n\n   # Create virtual environment\n   python -m venv venv\n\n   # Activate virtual environment\n   # Windows:\n   venv\\Scripts\\activate\n   # macOS/Linux:\n   source venv/bin/activate\n\n   # Install dependencies\n   pip install -r requirements-local.txt\n   ```\n\n3. **Environment Configuration**:\n   - Copy `.env.template` to `.env`\n   - Add your API keys:\n     ```\n     DATABASE_URL=sqlite:///blindmate.db\n     GEMINI_API_KEY=your_gemini_api_key_here\n     SESSION_SECRET=your_secret_session_key_here\n     GOOGLE_MAPS_API_KEY=your_google_maps_api_key_here\n     ```\n\n4. **Run the application**:\n   ```bash\n   python main.py\n   ```\n   \n5. **Access the application**:\n   - Open your browser and go to `http://localhost:5000`\n   - Allow camera and microphone permissions when prompted\n   - The app will automatically initialize and start the voice interaction\n\n## üéØ Usage Guide\n\n### First Time Setup\n1. When you first open BlindMate, it will ask: \"Should I start detection, Sir?\"\n2. Say \"Yes\" to enable object detection\n3. Grant camera permission when prompted\n4. The app will then ask: \"Would you like to enable location?\"\n5. Say \"Yes\" to enable navigation features\n6. Grant location permission when prompted\n\n### Voice Commands\nBlindMate understands natural language commands in multiple languages:\n\n**Object Detection:**\n- \"Start detection\" / \"Begin scanning\"\n- \"Stop detection\" / \"Pause scanning\"\n\n**Navigation:**\n- \"Take me to the library\"\n- \"Navigate to Central Park\"\n- \"Go to the nearest coffee shop\"\n\n**Language Control:**\n- \"Change language to Hindi\"\n- \"Switch to Tamil\"\n\n**System Control:**\n- \"Enable location\"\n- \"Stop\" (stops current activity)\n\n### Keyboard Shortcuts\nFor accessibility, these keyboard shortcuts are available:\n- `Ctrl + S`: Start/Stop object detection\n- `Ctrl + V`: Activate voice command\n- `Ctrl + L`: Enable location services\n\n### Supported Languages\n- **English (India)**: en-IN\n- **Hindi**: hi-IN (‡§π‡§ø‡§Ç‡§¶‡•Ä)\n- **Tamil**: ta-IN (‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç)\n- **Telugu**: te-IN (‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å)\n- **Bengali**: bn-IN (‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ)\n- **Marathi**: mr-IN (‡§Æ‡§∞‡§æ‡§†‡•Ä)\n- **Gujarati**: gu-IN (‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä)\n\n## üîß Features in Detail\n\n### Object Detection\n- Uses COCO-SSD model to detect 80+ object classes\n- Real-time processing at 30 FPS\n- Distance estimation based on object size\n- Audio announcements every 3 seconds to prevent spam\n- Visual overlay with bounding boxes and confidence scores\n\n### AI Voice Assistant\n- Powered by Google's Gemini Pro model\n- Processes natural language in 7 Indian languages\n- Fallback logic when AI service is unavailable\n- Structured JSON responses for reliable action execution\n\n### Navigation System\n- Opens directions in user's default maps application\n- Supports Google Maps, Apple Maps, and generic map services\n- Provides walking directions optimized for pedestrians\n- Voice guidance integration with object detection\n\n### Accessibility Features\n- WCAG 2.1 AA compliant design\n- Screen reader compatible with ARIA labels\n- High contrast colors and large fonts\n- Keyboard navigation support\n- Voice-first interaction model\n\n## üö® Browser Compatibility\n\n**Fully Supported:**\n- Chrome 80+ (recommended)\n- Edge 80+\n- Firefox 75+\n- Safari 13+\n\n**Required Permissions:**\n- Camera access for object detection\n- Microphone access for voice commands\n- Location access for navigation (optional)\n\n## üõü Troubleshooting\n\n### Common Issues\n\n**\"Camera not working\":**\n- Ensure camera permissions are granted\n- Check if another application is using the camera\n- Try refreshing the page\n\n**\"Voice commands not responding\":**\n- Check microphone permissions\n- Ensure you're speaking clearly\n- Try switching to English if using another language\n\n**\"AI responses seem incorrect\":**\n- Verify GEMINI_API_KEY is set correctly\n- Check internet connection\n- The app has fallback logic for basic commands\n\n**\"Navigation not working\":**\n- Enable location permissions\n- Check if popup blocker is preventing navigation window\n- Try manually opening Google Maps with the destination\n\n### Performance Tips\n- Use Chrome for best performance\n- Ensure good lighting for object detection\n- Speak clearly and at normal pace for voice commands\n- Close other camera-using applications\n\n## ü§ù Contributing\n\nBlindMate is designed to be accessible and helpful. If you have suggestions for improvements:\n\n1. Focus on accessibility enhancements\n2. Consider multilingual user needs\n3. Test with actual assistive technology users\n4. Ensure changes don't break voice-first interaction\n\n## üìÑ License\n\nThis project is designed for educational and assistive technology purposes. Please use responsibly and consider the privacy and safety of visually impaired users.\n\n## üÜò Support\n\nFor technical support or feature requests, please consider:\n- Testing in different browsers\n- Checking browser console for error messages\n- Verifying all permissions are granted\n- Ensuring stable internet connection for AI features\n","size_bytes":8958},"docs/replit.md":{"content":"# BlindMate - AI Assistant for Visually Impaired Users\n\n## Project Overview\nBlindMate is an advanced web-based assistive technology application designed to empower visually impaired users with intelligent navigation and interaction tools. The system combines real-time object detection, voice commands, GPS navigation, and accessibility features.\n\n## Key Features\n- **Real-time Object Detection**: Using TensorFlow.js and COCO-SSD model\n- **Voice-Controlled Navigation**: Google Maps-like walking navigation with voice confirmation\n- **Obstacle Detection During Navigation**: Camera-based obstacle alerts while navigating\n- **Multi-language Support**: Hindi, Tamil, Telugu, Bengali, Marathi, Gujarati, and English\n- **Accessibility-First Design**: High contrast, large fonts, keyboard navigation\n- **Mobile-Friendly**: Responsive design for smartphones and tablets\n\n## Technology Stack\n- **Backend**: Flask (Python) with SQLAlchemy\n- **Frontend**: Vanilla JavaScript, Bootstrap 5, TensorFlow.js\n- **APIs**: Google Directions API and Google Geocoding API, Google Generative AI (Gemini)\n- **AI Models**: COCO-SSD for object detection, Gemini for voice command processing\n- **Database**: PostgreSQL\n\n## Architecture\n\n### Backend Components\n- `app.py`: Main Flask application with API endpoints\n- `gemini_service.py`: AI service for voice command processing\n- `models.py`: Database models (User management)\n- `main.py`: Application entry point\n\n### Frontend Components\n- `index.html`: Main application interface\n- `app.js`: Core BlindMate functionality (camera, detection, voice)\n- `navigation.js`: Advanced navigation system with GPS tracking\n- `styles.css`: Accessibility-focused styling\n\n### API Endpoints\n- `/api/directions`: Google Directions API and Geocoding API integration for voice navigation\n- `/api/google-maps-key`: Secure API key delivery for frontend\n- `/api/process-command`: Voice command processing via Gemini AI\n- Static file serving for HTML, CSS, JS\n\n## Navigation System Features\n1. **Voice Confirmation**: \"Should I start navigation to [destination]?\" with Yes/No detection\n2. **Continuous GPS Tracking**: Uses `navigator.geolocation.watchPosition` for real-time updates\n3. **Automatic Step Progression**: Advances when within 25 meters of step endpoint\n4. **Smart Rerouting**: Triggers when user deviates >50 meters from planned route\n5. **Enhanced Error Handling**: Clear voice messages for location/route failures\n6. **Optimized Voice Instructions**: Short, clear commands designed for blind users\n7. **Emergency Stop Feature**: Safety button to immediately stop navigation\n8. **Obstacle Detection Integration**: Real-time alerts during navigation\n9. **Universal Destination Support**: Works with any Google Maps location worldwide\n10. **Permission Management**: Requests camera, microphone, and location access on load\n\n## Local Development Setup\n- **Requirements File**: `requirements-local.txt` created with all necessary dependencies\n- **Setup Script**: `setup-local.py` provides automated local environment setup\n- **Dependencies**:\n  - `flask>=3.1.1` - Web framework\n  - `flask-cors>=6.0.1` - Cross-origin resource sharing\n  - `flask-sqlalchemy>=3.1.1` - Database ORM\n  - `gunicorn>=23.0.0` - Production web server\n  - `psycopg2-binary>=2.9.10` - PostgreSQL adapter\n  - `requests>=2.31.0` - HTTP library for API calls\n  - `email-validator>=2.2.0` - Email validation\n  - `google-genai>=1.27.0` - Google Gemini AI integration\n  - `sift-stack-py>=0.7.0` - Enhanced functionality\n  - `typing-extensions>=4.8.0` - Type hints support\n\n## Recent Changes\n- **2025-08-10**: Updated Tutorial System to Match Current App Capabilities\n  - **UPDATED TUTORIAL FEATURES**: Tutorial now accurately reflects the current BlindMate app features\n  - **15 LANGUAGE SUPPORT**: Tutorial showcases global language support (not just 7 Indian languages)\n  - **5 VOICE TONE OPTIONS**: Updated tutorial to highlight friendly, formal, energetic, calm, and robotic tones\n  - **WAKE WORD INTEGRATION**: Tutorial now teaches \"Hey BlindMate\" voice activation\n  - **UNIVERSAL NAVIGATION**: Removed outdated location restrictions, now showcases worldwide navigation\n  - **ENHANCED OBJECT DETECTION**: Tutorial explains 1.5-second speech delays and anti-overlap technology\n  - **MOBILE GESTURE SUPPORT**: Added double-tap gesture information for mobile devices\n  - **BATTERY OPTIMIZATION**: Tutorial covers smart GPS frequency adjustment for battery saving\n  - **PRACTICE EXERCISES UPDATED**: 4 new exercises covering wake word, global navigation, voice customization, and language switching\n  - **DEMO CONTENT REFRESHED**: Updated audio demos to reflect current app capabilities\n  - **ACCURATE FEATURE LIST**: Tutorial introduction now lists actual implemented features instead of outdated ones\n\n- **2025-08-10**: Enhanced Speech System for Object Detection with Anti-Overlap & Delay Controls\n  - **SMOOTH SPEECH ANNOUNCEMENTS**: Added 1.5-second minimum delay between consecutive object announcements to prevent overlapping voices\n  - **IMPROVED SPEECH CANCELLATION**: Enhanced `synth.cancel()` with small timeout delays to ensure proper cancellation before new speech\n  - **INTELLIGENT DELAY MANAGEMENT**: Object announcements queue automatically when speech is active, with smart delay calculation\n  - **NON-OVERLAPPING VOICE OUTPUT**: All speech synthesis now cancels ongoing utterances before starting new ones for crystal-clear audio\n  - **ENHANCED SPEECH EVENT HANDLING**: Added proper `onstart`, `onend`, and `onerror` event handlers with detailed logging for better debugging\n  - **PRIORITY-BASED SPEECH QUEUE**: Object announcements use special delay logic while navigation commands remain high-priority\n  - **AUTOMATIC CLEANUP**: Speech delay timers are properly cleaned up when detection stops to prevent memory leaks\n  - **IMPROVED USER EXPERIENCE**: Users can now understand each announcement clearly without voice overlap confusion\n  - **DEBUG LOGGING**: Added console logs to track speech delay behavior and timing for better monitoring\n  - **FALLBACK HANDLING**: Enhanced error recovery with longer delays between retries for smoother operation\n\n- **2025-08-10**: Enhanced GPS Navigation System with Battery Optimization & Human-Friendly Voice Instructions\n  - **IMPROVED GPS ACCURACY**: Updated `navigator.geolocation.watchPosition` with `enableHighAccuracy: true`, `maximumAge: 0`, `timeout: 10000`\n  - **ADDED ROBUST ERROR HANDLING**: Clear voice + UI error messages for GPS failures with specific error codes (permission denied, signal unavailable, timeout)\n  - **CONVERTED RAW DATA TO HUMAN-FRIENDLY SPEECH**: Navigation instructions like \"Turn left in 20 meters\" instead of raw coordinates\n  - **OPTIMIZED BATTERY USAGE**: Smart tracking frequency - high frequency (2s) when moving, low frequency (8s) when stationary\n  - **AUTOMATIC DESTINATION DETECTION**: Stops tracking when within 10 meters of destination\n  - **SMOOTH SPEECH SYNTHESIS**: Always cancels old utterances before new ones to prevent overlapping voices\n  - **ENHANCED VOICE INSTRUCTIONS**: Conversational navigation like \"bear left\", \"keep going straight\", \"make a sharp right turn\"\n  - **COMPREHENSIVE ERROR FALLBACKS**: Shows text in UI when speech synthesis fails, includes retry mechanisms\n  - **MOBILE DEVICE OPTIMIZATION**: Detects mobile devices and applies battery-saving settings automatically\n  - **IMPROVED USER INTERFACE**: Error display overlays, speech fallback text, enhanced navigation status with distance info\n  - **PROXIMITY-BASED ANNOUNCEMENTS**: Urgent warnings at 10m, advance warnings at 50m, with smart warning flag management\n  - **SPEED CALCULATION**: Tracks user movement speed to optimize GPS polling frequency and battery life\n\n- **2025-08-10**: Complete Universal Navigation Implementation\n  - **REMOVED all OpenRouteService (ORS) code** from backend and frontend\n  - **REPLACED with Google Directions API and Google Geocoding API only**\n  - **COMPLETELY ELIMINATED ALL HARDCODED LOCATION RESTRICTIONS** from frontend and backend\n  - **REMOVED predefined locations object** and all location validation code from app.js\n  - **ENHANCED Google Geocoding API** for any destination names (including short names like \"hospital\")\n  - **UNIVERSAL DESTINATION SUPPORT**: Users can navigate to ANY place worldwide by name\n    - Examples: \"Take me to Eiffel Tower\", \"Navigate to Central Park\", \"Go to Starbucks nearby\"\n    - No more hardcoded location lists - any global destination works\n    - Voice commands work with landmarks, addresses, business names, and geographic locations\n  - **IMPLEMENTED continuous GPS tracking** with `navigator.geolocation.watchPosition`\n  - **ADDED automatic step progression** when within 25 meters of step endpoint\n  - **CREATED smart rerouting** when user deviates >50 meters from planned route\n  - **ENHANCED error handling** with specific voice messages:\n    - \"Location not found, please try again\" for ZERO_RESULTS geocoding\n    - \"Route not available\" for failed directions requests\n    - Clear voice feedback for all navigation errors\n  - **OPTIMIZED voice instructions** for visually impaired users:\n    - Short, clear commands like \"Turn left in 20 meters\"\n    - Simplified distance announcements (20m, 100m, 1.5km)\n    - Removed verbose phrases for better accessibility\n  - **ADDED emergency stop button** for safety during navigation\n  - **IMPLEMENTED obstacle detection** during navigation using COCO-SSD\n  - Voice confirmation workflow: \"Should I start navigation to [destination]?\"\n  - Works worldwide with any Google Maps location via voice commands\n  - Single API key configuration: `GOOGLE_MAPS_API_KEY` environment variable\n\n## User Preferences\n- **Communication Style**: Simple, everyday language (non-technical)\n- **Accessibility Priority**: High contrast, large text, voice feedback\n- **Navigation Style**: Google Maps-like walking mode with obstacle alerts\n- **Code Style**: Well-commented for easy modification\n\n## Development Notes\n- Google Maps API key configured as `GOOGLE_MAPS_API_KEY` environment variable (used for Directions, Geocoding, and Maps JavaScript APIs)\n- Google Gemini API key configured as `GOOGLE_API_KEY` environment variable\n- All permissions (camera, microphone, location) requested on page load\n- Navigation UI shows current step, total steps, and progress\n- Object detection continues during navigation for obstacle alerts\n- Automatic navigation end detection when destination is reached\n- 30-second timeout for all Google API requests to prevent hanging\n- Supports both exact addresses and general place names worldwide\n\n## Architecture Features\n- **Universal Navigation**: Works with ANY Google Maps location worldwide without restrictions\n- **No Hardcoded Locations**: Complete elimination of predefined location lists and validation\n- **Dynamic Geocoding**: All destinations resolved through Google Geocoding API in real-time\n- **Voice Confirmation**: \"Should I start navigation to [destination]?\" workflow\n- **Live GPS Tracking**: Uses `navigator.geolocation.watchPosition` for continuous updates\n- **Automatic Step Progression**: Advances when within 25 meters of step endpoint\n- **Smart Rerouting**: Triggers when user deviates >50 meters from planned route\n- **Obstacle Detection**: Real-time alerts during navigation using camera and AI\n- **Error Handling**: Comprehensive fallbacks for API failures and location not found\n- **Global Destination Support**: Users can name any place, landmark, address, or business name","size_bytes":11512},"static/css/styles.css":{"content":"/* BlindMate Custom Styles */\n\n/* High contrast and accessibility improvements */\n:root {\n    --text-size-large: 1.25rem;\n    --text-size-xlarge: 1.5rem;\n    --border-radius-large: 0.75rem;\n    --shadow-strong: 0 4px 6px -1px rgba(0, 0, 0, 0.4);\n}\n\n/* Base accessibility improvements */\nbody {\n    font-size: var(--text-size-large);\n    line-height: 1.6;\n    min-height: 100vh;\n    display: flex;\n    flex-direction: column;\n}\n\n/* Enhanced focus indicators for keyboard navigation */\nbutton:focus,\nselect:focus,\ninput:focus {\n    outline: 3px solid var(--bs-warning) !important;\n    outline-offset: 2px !important;\n    box-shadow: 0 0 0 0.2rem rgba(255, 193, 7, 0.25) !important;\n}\n\n/* Large, accessible buttons */\n.btn-lg {\n    padding: 1rem 2rem;\n    font-size: var(--text-size-xlarge);\n    font-weight: 600;\n    border-radius: var(--border-radius-large);\n    min-height: 60px;\n    box-shadow: var(--shadow-strong);\n    transition: all 0.3s ease;\n}\n\n.btn-lg:hover {\n    transform: translateY(-2px);\n    box-shadow: 0 6px 8px -1px rgba(0, 0, 0, 0.5);\n}\n\n.btn-lg:active {\n    transform: translateY(0);\n}\n\n/* Video container styling */\n#webcam {\n    border-radius: var(--border-radius-large);\n    min-height: 400px;\n    background-color: var(--bs-dark);\n}\n\n#canvas {\n    border-radius: var(--border-radius-large);\n    pointer-events: none;\n}\n\n/* Loading overlay */\n#loadingOverlay {\n    background-color: rgba(0, 0, 0, 0.8);\n    border-radius: var(--border-radius-large);\n    padding: 2rem;\n    color: white;\n    z-index: 10;\n}\n\n/* Status indicators */\n.badge {\n    font-size: 1rem;\n    padding: 0.5rem 1rem;\n}\n\n/* System status alerts */\n#systemStatus {\n    font-size: var(--text-size-large);\n    border-radius: var(--border-radius-large);\n    border: 2px solid;\n}\n\n/* Language selector */\n#languageSelect {\n    font-size: var(--text-size-large);\n    padding: 0.75rem 1rem;\n    border-radius: var(--border-radius-large);\n}\n\n/* Voice status indicator */\n#voiceStatus {\n    border-radius: var(--border-radius-large);\n    border: 2px solid var(--bs-primary);\n}\n\n/* Animation for listening indicator */\n@keyframes pulse {\n    0%, 100% {\n        opacity: 1;\n    }\n    50% {\n        opacity: 0.5;\n    }\n}\n\n.animate-pulse {\n    animation: pulse 2s infinite;\n}\n\n/* Card styling improvements */\n.card {\n    border-radius: var(--border-radius-large);\n    box-shadow: var(--shadow-strong);\n    border: 2px solid var(--bs-border-color);\n}\n\n.card-header {\n    border-radius: var(--border-radius-large) var(--border-radius-large) 0 0;\n    font-weight: 600;\n    font-size: var(--text-size-xlarge);\n}\n\n/* Header styling */\nheader h1 {\n    text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5);\n}\n\n/* Enhanced Navigation Styles */\n.navigation-display {\n    position: sticky;\n    top: 20px;\n    z-index: 100;\n}\n\n.navigation-display .card {\n    box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);\n    border: 2px solid var(--bs-primary);\n}\n\n.navigation-display .card-header {\n    font-weight: 600;\n    background: linear-gradient(45deg, var(--bs-primary), var(--bs-info));\n}\n\n.navigation-display .progress {\n    background-color: rgba(255, 255, 255, 0.2);\n    height: 8px;\n}\n\n.navigation-display .progress-bar {\n    background: linear-gradient(90deg, var(--bs-success), var(--bs-info));\n    transition: width 0.3s ease;\n}\n\n/* Mobile-specific navigation styles */\n@media (max-width: 768px) {\n    .navigation-display {\n        position: fixed;\n        bottom: 0;\n        left: 0;\n        right: 0;\n        top: auto;\n        z-index: 1050;\n        margin: 10px;\n        border-radius: 15px 15px 0 0;\n    }\n    \n    .navigation-display .card {\n        margin: 0;\n        border-radius: 15px 15px 0 0;\n    }\n    \n    /* Ensure main content doesn't hide behind fixed navigation */\n    body.navigation-active {\n        padding-bottom: 220px;\n    }\n    \n    /* Larger touch targets on mobile */\n    .btn-lg {\n        min-height: 70px;\n        font-size: 1.3rem;\n    }\n}\n\n/* Emergency stop button styling */\n#emergencyStop {\n    background: linear-gradient(45deg, var(--bs-danger), #a71e2a);\n    border: 2px solid var(--bs-danger);\n    font-weight: bold;\n    box-shadow: 0 2px 8px rgba(220, 53, 69, 0.3);\n    color: white;\n}\n\n#emergencyStop:hover {\n    background: linear-gradient(45deg, #a71e2a, #7a1719);\n    box-shadow: 0 4px 12px rgba(220, 53, 69, 0.4);\n    transform: translateY(-1px);\n}\n\n/* Enhanced detection indicator */\n.detection-indicator {\n    position: absolute;\n    top: 10px;\n    right: 10px;\n    background: rgba(0, 0, 0, 0.9);\n    color: white;\n    padding: 8px 12px;\n    border-radius: 20px;\n    font-size: 14px;\n    font-weight: 600;\n    display: flex;\n    align-items: center;\n    gap: 5px;\n    animation: pulse-detection 2s infinite;\n    border: 2px solid var(--bs-success);\n}\n\n@keyframes pulse-detection {\n    0% { opacity: 1; transform: scale(1); }\n    50% { opacity: 0.8; transform: scale(1.05); }\n    100% { opacity: 1; transform: scale(1); }\n}\n\n/* Voice feedback visual indicators */\n.voice-listening {\n    background: linear-gradient(45deg, var(--bs-warning), #e0a800);\n    animation: listening-pulse 1.5s ease-in-out infinite;\n    border-color: var(--bs-warning);\n}\n\n@keyframes listening-pulse {\n    0%, 100% { transform: scale(1); box-shadow: 0 0 0 0 rgba(255, 193, 7, 0.4); }\n    50% { transform: scale(1.02); box-shadow: 0 0 0 10px rgba(255, 193, 7, 0); }\n}\n\n.voice-speaking {\n    background: linear-gradient(45deg, var(--bs-info), #138496);\n    animation: speaking-wave 0.8s ease-in-out infinite;\n    border-color: var(--bs-info);\n}\n\n@keyframes speaking-wave {\n    0%, 100% { opacity: 1; }\n    25% { opacity: 0.9; }\n    50% { opacity: 0.8; }\n    75% { opacity: 0.9; }\n}\n\n/* Navigation step indicators */\n#currentInstruction {\n    font-size: 1.1rem;\n    line-height: 1.4;\n    color: var(--bs-dark);\n}\n\n#stepDistance {\n    color: var(--bs-primary);\n    font-weight: 700;\n}\n\n/* High contrast mode for better accessibility */\n@media (prefers-contrast: high) {\n    .card {\n        border: 3px solid #000000;\n        background: #ffffff;\n        color: #000000;\n    }\n    \n    .btn {\n        border: 3px solid;\n        font-weight: bold;\n    }\n    \n    .badge {\n        border: 2px solid #000000;\n        color: #000000;\n    }\n    \n    .navigation-display .card-header {\n        background: #000000;\n        color: #ffffff;\n    }\n}\n\n/* Reduced motion for accessibility */\n@media (prefers-reduced-motion: reduce) {\n    *, *::before, *::after {\n        animation-duration: 0.01ms !important;\n        animation-iteration-count: 1 !important;\n        transition-duration: 0.01ms !important;\n    }\n    \n    .btn-lg:hover {\n        transform: none;\n    }\n}\n\n/* Responsive video container */\n@media (max-width: 992px) {\n    .card-body {\n        min-height: 300px;\n    }\n    \n    #webcam {\n        min-height: 300px;\n    }\n}\n\n@media (max-width: 576px) {\n    .btn-lg {\n        padding: 0.75rem 1.5rem;\n        font-size: var(--text-size-large);\n        min-height: 50px;\n    }\n    \n    .card-body {\n        min-height: 250px;\n    }\n    \n    #webcam {\n        min-height: 250px;\n    }\n}\n\n/* High contrast mode support */\n@media (prefers-contrast: high) {\n    .btn {\n        border-width: 3px;\n    }\n    \n    .card {\n        border-width: 3px;\n    }\n    \n    #systemStatus {\n        border-width: 3px;\n    }\n}\n\n/* Reduced motion support */\n@media (prefers-reduced-motion: reduce) {\n    .btn-lg {\n        transition: none;\n    }\n    \n    .btn-lg:hover {\n        transform: none;\n    }\n    \n    .animate-pulse {\n        animation: none;\n    }\n}\n\n/* Screen reader only content */\n.sr-only {\n    position: absolute;\n    width: 1px;\n    height: 1px;\n    padding: 0;\n    margin: -1px;\n    overflow: hidden;\n    clip: rect(0, 0, 0, 0);\n    white-space: nowrap;\n    border: 0;\n}\n\n/* Canvas overlay for object detection */\n#canvas {\n    position: absolute !important;\n    top: 0 !important;\n    left: 0 !important;\n    width: 100% !important;\n    height: 100% !important;\n    pointer-events: none !important;\n    z-index: 10 !important;\n}\n\n/* Video and canvas container */\n.card-body {\n    position: relative !important;\n    overflow: hidden !important;\n}\n\n/* Object detection visual indicators */\n.detection-indicator {\n    position: absolute;\n    top: 10px;\n    right: 10px;\n    background: rgba(0, 255, 0, 0.8);\n    color: black;\n    padding: 5px 10px;\n    border-radius: 5px;\n    font-weight: bold;\n    font-size: 12px;\n    z-index: 15;\n}\n\n.detection-indicator.active {\n    animation: pulse-green 2s infinite;\n}\n\n@keyframes pulse-green {\n    0%, 100% { background: rgba(0, 255, 0, 0.8); }\n    50% { background: rgba(0, 255, 0, 1); }\n}\n\n/* Navigation directions display */\n.directions-panel {\n    position: fixed;\n    top: 50%;\n    left: 50%;\n    transform: translate(-50%, -50%);\n    background-color: var(--bs-dark);\n    color: white;\n    padding: 2rem;\n    border-radius: var(--border-radius-large);\n    box-shadow: var(--shadow-strong);\n    z-index: 1000;\n    max-width: 90vw;\n    max-height: 80vh;\n    overflow-y: auto;\n}\n\n/* Ensure proper contrast for all text */\n.text-light {\n    color: #ffffff !important;\n}\n\n.text-dark {\n    color: #000000 !important;\n}\n\n/* Universal Navigation Styles */\n.main-nav-button {\n    min-height: 60px;\n    font-size: 1.2rem;\n    font-weight: 600;\n    transition: all 0.3s ease;\n}\n\n.main-nav-button.listening {\n    background-color: #dc3545 !important;\n    border-color: #dc3545 !important;\n    animation: pulse 1.5s infinite;\n}\n\n.main-nav-button.navigating {\n    background-color: #198754 !important;\n    border-color: #198754 !important;\n}\n\n@keyframes pulse {\n    0% { box-shadow: 0 0 0 0 rgba(220, 53, 69, 0.7); }\n    70% { box-shadow: 0 0 0 10px rgba(220, 53, 69, 0); }\n    100% { box-shadow: 0 0 0 0 rgba(220, 53, 69, 0); }\n}\n\n.navigation-container {\n    position: relative;\n    z-index: 100;\n}\n\n.navigation-instruction {\n    background-color: #f8f9fa !important;\n    border: 2px solid #007bff;\n    border-radius: 8px;\n    padding: 15px;\n    font-size: 1.4rem;\n    font-weight: 700;\n    color: #000 !important;\n    line-height: 1.4;\n}\n","size_bytes":10040},"static/js/navigation.js":{"content":"/**\n * BlindMate Enhanced Navigation System\n * High-accuracy GPS tracking with optimized battery usage and human-friendly speech instructions\n * Features: Smart tracking frequency, robust error handling, clear voice navigation\n */\nclass UniversalNavigation {\n    constructor() {\n        // Navigation states\n        this.isNavigating = false;\n        this.currentRoute = null;\n        this.currentStepIndex = 0;\n        this.watchId = null;\n        this.currentPosition = null;\n        this.awaitingConfirmation = false;\n        this.currentDestination = null;\n        this.lastLocationUpdateTime = null;\n        this.userSpeed = 0; // m/s\n        this.stationary = false;\n        this.destinationReached = false;\n        \n        // Enhanced navigation configuration\n        this.config = {\n            stepProximityThreshold: 15, // meters - when to advance to next step\n            routeDeviationThreshold: 30, // meters - when to reroute\n            destinationReachedThreshold: 10, // meters - when destination is reached\n            // Battery optimization thresholds\n            highFrequencyInterval: 2000, // ms - when user is moving\n            lowFrequencyInterval: 8000, // ms - when user is stationary\n            stationarySpeedThreshold: 0.5, // m/s - below this is considered stationary\n            // Voice instruction optimization\n            voicePreviewDistance: 50, // meters - when to announce \"in X meters\"\n            repeatInstructionDistance: 25, // meters - repeat instructions if user hasn't moved\n            urgentAnnouncementDistance: 10, // meters - for urgent turn warnings\n            // Obstacle alert configuration\n            obstacleDetectionFrequency: 1000, // ms - how often to check for obstacles\n            obstacleAlertDistance: 'close', // close, medium, far\n            obstacleMinConfidence: 0.5 // minimum confidence to trigger alert\n        };\n        \n        // Google Maps integration\n        this.map = null;\n        this.directionsService = null;\n        this.directionsRenderer = null;\n        this.userMarker = null;\n        this.googleMapsApiKey = null;\n        \n        // Enhanced speech recognition and synthesis\n        this.recognition = null;\n        this.confirmationRecognition = null;\n        this.speechSynthesis = window.speechSynthesis;\n        this.isSpeaking = false;\n        this.isActivelyListening = false;\n        this.speechQueue = [];\n        this.lastUtterance = null;\n        this.speechCancellationTimer = null;\n        \n        // COCO-SSD model for obstacle detection\n        this.model = null;\n        this.isDetecting = false;\n        this.detectionCanvas = null;\n        this.detectionContext = null;\n        this.camera = null;\n        \n        // Real-time Obstacle Alert System\n        this.obstacleAlertEnabled = true;\n        this.lastObstacleAlert = 0;\n        this.obstacleAlertCooldown = 3000; // 3 seconds between alerts\n        this.obstacleDetectionInterval = null;\n        this.criticalObstacles = ['person', 'car', 'truck', 'bus', 'bicycle', 'motorcycle'];\n        this.warningObstacles = ['chair', 'dining table', 'potted plant', 'bench', 'fire hydrant'];\n        this.detectedObstacles = new Map(); // Track obstacle persistence\n        this.obstacleThresholds = {\n            critical: 0.6, // High confidence threshold for critical obstacles\n            warning: 0.5,  // Medium confidence for warning obstacles\n            minSize: 0.1   // Minimum size (% of screen) to trigger alert\n        };\n        \n        // Enhanced permissions and error handling\n        this.permissions = {\n            camera: false,\n            microphone: false,\n            location: false\n        };\n        this.errorStates = {\n            gpsLost: false,\n            speechFailed: false,\n            routingFailed: false\n        };\n        \n        // Mobile device detection\n        this.isMobile = this.detectMobileDevice();\n        \n        this.initialize();\n    }\n    \n    /**\n     * Detect if running on mobile device for battery optimization\n     */\n    detectMobileDevice() {\n        const userAgent = navigator.userAgent;\n        const isMobile = /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(userAgent) || \n                         'ontouchstart' in window || \n                         navigator.maxTouchPoints > 0;\n        \n        console.log('Mobile device detected:', isMobile, {\n            userAgent: userAgent,\n            ontouchstart: 'ontouchstart' in window,\n            maxTouchPoints: navigator.maxTouchPoints\n        });\n        \n        return isMobile;\n    }\n    \n    /**\n     * Initialize the navigation system\n     */\n    async initialize() {\n        console.log('Initializing BlindMate Navigation System...');\n        \n        this.setupSpeechRecognition();\n        this.setupUIEventListeners();\n        \n        // Request all permissions on page load\n        await this.requestAllPermissions();\n        \n        // Initialize camera for obstacle detection\n        await this.initializeCamera();\n        \n        // Load object detection model\n        await this.loadModel();\n        \n        // Get Google Maps API key\n        await this.getGoogleMapsApiKey();\n        \n        console.log('BlindMate Navigation System initialized');\n        \n        // Setup mobile-specific optimizations\n        if (this.isMobile) {\n            console.log('Checking mobile device for double-tap setup:', this.isMobile);\n            this.setupMobileOptimizations();\n        } else {\n            console.log('Desktop device - double-tap not enabled');\n        }\n    }\n    \n    /**\n     * Setup mobile-specific optimizations for battery life\n     */\n    setupMobileOptimizations() {\n        // Enable high accuracy mode for mobile devices\n        this.config.highAccuracyMode = true;\n        \n        // Reduce detection frequency on mobile to save battery\n        this.config.obstacleDetectionInterval = 3000;\n        \n        // Enable more aggressive stationary detection\n        this.config.stationaryTimeout = 30000; // 30 seconds\n        \n        console.log('Mobile optimizations enabled for battery efficiency');\n    }\n    \n    /**\n     * Get Google Maps API key from backend\n     */\n    async getGoogleMapsApiKey() {\n        try {\n            const response = await fetch('/api/google-maps-key');\n            if (response.ok) {\n                const data = await response.json();\n                this.googleMapsApiKey = data.key;\n                console.log('Google Maps API key retrieved');\n                \n                // Initialize Google Maps if key is available\n                if (window.google && window.google.maps) {\n                    this.initializeGoogleMaps();\n                }\n            } else {\n                console.error('Failed to get Google Maps API key');\n            }\n        } catch (error) {\n            console.error('Error getting Google Maps API key:', error);\n        }\n    }\n    \n    /**\n     * Initialize Google Maps\n     */\n    initializeGoogleMaps() {\n        if (!this.googleMapsApiKey) {\n            console.error('Google Maps API key not available');\n            return;\n        }\n        \n        console.log('Google Maps JavaScript API will be used for navigation');\n        \n        // Initialize map\n        const defaultCenter = this.currentPosition ? \n            { lat: this.currentPosition.latitude, lng: this.currentPosition.longitude } :\n            { lat: 28.6139, lng: 77.2090 }; // Default to Delhi\n        \n        this.map = new google.maps.Map(document.getElementById('map'), {\n            zoom: 16,\n            center: defaultCenter,\n            mapTypeId: google.maps.MapTypeId.ROADMAP\n        });\n        \n        this.directionsService = new google.maps.DirectionsService();\n        this.directionsRenderer = new google.maps.DirectionsRenderer({\n            map: this.map,\n            suppressMarkers: false\n        });\n        \n        console.log('Google Maps initialized');\n    }\n    \n    /**\n     * Initialize map (called by Google Maps API callback)\n     */\n    initializeMap() {\n        console.log('Google Maps callback triggered');\n        this.initializeGoogleMaps();\n    }\n    \n    /**\n     * Request all permissions on page load\n     */\n    async requestAllPermissions() {\n        console.log('Requesting all permissions...');\n        \n        try {\n            // Request microphone permission\n            console.log('Requesting microphone permission...');\n            const audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });\n            this.permissions.microphone = true;\n            audioStream.getTracks().forEach(track => track.stop());\n            console.log('Microphone permission granted');\n            \n            // Request camera permission\n            console.log('Requesting camera permission...');\n            const videoStream = await navigator.mediaDevices.getUserMedia({ video: true });\n            this.permissions.camera = true;\n            videoStream.getTracks().forEach(track => track.stop());\n            console.log('Camera permission granted');\n            \n            // Request location permission\n            console.log('Requesting location permission...');\n            this.currentPosition = await this.getCurrentPosition();\n            this.permissions.location = true;\n            console.log('Location permission granted');\n            \n            this.speak('All permissions granted. Navigation system ready.');\n            \n        } catch (error) {\n            console.error('Permission request failed:', error);\n            this.speak('Some permissions were denied. Please enable all permissions for full functionality.');\n        }\n    }\n    \n    /**\n     * Get current position with enhanced accuracy and error handling\n     */\n    getCurrentPosition() {\n        return new Promise((resolve, reject) => {\n            if (!navigator.geolocation) {\n                const error = new Error('Geolocation not supported on this device');\n                this.handleLocationError(error);\n                reject(error);\n                return;\n            }\n            \n            navigator.geolocation.getCurrentPosition(\n                position => {\n                    this.errorStates.gpsLost = false;\n                    console.log('GPS position acquired successfully');\n                    resolve(position.coords);\n                },\n                error => {\n                    this.handleLocationError(error);\n                    reject(error);\n                },\n                {\n                    enableHighAccuracy: true,\n                    timeout: 10000,\n                    maximumAge: 0 // Always get fresh location\n                }\n            );\n        });\n    }\n    \n    /**\n     * Handle location errors with user-friendly messages and UI updates\n     */\n    handleLocationError(error) {\n        this.errorStates.gpsLost = true;\n        let errorMessage = '';\n        let uiMessage = '';\n        \n        switch(error.code) {\n            case error.PERMISSION_DENIED:\n                errorMessage = 'Location access denied. Please enable location permissions in your browser settings.';\n                uiMessage = 'Location Permission Denied';\n                break;\n            case error.POSITION_UNAVAILABLE:\n                errorMessage = 'Location information unavailable. Please check your GPS connection.';\n                uiMessage = 'GPS Signal Unavailable';\n                break;\n            case error.TIMEOUT:\n                errorMessage = 'Location request timed out. Trying again...';\n                uiMessage = 'GPS Signal Weak';\n                break;\n            default:\n                errorMessage = 'Unknown location error occurred. Please try again.';\n                uiMessage = 'Location Error';\n                break;\n        }\n        \n        console.error('Location error:', error.message, errorMessage);\n        this.speakErrorMessage(errorMessage);\n        this.updateStatusDisplay(uiMessage, errorMessage);\n        this.showErrorInUI(uiMessage, errorMessage);\n    }\n    \n    /**\n     * Show error message in UI with clear visual indication\n     */\n    showErrorInUI(title, message) {\n        const errorElement = document.getElementById('errorDisplay');\n        if (errorElement) {\n            errorElement.innerHTML = `\n                <div class=\"error-message\">\n                    <i class=\"fas fa-exclamation-triangle\"></i>\n                    <strong>${title}</strong>\n                    <p>${message}</p>\n                </div>\n            `;\n            errorElement.style.display = 'block';\n            \n            // Auto-hide after 10 seconds\n            setTimeout(() => {\n                errorElement.style.display = 'none';\n            }, 10000);\n        }\n    }\n    \n    /**\n     * Initialize camera for obstacle detection\n     */\n    async initializeCamera() {\n        try {\n            const video = document.getElementById('webcam');\n            if (!video) {\n                console.warn('Webcam element not found');\n                return;\n            }\n            \n            this.camera = video;\n            const stream = await navigator.mediaDevices.getUserMedia({\n                video: { width: 640, height: 480 }\n            });\n            \n            video.srcObject = stream;\n            await new Promise(resolve => {\n                video.onloadedmetadata = () => {\n                    video.play();\n                    resolve();\n                };\n            });\n            \n            console.log('Camera initialized successfully');\n            \n            // Setup detection canvas\n            this.detectionCanvas = document.getElementById('canvas') || document.createElement('canvas');\n            this.detectionContext = this.detectionCanvas.getContext('2d');\n            \n        } catch (error) {\n            console.error('Camera initialization failed:', error);\n        }\n    }\n    \n    /**\n     * Load COCO-SSD model for object detection\n     */\n    async loadModel() {\n        try {\n            if (typeof cocoSsd === 'undefined') {\n                console.warn('COCO-SSD not loaded, obstacle detection disabled');\n                return;\n            }\n            \n            console.log('Loading COCO-SSD model for navigation...');\n            this.model = await cocoSsd.load();\n            console.log('COCO-SSD model loaded successfully');\n            console.log('COCO-SSD model loaded for navigation');\n            \n        } catch (error) {\n            console.error('Failed to load COCO-SSD model:', error);\n        }\n    }\n    \n    /**\n     * Setup speech recognition for navigation commands\n     */\n    setupSpeechRecognition() {\n        if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {\n            console.error('Speech recognition not supported');\n            return;\n        }\n        \n        console.log('Initializing speech recognition...');\n        \n        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n        \n        // Main navigation recognition\n        this.recognition = new SpeechRecognition();\n        this.recognition.continuous = false;\n        this.recognition.interimResults = false;\n        this.recognition.lang = 'en-US';\n        \n        console.log('Speech recognition object created successfully');\n        \n        this.recognition.onstart = () => {\n            console.log('Navigation speech recognition started');\n            this.isActivelyListening = true;\n            this.updateStatusDisplay('Listening...', 'Speak your destination');\n        };\n        \n        this.recognition.onresult = (event) => {\n            if (event.results && event.results[0]) {\n                const command = event.results[0][0].transcript.toLowerCase().trim();\n                console.log('Navigation command received:', command);\n                this.processNavigationCommand(command);\n            }\n        };\n        \n        this.recognition.onerror = (event) => {\n            console.error('Navigation speech recognition error:', event.error);\n            const wasActivelyListening = this.isActivelyListening;\n            this.isActivelyListening = false;\n            \n            // Only speak error messages for meaningful errors, not technical ones\n            if (event.error === 'not-allowed') {\n                this.speak('Microphone access is required for navigation. Please allow microphone access.');\n            } else if (event.error === 'no-speech' && wasActivelyListening) {\n                this.speak('No speech detected. Please try again.');\n            }\n            // Don't speak for 'aborted' or other technical errors that happen during normal operation\n        };\n        \n        this.recognition.onend = () => {\n            console.log('Navigation speech recognition ended');\n            this.isActivelyListening = false;\n        };\n        \n        // Confirmation recognition\n        this.confirmationRecognition = new SpeechRecognition();\n        this.confirmationRecognition.continuous = false;\n        this.confirmationRecognition.interimResults = false;\n        this.confirmationRecognition.lang = 'en-US';\n        \n        this.confirmationRecognition.onresult = (event) => {\n            if (event.results && event.results[0]) {\n                const response = event.results[0][0].transcript.toLowerCase().trim();\n                this.processConfirmation(response);\n            }\n        };\n    }\n    \n    /**\n     * Setup enhanced UI event listeners\n     */\n    setupUIEventListeners() {\n        console.log('UI event listeners setup complete');\n        \n        // Volume key detection for hands-free operation\n        document.addEventListener('keydown', (event) => {\n            if (event.code === 'AudioVolumeUp' || event.key === 'AudioVolumeUp') {\n                event.preventDefault();\n                this.startListening();\n            }\n        });\n        \n        // Main navigation button (primary interface)\n        const mainBtn = document.getElementById('mainButton');\n        if (mainBtn) {\n            mainBtn.addEventListener('click', () => {\n                if (this.isNavigating) {\n                    this.stopNavigation();\n                } else {\n                    this.startListening();\n                }\n                this.updateMainButtonState();\n            });\n        }\n        \n        // Navigation control buttons\n        const emergencyBtn = document.getElementById('emergencyStop');\n        if (emergencyBtn) {\n            emergencyBtn.addEventListener('click', () => {\n                this.emergencyStop();\n            });\n        }\n        \n        const showMapBtn = document.getElementById('showMapBtn');\n        if (showMapBtn) {\n            showMapBtn.addEventListener('click', () => {\n                this.showNavigationMap();\n            });\n        }\n        \n        const testVoiceBtn = document.getElementById('testVoiceBtn');\n        if (testVoiceBtn) {\n            testVoiceBtn.addEventListener('click', () => {\n                this.testVoiceRecognition();\n            });\n        }\n        \n        const resumeNavigationBtn = document.getElementById('resumeNavigation');\n        if (resumeNavigationBtn) {\n            resumeNavigationBtn.addEventListener('click', () => {\n                this.resumeNavigationFromMap();\n            });\n        }\n        \n        const toggleObstacleBtn = document.getElementById('toggleObstacleAlerts');\n        if (toggleObstacleBtn) {\n            toggleObstacleBtn.addEventListener('click', () => {\n                this.toggleObstacleAlerts();\n            });\n        }\n        \n        // Accessibility: Allow Enter key to activate main button\n        if (mainBtn) {\n            mainBtn.addEventListener('keydown', (event) => {\n                if (event.key === 'Enter' || event.key === ' ') {\n                    event.preventDefault();\n                    mainBtn.click();\n                }\n            });\n        }\n        \n        // Legacy button support\n        const startNavBtn = document.getElementById('startNavigationBtn');\n        if (startNavBtn) {\n            startNavBtn.addEventListener('click', () => this.startListening());\n        }\n        \n        const stopNavBtn = document.getElementById('stopNavigationBtn');\n        if (stopNavBtn) {\n            stopNavBtn.addEventListener('click', () => this.stopNavigation());\n        }\n    }\n    \n    /**\n     * Update main button state based on navigation status\n     */\n    updateMainButtonState() {\n        const mainBtn = document.getElementById('mainButton');\n        if (!mainBtn) return;\n        \n        if (this.isNavigating) {\n            mainBtn.innerHTML = '<i class=\"fas fa-stop\"></i> Stop Navigation';\n            mainBtn.classList.add('navigating');\n            mainBtn.classList.remove('listening');\n        } else if (this.awaitingConfirmation) {\n            mainBtn.innerHTML = '<i class=\"fas fa-microphone\"></i> Listening...';\n            mainBtn.classList.add('listening');\n            mainBtn.classList.remove('navigating');\n        } else {\n            mainBtn.innerHTML = '<i class=\"fas fa-microphone\"></i> Start Listening';\n            mainBtn.classList.remove('listening', 'navigating');\n        }\n        \n        // Show/hide navigation controls\n        const navControls = document.getElementById('navigationControls');\n        if (navControls) {\n            navControls.style.display = this.isNavigating ? 'block' : 'none';\n        }\n    }\n    \n    /**\n     * Emergency stop with immediate feedback\n     */\n    emergencyStop() {\n        console.log('Emergency stop activated');\n        this.speakWithPriority('Navigation stopped immediately.', 'high');\n        this.stopNavigation();\n        this.updateMainButtonState();\n    }\n    \n    /**\n     * Start listening for navigation commands\n     */\n    startListening() {\n        this.isActivelyListening = true;\n        if (this.awaitingConfirmation) {\n            this.confirmationRecognition.start();\n        } else {\n            this.recognition.start();\n        }\n    }\n    \n    /**\n     * Process navigation commands\n     */\n    async processNavigationCommand(command) {\n        console.log('Processing navigation command:', command);\n        \n        // Extract destination from command\n        let destination = this.extractDestination(command);\n        if (!destination) {\n            this.speak('I didn\\'t understand the destination. Please say \"take me to\" followed by a location.');\n            return;\n        }\n        \n        // Confirm navigation\n        this.currentDestination = destination;\n        this.awaitingConfirmation = true;\n        this.speak(`Should I start navigation to ${destination}?`);\n        this.updateStatusDisplay('Waiting for confirmation', 'Say \"yes\" or \"no\"');\n    }\n    \n    /**\n     * Extract destination from voice command\n     */\n    extractDestination(command) {\n        // Common patterns for navigation commands\n        const patterns = [\n            /(?:take me to|navigate to|go to|direction to|directions to)\\s+(.+)/i,\n            /(?:how to get to|where is|find)\\s+(.+)/i,\n            /(.+)/i // fallback - treat entire command as destination\n        ];\n        \n        for (const pattern of patterns) {\n            const match = command.match(pattern);\n            if (match && match[1]) {\n                return match[1].trim();\n            }\n        }\n        \n        return null;\n    }\n    \n    /**\n     * Process confirmation responses\n     */\n    async processConfirmation(response) {\n        this.awaitingConfirmation = false;\n        \n        if (response.includes('yes') || response.includes('yeah') || response.includes('start')) {\n            this.speak('Starting navigation...');\n            await this.startNavigation(this.currentDestination);\n        } else {\n            this.speak('Navigation cancelled.');\n            this.currentDestination = null;\n            this.updateStatusDisplay('Ready', 'Press button to start navigation');\n        }\n    }\n    \n    /**\n     * Enhanced navigation startup with comprehensive error handling\n     */\n    async startNavigation(destination) {\n        try {\n            // Reset navigation state\n            this.destinationReached = false;\n            this.urgentWarningGiven = false;\n            this.previewWarningGiven = false;\n            \n            if (!this.currentPosition) {\n                try {\n                    this.currentPosition = await this.getCurrentPosition();\n                } catch (error) {\n                    this.handleLocationError(error);\n                    return;\n                }\n            }\n            \n            this.updateStatusDisplay('Getting directions...', 'Please wait');\n            this.speakWithPriority('Getting directions to your destination.', 'normal');\n            \n            const origin = `${this.currentPosition.latitude},${this.currentPosition.longitude}`;\n            \n            // Call backend API to get directions\n            const response = await fetch('/api/directions', {\n                method: 'POST',\n                headers: { 'Content-Type': 'application/json' },\n                body: JSON.stringify({\n                    origin: origin,\n                    destination: destination\n                })\n            });\n            \n            const data = await response.json();\n            \n            if (!data.success) {\n                const errorMessage = data.message || 'Navigation failed. Please try again.';\n                this.errorStates.routingFailed = true;\n                this.speakErrorMessage(errorMessage);\n                this.updateStatusDisplay('Navigation Failed', errorMessage);\n                this.showErrorInUI('Route Not Found', errorMessage);\n                return;\n            }\n            \n            this.currentRoute = data;\n            this.currentStepIndex = 0;\n            this.isNavigating = true;\n            this.errorStates.routingFailed = false;\n            \n            // Start intelligent GPS tracking with battery optimization\n            this.startContinuousGPSTracking();\n            \n            // Start real-time obstacle alert system\n            this.startObstacleAlertSystem();\n            \n            // Display route on map if available\n            if (this.map && this.directionsRenderer) {\n                this.displayRouteOnMap();\n            }\n            \n            // Start navigation announcements\n            this.announceRoute();\n            \n            // Enable obstacle detection during navigation\n            this.startObstacleDetection();\n            \n            // Update UI state\n            this.updateMainButtonState();\n            \n            // Show navigation info overlay\n            const navigationInfo = document.getElementById('navigationInfo');\n            if (navigationInfo) {\n                navigationInfo.style.display = 'block';\n            }\n            \n            console.log('Enhanced navigation started successfully');\n            \n        } catch (error) {\n            console.error('Navigation start failed:', error);\n            this.errorStates.routingFailed = true;\n            const errorMessage = 'Failed to start navigation. Please check your connection and try again.';\n            this.speakErrorMessage(errorMessage);\n            this.showErrorInUI('Navigation Error', errorMessage);\n        }\n    }\n    \n    /**\n     * Announce route information\n     */\n    announceRoute() {\n        if (!this.currentRoute || !this.currentRoute.route) return;\n        \n        const route = this.currentRoute.route;\n        const totalDistance = route.distance;\n        const totalDuration = route.duration;\n        \n        this.speak(`Route found. Total distance ${totalDistance}, estimated time ${totalDuration}. Starting navigation.`);\n        \n        // Announce first step\n        setTimeout(() => {\n            this.announceCurrentStep();\n        }, 3000);\n    }\n    \n    /**\n     * Announce current navigation step with human-friendly voice instructions\n     */\n    announceCurrentStep() {\n        if (!this.isNavigating || !this.currentRoute) return;\n        \n        const steps = this.currentRoute.route.steps;\n        if (this.currentStepIndex >= steps.length) {\n            this.navigationComplete();\n            return;\n        }\n        \n        const currentStep = steps[this.currentStepIndex];\n        if (!currentStep) return;\n        \n        // Reset warning flags for new step\n        this.urgentWarningGiven = false;\n        this.previewWarningGiven = false;\n        \n        const instruction = this.optimizeVoiceInstruction(currentStep.instruction);\n        const distance = this.simplifyDistance(currentStep.distance_value || currentStep.distance_meters || 0);\n        \n        // Create clear, conversational voice instruction like \"Turn left in 20 meters\"\n        let voiceInstruction = `${instruction}`;\n        if (distance && !instruction.toLowerCase().includes('arrive')) {\n            voiceInstruction = `${instruction} in ${distance}`;\n        }\n        \n        this.speakWithPriority(voiceInstruction, 'normal');\n        this.updateStatusDisplay(`Step ${this.currentStepIndex + 1} of ${steps.length}`, instruction);\n        this.updateNavigationDisplay(instruction, distance);\n        \n        console.log(`Navigation step ${this.currentStepIndex + 1}: ${voiceInstruction}`);\n    }\n    \n    /**\n     * Update navigation display in UI\n     */\n    updateNavigationDisplay(instruction, distance) {\n        const currentStepElement = document.getElementById('currentStep');\n        const stepDistanceElement = document.getElementById('stepDistance');\n        \n        if (currentStepElement) {\n            currentStepElement.textContent = instruction;\n        }\n        \n        if (stepDistanceElement && distance) {\n            stepDistanceElement.textContent = `Distance: ${distance}`;\n        }\n        \n        // Show navigation info overlay\n        const navigationInfo = document.getElementById('navigationInfo');\n        if (navigationInfo) {\n            navigationInfo.style.display = 'block';\n        }\n    }\n    \n    /**\n     * Convert navigation data into human-friendly speech instructions\n     */\n    optimizeVoiceInstruction(instruction) {\n        let optimized = instruction.toLowerCase().trim();\n        \n        // Convert raw navigation data into conversational instructions\n        \n        // Handle turns with clear directional language\n        optimized = optimized.replace(/turn\\s+slight\\s+left/gi, 'bear left');\n        optimized = optimized.replace(/turn\\s+slight\\s+right/gi, 'bear right');\n        optimized = optimized.replace(/turn\\s+sharp\\s+left/gi, 'make a sharp left turn');\n        optimized = optimized.replace(/turn\\s+sharp\\s+right/gi, 'make a sharp right turn');\n        optimized = optimized.replace(/turn\\s+left/gi, 'turn left');\n        optimized = optimized.replace(/turn\\s+right/gi, 'turn right');\n        \n        // Handle straight movements\n        optimized = optimized.replace(/head\\s+north/gi, 'go straight ahead');\n        optimized = optimized.replace(/head\\s+south/gi, 'go straight ahead');\n        optimized = optimized.replace(/head\\s+east/gi, 'go straight ahead');\n        optimized = optimized.replace(/head\\s+west/gi, 'go straight ahead');\n        optimized = optimized.replace(/continue\\s+straight/gi, 'keep going straight');\n        optimized = optimized.replace(/proceed\\s+/gi, '');\n        optimized = optimized.replace(/continue\\s+/gi, 'keep going ');\n        \n        // Simplify common phrases\n        optimized = optimized.replace(/walk\\s+/gi, '');\n        optimized = optimized.replace(/go\\s+/gi, '');\n        optimized = optimized.replace(/head\\s+/gi, '');\n        optimized = optimized.replace(/use\\s+the\\s+/gi, 'take the ');\n        optimized = optimized.replace(/\\s+toward\\s+/gi, ' toward ');\n        optimized = optimized.replace(/\\s+towards\\s+/gi, ' toward ');\n        \n        // Handle destination phrases\n        optimized = optimized.replace(/destination\\s+will\\s+be\\s+on\\s+the\\s+/gi, 'your destination is on the ');\n        optimized = optimized.replace(/your\\s+destination\\s+is\\s+/gi, 'you will arrive ');\n        \n        // Handle street/road references\n        optimized = optimized.replace(/\\s+on\\s+([A-Za-z\\s]+)\\s+road/gi, ' on $1 Road');\n        optimized = optimized.replace(/\\s+on\\s+([A-Za-z\\s]+)\\s+street/gi, ' on $1 Street');\n        optimized = optimized.replace(/\\s+on\\s+([A-Za-z\\s]+)\\s+avenue/gi, ' on $1 Avenue');\n        \n        // Add helpful distance context\n        optimized = optimized.replace(/^(.+)$/i, (match) => {\n            // Don't repeat if already contains distance context\n            if (match.includes('meter') || match.includes('km') || match.includes('mile')) {\n                return match;\n            }\n            return match;\n        });\n        \n        // Clean up multiple spaces and capitalize\n        optimized = optimized.replace(/\\s+/g, ' ').trim();\n        optimized = optimized.charAt(0).toUpperCase() + optimized.slice(1);\n        \n        // Ensure instruction ends properly\n        if (!optimized.endsWith('.') && !optimized.endsWith('!')) {\n            optimized += '.';\n        }\n        \n        return optimized;\n    }\n    \n    /**\n     * Simplify distance for voice announcements\n     */\n    simplifyDistance(meters) {\n        if (meters < 50) {\n            return `${Math.round(meters / 10) * 10} meters`;\n        } else if (meters < 1000) {\n            return `${Math.round(meters / 50) * 50} meters`;\n        } else {\n            const km = (meters / 1000).toFixed(1);\n            return `${km} kilometers`;\n        }\n    }\n    \n    /**\n     * Start intelligent GPS tracking with battery optimization\n     */\n    startContinuousGPSTracking() {\n        if (this.watchId) {\n            navigator.geolocation.clearWatch(this.watchId);\n        }\n        \n        this.watchId = navigator.geolocation.watchPosition(\n            (position) => {\n                this.updateNavigationPosition(position.coords);\n            },\n            (error) => {\n                this.handleLocationError(error);\n                \n                // Retry GPS after error with exponential backoff\n                setTimeout(() => {\n                    if (this.isNavigating && !this.watchId) {\n                        console.log('Retrying GPS tracking after error...');\n                        this.startContinuousGPSTracking();\n                    }\n                }, 5000);\n            },\n            {\n                enableHighAccuracy: true,\n                timeout: 10000,\n                maximumAge: 0 // Always get fresh location for navigation\n            }\n        );\n        \n        console.log('Enhanced GPS tracking started with battery optimization');\n    }\n    \n    /**\n     * Calculate user speed and determine if stationary for battery optimization\n     */\n    calculateUserSpeed(newCoords) {\n        if (!this.lastLocationUpdateTime || !this.currentPosition) {\n            this.lastLocationUpdateTime = Date.now();\n            return 0;\n        }\n        \n        const now = Date.now();\n        const timeDelta = (now - this.lastLocationUpdateTime) / 1000; // seconds\n        \n        if (timeDelta < 1) return this.userSpeed; // Avoid too frequent calculations\n        \n        const distance = this.calculateDistance(\n            this.currentPosition.latitude, this.currentPosition.longitude,\n            newCoords.latitude, newCoords.longitude\n        );\n        \n        const speed = distance / timeDelta; // m/s\n        this.userSpeed = speed;\n        this.lastLocationUpdateTime = now;\n        \n        // Determine if user is stationary\n        const wasStationary = this.stationary;\n        this.stationary = speed < this.config.stationarySpeedThreshold;\n        \n        // Log speed changes for battery optimization\n        if (wasStationary !== this.stationary) {\n            console.log(`User movement changed: ${this.stationary ? 'Stationary' : 'Moving'} (Speed: ${speed.toFixed(2)} m/s)`);\n            this.optimizeTrackingFrequency();\n        }\n        \n        return speed;\n    }\n    \n    /**\n     * Optimize GPS tracking frequency based on user movement to save battery\n     */\n    optimizeTrackingFrequency() {\n        if (!this.isNavigating) return;\n        \n        // Restart tracking with optimized settings\n        if (this.watchId) {\n            navigator.geolocation.clearWatch(this.watchId);\n        }\n        \n        const trackingOptions = {\n            enableHighAccuracy: true,\n            timeout: 10000,\n            maximumAge: this.stationary ? this.config.lowFrequencyInterval : 1000\n        };\n        \n        console.log(`Optimizing GPS tracking: ${this.stationary ? 'Low' : 'High'} frequency mode`);\n        \n        this.watchId = navigator.geolocation.watchPosition(\n            (position) => {\n                this.updateNavigationPosition(position.coords);\n            },\n            (error) => {\n                this.handleLocationError(error);\n            },\n            trackingOptions\n        );\n    }\n    \n    /**\n     * Enhanced position update with destination detection and human-friendly instructions\n     */\n    updateNavigationPosition(coords) {\n        if (!this.isNavigating || !this.currentRoute) return;\n        \n        const newLat = coords.latitude;\n        const newLng = coords.longitude;\n        \n        // Calculate user speed for battery optimization\n        this.calculateUserSpeed(coords);\n        \n        // Update current position\n        this.currentPosition = coords;\n        \n        // Update map marker if available\n        if (this.map && this.userMarker) {\n            this.userMarker.setPosition({ lat: newLat, lng: newLng });\n        }\n        \n        // Check if destination is reached first (highest priority)\n        if (this.checkDestinationReached(newLat, newLng)) {\n            return; // Stop processing if destination reached\n        }\n        \n        // Check if user reached current step\n        this.checkStepProgress(newLat, newLng);\n        \n        // Check if user deviated from route\n        this.checkRouteDeviation(newLat, newLng);\n        \n        // Provide proximity-based voice instructions\n        this.provideProximityInstructions(newLat, newLng);\n    }\n    \n    /**\n     * Check if user has reached the final destination\n     */\n    checkDestinationReached(lat, lng) {\n        if (!this.currentRoute || this.destinationReached) return false;\n        \n        const steps = this.currentRoute.route.steps;\n        const finalStep = steps[steps.length - 1];\n        \n        if (!finalStep || !finalStep.end_location) return false;\n        \n        const distanceToDestination = this.calculateDistance(\n            lat, lng, \n            finalStep.end_location.lat, \n            finalStep.end_location.lng\n        );\n        \n        if (distanceToDestination <= this.config.destinationReachedThreshold) {\n            this.destinationReached = true;\n            this.navigationComplete();\n            return true;\n        }\n        \n        return false;\n    }\n    \n    /**\n     * Provide human-friendly proximity-based voice instructions\n     */\n    provideProximityInstructions(lat, lng) {\n        if (!this.currentRoute || this.currentStepIndex >= this.currentRoute.route.steps.length) return;\n        \n        const currentStep = this.currentRoute.route.steps[this.currentStepIndex];\n        if (!currentStep || !currentStep.end_location) return;\n        \n        const distanceToStepEnd = this.calculateDistance(\n            lat, lng,\n            currentStep.end_location.lat,\n            currentStep.end_location.lng\n        );\n        \n        // Provide urgent warning for immediate turns\n        if (distanceToStepEnd <= this.config.urgentAnnouncementDistance && !this.urgentWarningGiven) {\n            const instruction = this.optimizeVoiceInstruction(currentStep.instruction);\n            this.speakWithPriority(`${instruction} now!`, 'high');\n            this.urgentWarningGiven = true;\n        }\n        // Provide advance warning\n        else if (distanceToStepEnd <= this.config.voicePreviewDistance && !this.previewWarningGiven) {\n            const instruction = this.optimizeVoiceInstruction(currentStep.instruction);\n            const distance = this.simplifyDistance(distanceToStepEnd);\n            this.speakWithPriority(`${instruction} in ${distance}`, 'normal');\n            this.previewWarningGiven = true;\n        }\n    }\n    \n    /**\n     * Enhanced step progress checking with warning flag management\n     */\n    checkStepProgress(lat, lng) {\n        if (!this.currentRoute || this.currentStepIndex >= this.currentRoute.route.steps.length) return;\n        \n        const currentStep = this.currentRoute.route.steps[this.currentStepIndex];\n        if (!currentStep || !currentStep.end_location) return;\n        \n        const stepEndLat = currentStep.end_location.lat;\n        const stepEndLng = currentStep.end_location.lng;\n        \n        // Calculate distance to step endpoint\n        const distance = this.calculateDistance(lat, lng, stepEndLat, stepEndLng);\n        \n        // If within threshold of step endpoint, advance to next step\n        if (distance <= this.config.stepProximityThreshold) {\n            this.currentStepIndex++;\n            \n            // Reset warning flags for next step\n            this.urgentWarningGiven = false;\n            this.previewWarningGiven = false;\n            \n            if (this.currentStepIndex >= this.currentRoute.route.steps.length) {\n                this.navigationComplete();\n            } else {\n                // Announce next step with human-friendly instruction\n                setTimeout(() => {\n                    this.announceCurrentStep();\n                }, 1000);\n            }\n            \n            console.log(`Advanced to navigation step ${this.currentStepIndex + 1}`);\n        }\n    }\n    \n    /**\n     * Check if user has deviated significantly from the planned route\n     */\n    checkRouteDeviation(lat, lng) {\n        if (!this.currentRoute || this.currentStepIndex >= this.currentRoute.route.steps.length) return;\n        \n        const currentStep = this.currentRoute.route.steps[this.currentStepIndex];\n        const stepStartLat = currentStep.start_location.lat;\n        const stepStartLng = currentStep.start_location.lng;\n        const stepEndLat = currentStep.end_location.lat;\n        const stepEndLng = currentStep.end_location.lng;\n        \n        // Calculate distance from user to the step route line\n        const distanceToRoute = this.calculateDistanceToLine(\n            lat, lng, \n            stepStartLat, stepStartLng, \n            stepEndLat, stepEndLng\n        );\n        \n        // If user is more than 50 meters off route, trigger rerouting\n        if (distanceToRoute > 50) {\n            this.handleRouteDeviation();\n        }\n    }\n    \n    /**\n     * Handle when user deviates from planned route\n     */\n    async handleRouteDeviation() {\n        if (this.reroutingInProgress) return;\n        \n        this.reroutingInProgress = true;\n        this.speak('You seem off route. Getting updated directions...');\n        \n        try {\n            // Get new route from current position\n            await this.startNavigation(this.currentDestination);\n            this.speak('Route updated. Follow new directions.');\n        } catch (error) {\n            console.error('Rerouting failed:', error);\n            this.speak('Unable to update route. Continue to destination.');\n        } finally {\n            this.reroutingInProgress = false;\n        }\n    }\n    \n    /**\n     * Update position during navigation\n     */\n    updatePosition(newPosition) {\n        this.currentPosition = newPosition;\n        \n        if (this.userMarker && this.map) {\n            this.userMarker.setPosition({\n                lat: newPosition.latitude,\n                lng: newPosition.longitude\n            });\n            this.map.panTo({\n                lat: newPosition.latitude,\n                lng: newPosition.longitude\n            });\n        }\n        \n        // Check if user reached current step\n        this.checkStepProgress();\n    }\n    \n    /**\n     * Check if user has reached the current step\n     */\n    checkStepProgress() {\n        if (!this.isNavigating || !this.currentRoute) return;\n        \n        const steps = this.currentRoute.route.steps;\n        if (this.currentStepIndex >= steps.length) return;\n        \n        const currentStep = steps[this.currentStepIndex];\n        const targetLocation = currentStep.end_location;\n        \n        const distance = this.calculateDistance(\n            { lat: this.currentPosition.latitude, lng: this.currentPosition.longitude },\n            { lat: targetLocation.lat, lng: targetLocation.lng }\n        );\n        \n        // If within threshold, move to next step\n        if (distance <= this.config.stepProximityThreshold) {\n            this.currentStepIndex++;\n            \n            if (this.currentStepIndex >= steps.length) {\n                this.navigationComplete();\n            } else {\n                // Announce next step after a brief pause\n                setTimeout(() => {\n                    this.announceCurrentStep();\n                }, 1000);\n            }\n        }\n    }\n    \n    /**\n     * Calculate distance between two coordinates (Haversine formula)\n     */\n    calculateDistance(pos1, pos2) {\n        const R = 6371e3; // Earth's radius in meters\n        const œÜ1 = pos1.lat * Math.PI / 180;\n        const œÜ2 = pos2.lat * Math.PI / 180;\n        const ŒîœÜ = (pos2.lat - pos1.lat) * Math.PI / 180;\n        const ŒîŒª = (pos2.lng - pos1.lng) * Math.PI / 180;\n\n        const a = Math.sin(ŒîœÜ/2) * Math.sin(ŒîœÜ/2) +\n                Math.cos(œÜ1) * Math.cos(œÜ2) *\n                Math.sin(ŒîŒª/2) * Math.sin(ŒîŒª/2);\n        const c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1-a));\n\n        return R * c; // Distance in meters\n    }\n    \n    /**\n     * Display route on Google Maps\n     */\n    displayRouteOnMap() {\n        if (!this.map || !this.directionsService || !this.currentRoute) return;\n        \n        console.log('Displaying route on map');\n        \n        const steps = this.currentRoute.route.steps;\n        if (!steps || steps.length === 0) return;\n        \n        const origin = new google.maps.LatLng(\n            this.currentPosition.latitude,\n            this.currentPosition.longitude\n        );\n        \n        const destination = new google.maps.LatLng(\n            steps[steps.length - 1].end_location.lat,\n            steps[steps.length - 1].end_location.lng\n        );\n        \n        const request = {\n            origin: origin,\n            destination: destination,\n            travelMode: google.maps.TravelMode.WALKING\n        };\n        \n        this.directionsService.route(request, (result, status) => {\n            if (status === google.maps.DirectionsStatus.OK) {\n                this.directionsRenderer.setDirections(result);\n                \n                // Add user marker\n                if (!this.userMarker) {\n                    this.userMarker = new google.maps.Marker({\n                        position: origin,\n                        map: this.map,\n                        title: 'Your Location',\n                        icon: {\n                            path: google.maps.SymbolPath.CIRCLE,\n                            scale: 8,\n                            fillColor: '#4285F4',\n                            fillOpacity: 1,\n                            strokeColor: '#ffffff',\n                            strokeWeight: 2\n                        }\n                    });\n                }\n            }\n        });\n    }\n    \n    /**\n     * Start obstacle detection during navigation\n     */\n    startObstacleDetection() {\n        if (!this.model || !this.camera) return;\n        \n        this.isDetecting = true;\n        this.detectObjects();\n        console.log('Obstacle detection started during navigation');\n    }\n    \n    /**\n     * Detect objects for obstacle avoidance\n     */\n    async detectObjects() {\n        if (!this.isDetecting || !this.model || !this.camera) return;\n        \n        try {\n            const predictions = await this.model.detect(this.camera);\n            \n            // Filter for potential obstacles\n            const obstacles = predictions.filter(pred => \n                ['person', 'bicycle', 'car', 'motorcycle', 'bus', 'truck', 'traffic light', 'stop sign'].includes(pred.class) && \n                pred.score > 0.5\n            );\n            \n            if (obstacles.length > 0) {\n                const obstacleTypes = [...new Set(obstacles.map(obs => obs.class))];\n                this.speak(`Obstacle detected: ${obstacleTypes.join(', ')} ahead.`, 'high');\n            }\n            \n        } catch (error) {\n            console.error('Object detection error:', error);\n        }\n        \n        // Continue detection\n        if (this.isDetecting) {\n            setTimeout(() => this.detectObjects(), 2000);\n        }\n    }\n    \n    /**\n     * Navigation completed\n     */\n    navigationComplete() {\n        this.isNavigating = false;\n        \n        // Stop GPS tracking\n        if (this.watchId) {\n            navigator.geolocation.clearWatch(this.watchId);\n            this.watchId = null;\n        }\n        \n        // Stop obstacle detection\n        this.isDetecting = false;\n        \n        // Hide navigation controls\n        const navControls = document.getElementById('navigationControls');\n        if (navControls) {\n            navControls.style.display = 'none';\n        }\n        \n        // Announce completion\n        this.speak('You have arrived at your destination. Navigation complete.');\n        this.updateStatusDisplay('Navigation Complete', 'You have arrived at your destination');\n        \n        // Clear route data\n        this.currentRoute = null;\n        this.currentStepIndex = 0;\n        this.currentDestination = null;\n        \n        console.log('Navigation completed successfully');\n    }\n    \n    /**\n     * Enhanced navigation stop with complete state cleanup\n     */\n    stopNavigation() {\n        console.log('Stopping navigation with complete cleanup');\n        \n        this.isNavigating = false;\n        this.reroutingInProgress = false;\n        this.destinationReached = false;\n        \n        // Stop intelligent GPS tracking\n        if (this.watchId) {\n            navigator.geolocation.clearWatch(this.watchId);\n            this.watchId = null;\n        }\n        \n        // Stop obstacle detection and alert system\n        this.stopObstacleAlertSystem();\n        \n        // Hide navigation UI elements\n        const navigationInfo = document.getElementById('navigationInfo');\n        if (navigationInfo) {\n            navigationInfo.style.display = 'none';\n        }\n        \n        const emergencyBtn = document.getElementById('emergencyStop');\n        if (emergencyBtn) {\n            emergencyBtn.style.display = 'none';\n        }\n        \n        // Legacy support\n        const navControls = document.getElementById('navigationControls');\n        if (navControls) {\n            navControls.style.display = 'none';\n        }\n        \n        // Clear map route\n        if (this.directionsRenderer) {\n            this.directionsRenderer.setDirections(null);\n        }\n        \n        // Cancel any current speech\n        if (this.speechSynthesis.speaking) {\n            this.speechSynthesis.cancel();\n        }\n        \n        // Announce stop\n        this.speakWithPriority('Navigation stopped.', 'high');\n        this.updateStatusDisplay('Ready to Navigate', 'Press the button or Volume Up key to start');\n        \n        // Clear route data and reset flags\n        this.currentRoute = null;\n        this.currentStepIndex = 0;\n        this.currentDestination = null;\n        this.awaitingConfirmation = false;\n        this.urgentWarningGiven = false;\n        this.previewWarningGiven = false;\n        this.userSpeed = 0;\n        this.stationary = false;\n        \n        // Update UI state\n        this.updateMainButtonState();\n        \n        console.log('Navigation stopped successfully');\n    }\n    \n    /**\n     * Calculate distance between two coordinates (Haversine formula)\n     */\n    calculateDistance(lat1, lng1, lat2, lng2) {\n        const R = 6371e3; // Earth's radius in meters\n        const œÜ1 = lat1 * Math.PI / 180;\n        const œÜ2 = lat2 * Math.PI / 180;\n        const ŒîœÜ = (lat2 - lat1) * Math.PI / 180;\n        const ŒîŒª = (lng2 - lng1) * Math.PI / 180;\n\n        const a = Math.sin(ŒîœÜ/2) * Math.sin(ŒîœÜ/2) +\n                Math.cos(œÜ1) * Math.cos(œÜ2) *\n                Math.sin(ŒîŒª/2) * Math.sin(ŒîŒª/2);\n        const c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1-a));\n\n        return R * c; // Distance in meters\n    }\n    \n    /**\n     * Calculate distance from point to line segment\n     */\n    calculateDistanceToLine(px, py, x1, y1, x2, y2) {\n        const A = px - x1;\n        const B = py - y1;\n        const C = x2 - x1;\n        const D = y2 - y1;\n\n        const dot = A * C + B * D;\n        const lenSq = C * C + D * D;\n        \n        if (lenSq === 0) {\n            return this.calculateDistance(px, py, x1, y1);\n        }\n        \n        let param = dot / lenSq;\n        \n        let xx, yy;\n        if (param < 0) {\n            xx = x1;\n            yy = y1;\n        } else if (param > 1) {\n            xx = x2;\n            yy = y2;\n        } else {\n            xx = x1 + param * C;\n            yy = y1 + param * D;\n        }\n\n        return this.calculateDistance(px, py, xx, yy);\n    }\n    \n    /**\n     * Enhanced speech synthesis with smooth overlapping voice cancellation\n     */\n    speak(text, priority = 'normal') {\n        this.speakWithPriority(text, priority);\n    }\n    \n    /**\n     * Speak with priority and overlapping voice management\n     */\n    speakWithPriority(text, priority = 'normal') {\n        console.log(`Speaking (${priority}): ${text}`);\n        \n        try {\n            // Cancel any pending speech cancellation\n            if (this.speechCancellationTimer) {\n                clearTimeout(this.speechCancellationTimer);\n                this.speechCancellationTimer = null;\n            }\n            \n            // Always cancel current speech before speaking new text\n            // This prevents overlapping voices and ensures clear communication\n            if (this.speechSynthesis.speaking || this.speechSynthesis.pending) {\n                this.speechSynthesis.cancel();\n                \n                // Small delay to ensure cancellation completes\n                setTimeout(() => {\n                    this.performSpeech(text, priority);\n                }, 100);\n            } else {\n                this.performSpeech(text, priority);\n            }\n            \n        } catch (error) {\n            console.error('Speech synthesis error:', error);\n            this.handleSpeechError(text);\n        }\n    }\n    \n    /**\n     * Perform the actual speech synthesis\n     */\n    performSpeech(text, priority = 'normal') {\n        try {\n            const utterance = new SpeechSynthesisUtterance(text);\n            \n            // Optimize speech settings for accessibility\n            utterance.rate = 0.9; // Slightly slower for clarity\n            utterance.volume = 1.0;\n            utterance.pitch = 1.0;\n            utterance.lang = 'en-US';\n            \n            // Set up event handlers\n            utterance.onstart = () => {\n                this.isSpeaking = true;\n                this.errorStates.speechFailed = false;\n                console.log('Speech started successfully');\n            };\n            \n            utterance.onend = () => {\n                this.isSpeaking = false;\n                console.log('Speech ended normally');\n            };\n            \n            utterance.onerror = (event) => {\n                console.error('Speech synthesis error:', event.error);\n                this.handleSpeechError(text);\n            };\n            \n            // Store last utterance for reference\n            this.lastUtterance = utterance;\n            \n            // Speak the text\n            this.speechSynthesis.speak(utterance);\n            \n        } catch (error) {\n            console.error('Speech creation error:', error);\n            this.handleSpeechError(text);\n        }\n    }\n    \n    /**\n     * Handle speech synthesis errors with fallback options\n     */\n    handleSpeechError(originalText) {\n        this.errorStates.speechFailed = true;\n        this.isSpeaking = false;\n        \n        console.error('Speech synthesis failed for text:', originalText);\n        \n        // Show the text in UI as fallback\n        this.showTextInUI(originalText);\n        \n        // Try to reinitialize speech synthesis\n        setTimeout(() => {\n            if ('speechSynthesis' in window) {\n                this.speechSynthesis = window.speechSynthesis;\n                console.log('Speech synthesis reinitialized after error');\n            }\n        }, 1000);\n    }\n    \n    /**\n     * Show text in UI when speech fails\n     */\n    showTextInUI(text) {\n        const textDisplay = document.getElementById('speechFallbackDisplay');\n        if (textDisplay) {\n            textDisplay.textContent = text;\n            textDisplay.style.display = 'block';\n            \n            // Auto-hide after 5 seconds\n            setTimeout(() => {\n                textDisplay.style.display = 'none';\n            }, 5000);\n        }\n    }\n    \n    /**\n     * Speak error messages with special handling\n     */\n    speakErrorMessage(errorText) {\n        // Use high priority for error messages\n        this.speakWithPriority(errorText, 'high');\n        \n        // Also show in UI for redundancy\n        this.showTextInUI(errorText);\n    }\n    \n    /**\n     * Update status display\n     */\n    updateStatusDisplay(title, subtitle) {\n        const statusTitle = document.getElementById('navigationStatus');\n        const statusSubtitle = document.getElementById('navigationSubtitle');\n        \n        if (statusTitle) statusTitle.textContent = title;\n        if (statusSubtitle) statusSubtitle.textContent = subtitle;\n    }\n    \n    /**\n     * Show navigation map in modal\n     */\n    showNavigationMap() {\n        console.log('Showing navigation map');\n        \n        if (!this.currentRoute) {\n            this.speak('No active route to display. Please start navigation first.', true);\n            return;\n        }\n        \n        // Initialize map in modal if not already done\n        this.initializeMapModal();\n        \n        // Show the modal\n        const mapModal = new bootstrap.Modal(document.getElementById('navigationMapModal'));\n        mapModal.show();\n        \n        // Update route info\n        const routeInfo = document.getElementById('routeInfo');\n        if (routeInfo && this.currentRoute) {\n            const totalDistance = this.currentRoute.legs[0].distance.text;\n            const totalTime = this.currentRoute.legs[0].duration.text;\n            const currentStep = this.currentStepIndex + 1;\n            const totalSteps = this.currentRoute.legs[0].steps.length;\n            \n            routeInfo.textContent = `Step ${currentStep}/${totalSteps} ‚Ä¢ ${totalDistance} ‚Ä¢ ${totalTime}`;\n        }\n        \n        this.speak('Navigation map is now displayed. Use the Resume Navigation button to continue.', true);\n    }\n    \n    /**\n     * Initialize map in modal\n     */\n    initializeMapModal() {\n        if (this.modalMap) return; // Already initialized\n        \n        const mapContainer = document.getElementById('navigationMap');\n        if (!mapContainer) return;\n        \n        // Create map with current location\n        this.modalMap = new google.maps.Map(mapContainer, {\n            zoom: 15,\n            center: this.currentPosition ? \n                { lat: this.currentPosition.latitude, lng: this.currentPosition.longitude } :\n                { lat: 0, lng: 0 },\n            mapTypeId: google.maps.MapTypeId.ROADMAP\n        });\n        \n        // Show current route\n        if (this.currentRoute) {\n            const directionsRenderer = new google.maps.DirectionsRenderer({\n                directions: { routes: [this.currentRoute] },\n                map: this.modalMap,\n                suppressMarkers: false\n            });\n        }\n        \n        // Show current position\n        if (this.currentPosition) {\n            this.modalUserMarker = new google.maps.Marker({\n                position: { lat: this.currentPosition.latitude, lng: this.currentPosition.longitude },\n                map: this.modalMap,\n                title: 'Your Current Location',\n                icon: {\n                    url: 'data:image/svg+xml;charset=UTF-8,' + encodeURIComponent('<svg width=\"20\" height=\"20\" viewBox=\"0 0 20 20\" xmlns=\"http://www.w3.org/2000/svg\"><circle cx=\"10\" cy=\"10\" r=\"8\" fill=\"#007bff\" stroke=\"#fff\" stroke-width=\"2\"/></svg>'),\n                    scaledSize: new google.maps.Size(20, 20)\n                }\n            });\n        }\n    }\n    \n    /**\n     * Resume navigation from map modal\n     */\n    resumeNavigationFromMap() {\n        console.log('Resuming navigation from map');\n        \n        // Close the modal\n        const mapModal = bootstrap.Modal.getInstance(document.getElementById('navigationMapModal'));\n        if (mapModal) {\n            mapModal.hide();\n        }\n        \n        this.speak('Navigation resumed. Continue following the voice instructions.', true);\n    }\n    \n    /**\n     * Test voice recognition functionality\n     */\n    testVoiceRecognition() {\n        console.log('Testing voice recognition');\n        \n        this.speak('Voice recognition test starting. Please say something after the beep.', true);\n        \n        // Give a moment for the announcement to finish\n        setTimeout(() => {\n            if (!this.recognition) {\n                this.speak('Voice recognition is not available on this device.', true);\n                return;\n            }\n            \n            // Test recognition\n            const testRecognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();\n            testRecognition.continuous = false;\n            testRecognition.interimResults = false;\n            testRecognition.lang = 'en-US';\n            \n            testRecognition.onstart = () => {\n                this.speak('Now listening. Say anything to test voice recognition.', true);\n            };\n            \n            testRecognition.onresult = (event) => {\n                const transcript = event.results[0][0].transcript;\n                const confidence = event.results[0][0].confidence;\n                \n                console.log('Voice test result:', transcript, 'Confidence:', confidence);\n                this.speak(`Voice recognition working. I heard: ${transcript}`, true);\n            };\n            \n            testRecognition.onerror = (event) => {\n                console.error('Voice test error:', event.error);\n                let errorMessage = 'Voice recognition test failed.';\n                \n                switch (event.error) {\n                    case 'not-allowed':\n                        errorMessage = 'Microphone access denied. Please allow microphone access.';\n                        break;\n                    case 'no-speech':\n                        errorMessage = 'No speech detected. Please try speaking clearly.';\n                        break;\n                    case 'audio-capture':\n                        errorMessage = 'No microphone found. Please check your microphone.';\n                        break;\n                    case 'network':\n                        errorMessage = 'Network error. Please check your internet connection.';\n                        break;\n                }\n                \n                this.speak(errorMessage, true);\n            };\n            \n            testRecognition.onend = () => {\n                console.log('Voice recognition test completed');\n            };\n            \n            try {\n                testRecognition.start();\n            } catch (error) {\n                console.error('Failed to start voice test:', error);\n                this.speak('Failed to start voice recognition test. Please try again.', true);\n            }\n        }, 2000);\n    }\n    \n    /**\n     * Start Real-time Obstacle Alert System\n     */\n    startObstacleAlertSystem() {\n        console.log('Starting real-time obstacle alert system');\n        \n        if (!this.model) {\n            console.warn('COCO-SSD model not loaded, obstacle alerts disabled');\n            return;\n        }\n        \n        if (!this.camera) {\n            console.warn('Camera not available, obstacle alerts disabled');\n            return;\n        }\n        \n        this.obstacleAlertEnabled = true;\n        this.isDetecting = true;\n        \n        // Start obstacle detection at configured frequency\n        this.obstacleDetectionInterval = setInterval(() => {\n            if (this.isNavigating && this.obstacleAlertEnabled) {\n                this.detectObstacles();\n            }\n        }, this.config.obstacleDetectionFrequency);\n        \n        this.speakWithPriority('Obstacle alert system activated', 'medium');\n        console.log('Obstacle alert system started');\n    }\n    \n    /**\n     * Stop Real-time Obstacle Alert System\n     */\n    stopObstacleAlertSystem() {\n        console.log('Stopping obstacle alert system');\n        \n        this.obstacleAlertEnabled = false;\n        this.isDetecting = false;\n        \n        if (this.obstacleDetectionInterval) {\n            clearInterval(this.obstacleDetectionInterval);\n            this.obstacleDetectionInterval = null;\n        }\n        \n        // Clear detected obstacles\n        this.detectedObstacles.clear();\n        \n        console.log('Obstacle alert system stopped');\n    }\n    \n    /**\n     * Detect obstacles using COCO-SSD model\n     */\n    async detectObstacles() {\n        if (!this.model || !this.camera || !this.isNavigating) {\n            return;\n        }\n        \n        try {\n            // Get video element for detection\n            const video = this.camera;\n            if (video.readyState !== 4) return; // Video not ready\n            \n            // Run object detection\n            const predictions = await this.model.detect(video);\n            \n            // Process predictions for obstacle alerts\n            this.processObstacleDetections(predictions);\n            \n        } catch (error) {\n            console.error('Obstacle detection error:', error);\n        }\n    }\n    \n    /**\n     * Process obstacle detections and trigger alerts\n     */\n    processObstacleDetections(predictions) {\n        const now = Date.now();\n        const currentObstacles = new Map();\n        \n        // Analyze each prediction\n        for (const prediction of predictions) {\n            const { class: className, score, bbox } = prediction;\n            const [x, y, width, height] = bbox;\n            \n            // Calculate obstacle size relative to screen\n            const obstacleSize = (width * height) / (this.camera.videoWidth * this.camera.videoHeight);\n            \n            // Skip small objects\n            if (obstacleSize < this.obstacleThresholds.minSize) continue;\n            \n            // Determine obstacle priority and threshold\n            let priority = 'none';\n            let threshold = 1.0;\n            \n            if (this.criticalObstacles.includes(className)) {\n                priority = 'critical';\n                threshold = this.obstacleThresholds.critical;\n            } else if (this.warningObstacles.includes(className)) {\n                priority = 'warning';\n                threshold = this.obstacleThresholds.warning;\n            }\n            \n            // Check if obstacle meets confidence threshold\n            if (score >= threshold && priority !== 'none') {\n                const obstacleKey = `${className}_${Math.round(x)}_${Math.round(y)}`;\n                \n                currentObstacles.set(obstacleKey, {\n                    className,\n                    score,\n                    bbox,\n                    priority,\n                    size: obstacleSize,\n                    position: this.determineObstaclePosition(x, width),\n                    distance: this.estimateObstacleDistance(width, height, className)\n                });\n            }\n        }\n        \n        // Update persistent obstacle tracking\n        this.updateObstacleTracking(currentObstacles);\n        \n        // Generate alerts for persistent obstacles\n        this.generateObstacleAlerts(currentObstacles);\n    }\n    \n    /**\n     * Determine obstacle position relative to user (left, center, right)\n     */\n    determineObstaclePosition(x, width) {\n        const centerX = x + width / 2;\n        const screenWidth = this.camera.videoWidth;\n        const leftThird = screenWidth / 3;\n        const rightThird = (screenWidth * 2) / 3;\n        \n        if (centerX < leftThird) return 'left';\n        if (centerX > rightThird) return 'right';\n        return 'center';\n    }\n    \n    /**\n     * Estimate obstacle distance based on size and type\n     */\n    estimateObstacleDistance(width, height, className) {\n        const objectSize = width * height;\n        const screenSize = this.camera.videoWidth * this.camera.videoHeight;\n        const sizeRatio = objectSize / screenSize;\n        \n        // Simple distance estimation based on object size\n        if (sizeRatio > 0.4) return 'very close';\n        if (sizeRatio > 0.2) return 'close';\n        if (sizeRatio > 0.1) return 'medium';\n        return 'far';\n    }\n    \n    /**\n     * Update obstacle tracking for persistence\n     */\n    updateObstacleTracking(currentObstacles) {\n        const now = Date.now();\n        \n        // Add new obstacles or update existing ones\n        for (const [key, obstacle] of currentObstacles) {\n            if (this.detectedObstacles.has(key)) {\n                // Update existing obstacle\n                const existing = this.detectedObstacles.get(key);\n                existing.lastSeen = now;\n                existing.detectionCount++;\n                existing.score = Math.max(existing.score, obstacle.score);\n            } else {\n                // Add new obstacle\n                this.detectedObstacles.set(key, {\n                    ...obstacle,\n                    firstSeen: now,\n                    lastSeen: now,\n                    detectionCount: 1,\n                    alertGiven: false\n                });\n            }\n        }\n        \n        // Remove old obstacles (not seen for 2 seconds)\n        for (const [key, obstacle] of this.detectedObstacles) {\n            if (now - obstacle.lastSeen > 2000) {\n                this.detectedObstacles.delete(key);\n            }\n        }\n    }\n    \n    /**\n     * Generate audio alerts for detected obstacles\n     */\n    generateObstacleAlerts(currentObstacles) {\n        const now = Date.now();\n        \n        // Check cooldown period\n        if (now - this.lastObstacleAlert < this.obstacleAlertCooldown) {\n            return;\n        }\n        \n        // Find the most urgent obstacle to alert about\n        let mostUrgentObstacle = null;\n        let highestPriority = 0;\n        \n        for (const [key, obstacle] of this.detectedObstacles) {\n            // Only alert about persistent obstacles (seen multiple times)\n            if (obstacle.detectionCount < 2 || obstacle.alertGiven) continue;\n            \n            let priorityScore = 0;\n            if (obstacle.priority === 'critical') priorityScore = 10;\n            if (obstacle.priority === 'warning') priorityScore = 5;\n            \n            // Add urgency based on distance\n            if (obstacle.distance === 'very close') priorityScore += 5;\n            if (obstacle.distance === 'close') priorityScore += 3;\n            if (obstacle.distance === 'medium') priorityScore += 1;\n            \n            if (priorityScore > highestPriority) {\n                highestPriority = priorityScore;\n                mostUrgentObstacle = obstacle;\n            }\n        }\n        \n        // Generate alert for most urgent obstacle\n        if (mostUrgentObstacle) {\n            this.announceObstacle(mostUrgentObstacle);\n            mostUrgentObstacle.alertGiven = true;\n            this.lastObstacleAlert = now;\n        }\n    }\n    \n    /**\n     * Announce detected obstacle with appropriate urgency\n     */\n    announceObstacle(obstacle) {\n        let alertMessage = '';\n        let speechPriority = 'medium';\n        \n        // Determine speech priority\n        if (obstacle.priority === 'critical' && obstacle.distance === 'very close') {\n            speechPriority = 'high';\n        } else if (obstacle.priority === 'critical') {\n            speechPriority = 'medium';\n        }\n        \n        // Create natural alert message\n        const directionText = obstacle.position === 'center' ? 'ahead' : `on your ${obstacle.position}`;\n        const distanceText = obstacle.distance === 'very close' ? 'very close' : obstacle.distance;\n        \n        // Customize message based on object type\n        if (obstacle.className === 'person') {\n            alertMessage = `Person ${distanceText} ${directionText}`;\n        } else if (['car', 'truck', 'bus'].includes(obstacle.className)) {\n            alertMessage = `Vehicle ${distanceText} ${directionText}`;\n        } else if (['bicycle', 'motorcycle'].includes(obstacle.className)) {\n            alertMessage = `${obstacle.className} ${distanceText} ${directionText}`;\n        } else {\n            alertMessage = `Obstacle ${distanceText} ${directionText}`;\n        }\n        \n        // Add urgency marker for critical close obstacles\n        if (obstacle.priority === 'critical' && obstacle.distance === 'very close') {\n            alertMessage = 'Caution! ' + alertMessage;\n        }\n        \n        console.log('Obstacle alert:', alertMessage);\n        this.speakWithPriority(alertMessage, speechPriority);\n    }\n    \n    /**\n     * Toggle obstacle alerts on/off\n     */\n    toggleObstacleAlerts() {\n        if (this.obstacleAlertEnabled) {\n            this.obstacleAlertEnabled = false;\n            this.speakWithPriority('Obstacle alerts disabled', 'medium');\n            console.log('Obstacle alerts disabled by user');\n            \n            // Update button text\n            const btn = document.getElementById('toggleObstacleAlerts');\n            if (btn) {\n                btn.innerHTML = '<i class=\"fas fa-triangle-exclamation\"></i> Enable Obstacle Alerts';\n                btn.classList.remove('btn-outline-warning');\n                btn.classList.add('btn-outline-success');\n            }\n        } else {\n            this.obstacleAlertEnabled = true;\n            this.speakWithPriority('Obstacle alerts enabled', 'medium');\n            console.log('Obstacle alerts enabled by user');\n            \n            // Update button text\n            const btn = document.getElementById('toggleObstacleAlerts');\n            if (btn) {\n                btn.innerHTML = '<i class=\"fas fa-triangle-exclamation\"></i> Disable Obstacle Alerts';\n                btn.classList.remove('btn-outline-success');\n                btn.classList.add('btn-outline-warning');\n            }\n        }\n    }\n}\n\n// Initialize navigation system when DOM is ready\ndocument.addEventListener('DOMContentLoaded', () => {\n    console.log('Initializing BlindMate Navigation System...');\n    window.blindMateNavigation = new UniversalNavigation();\n});\n\n// Enhanced error handling to prevent console errors\nwindow.addEventListener('error', (event) => {\n    console.error('Navigation system error:', event.error);\n});\n\nwindow.addEventListener('unhandledrejection', (event) => {\n    console.error('Navigation system unhandled promise rejection:', event.reason);\n    event.preventDefault();\n});","size_bytes":76049},"static/js/onboarding.js":{"content":"/**\n * BlindMate Onboarding Tutorial JavaScript\n * Comprehensive tutorial system for first-time users\n */\n\nclass OnboardingTutorial {\n    constructor() {\n        this.currentStep = 1;\n        this.totalSteps = 8;\n        this.audioEnabled = true;\n        this.synthesis = window.speechSynthesis;\n        this.practiceExercises = [\n            {\n                title: \"Exercise 1: Wake Word Practice\",\n                instruction: \"Say 'Hey BlindMate, start detection' and wait for a response.\",\n                expectedResponse: \"detection\",\n                successMessage: \"Perfect! You've mastered the wake word feature.\"\n            },\n            {\n                title: \"Exercise 2: Universal Navigation\",\n                instruction: \"Say 'Hey BlindMate, take me to Times Square' to practice global navigation.\",\n                expectedResponse: \"times square\",\n                successMessage: \"Excellent! You can now navigate anywhere worldwide.\"\n            },\n            {\n                title: \"Exercise 3: Voice Customization\",\n                instruction: \"Say 'Change tone to energetic' to practice voice customization.\",\n                expectedResponse: \"energetic\",\n                successMessage: \"Great! You've learned how to customize BlindMate's voice.\"\n            },\n            {\n                title: \"Exercise 4: Language Switching\",\n                instruction: \"Say 'Change language to Hindi' to practice multilingual features.\",\n                expectedResponse: \"hindi\",\n                successMessage: \"Wonderful! You can now use BlindMate in 15 different languages.\"\n            }\n        ];\n        this.currentExercise = 0;\n        this.practiceProgress = 0;\n        \n        this.init();\n    }\n    \n    init() {\n        this.updateProgressIndicator();\n        this.initializeSpeechRecognition();\n        this.speakCurrentStep();\n        \n        // Announce tutorial start\n        setTimeout(() => {\n            this.speak(\"Welcome to the BlindMate tutorial. This interactive guide will teach you how to use all features safely. You can navigate using the next and previous buttons, or use keyboard shortcuts. Press space for next, or escape to exit the tutorial.\");\n        }, 1000);\n        \n        // Keyboard shortcuts\n        document.addEventListener('keydown', (e) => {\n            switch(e.key) {\n                case ' ':\n                case 'ArrowRight':\n                    e.preventDefault();\n                    this.nextStep();\n                    break;\n                case 'ArrowLeft':\n                    e.preventDefault();\n                    this.previousStep();\n                    break;\n                case 'Escape':\n                    this.exitTutorial();\n                    break;\n                case 'r':\n                case 'R':\n                    if (e.ctrlKey) {\n                        e.preventDefault();\n                        this.repeatCurrentStep();\n                    }\n                    break;\n            }\n        });\n    }\n    \n    initializeSpeechRecognition() {\n        if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {\n            console.warn('Speech recognition not available');\n            return;\n        }\n        \n        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n        this.recognition = new SpeechRecognition();\n        \n        this.recognition.continuous = false;\n        this.recognition.interimResults = false;\n        this.recognition.lang = 'en-US';\n        \n        this.recognition.onresult = (event) => {\n            const command = event.results[0][0].transcript.toLowerCase().trim();\n            this.handleVoiceCommand(command);\n        };\n        \n        this.recognition.onerror = (event) => {\n            console.error('Speech recognition error:', event.error);\n        };\n    }\n    \n    handleVoiceCommand(command) {\n        // Tutorial navigation commands\n        if (command.includes('next') || command.includes('continue')) {\n            this.nextStep();\n        } else if (command.includes('previous') || command.includes('back')) {\n            this.previousStep();\n        } else if (command.includes('repeat')) {\n            this.repeatCurrentStep();\n        } else if (command.includes('exit') || command.includes('quit')) {\n            this.exitTutorial();\n        }\n        \n        // Practice exercise handling\n        if (this.currentStep === 7 && this.isPracticing) {\n            this.handlePracticeCommand(command);\n        }\n    }\n    \n    updateProgressIndicator() {\n        const progressDots = document.getElementById('progressDots');\n        const progressText = document.getElementById('progressText');\n        \n        // Create progress dots\n        progressDots.innerHTML = '';\n        for (let i = 1; i <= this.totalSteps; i++) {\n            const dot = document.createElement('div');\n            dot.className = 'progress-dot';\n            \n            if (i < this.currentStep) {\n                dot.classList.add('completed');\n            } else if (i === this.currentStep) {\n                dot.classList.add('active');\n            }\n            \n            progressDots.appendChild(dot);\n        }\n        \n        progressText.textContent = `Step ${this.currentStep} of ${this.totalSteps}`;\n    }\n    \n    showStep(stepNumber) {\n        // Hide all steps\n        document.querySelectorAll('.step-card').forEach(card => {\n            card.classList.remove('active');\n        });\n        \n        // Show current step\n        const currentStepCard = document.querySelector(`[data-step=\"${stepNumber}\"]`);\n        if (currentStepCard) {\n            currentStepCard.classList.add('active');\n        }\n        \n        this.updateProgressIndicator();\n        this.updateNavigationButtons();\n        \n        // Announce step change\n        setTimeout(() => {\n            this.speakCurrentStep();\n        }, 500);\n    }\n    \n    speakCurrentStep() {\n        if (!this.audioEnabled) return;\n        \n        const currentStepCard = document.querySelector(`[data-step=\"${this.currentStep}\"]`);\n        if (!currentStepCard) return;\n        \n        const title = currentStepCard.querySelector('.step-title').textContent;\n        const content = currentStepCard.querySelector('.step-content p').textContent;\n        \n        const announcement = `Step ${this.currentStep} of ${this.totalSteps}: ${title}. ${content}`;\n        this.speak(announcement);\n    }\n    \n    nextStep() {\n        if (this.currentStep < this.totalSteps) {\n            this.currentStep++;\n            this.showStep(this.currentStep);\n            \n            // Special handling for practice step\n            if (this.currentStep === 7) {\n                this.initializePracticeSession();\n            }\n        }\n    }\n    \n    previousStep() {\n        if (this.currentStep > 1) {\n            this.currentStep--;\n            this.showStep(this.currentStep);\n        }\n    }\n    \n    updateNavigationButtons() {\n        const prevButton = document.getElementById('prevButton');\n        const nextButton = document.getElementById('nextButton');\n        \n        prevButton.disabled = this.currentStep === 1;\n        \n        if (this.currentStep === this.totalSteps) {\n            nextButton.innerHTML = '<i class=\"fas fa-check\"></i> Complete';\n            nextButton.onclick = () => this.completeTutorial();\n        } else {\n            nextButton.innerHTML = 'Next <i class=\"fas fa-chevron-right\"></i>';\n            nextButton.onclick = () => this.nextStep();\n        }\n    }\n    \n    initializePracticeSession() {\n        this.currentExercise = 0;\n        this.practiceProgress = 0;\n        this.updatePracticeExercise();\n        \n        // Show practice progress\n        const practiceProgressDiv = document.getElementById('practiceProgress');\n        practiceProgressDiv.style.display = 'block';\n    }\n    \n    updatePracticeExercise() {\n        if (this.currentExercise >= this.practiceExercises.length) {\n            this.completePracticeSession();\n            return;\n        }\n        \n        const exercise = this.practiceExercises[this.currentExercise];\n        \n        document.getElementById('exerciseTitle').textContent = exercise.title;\n        document.getElementById('exerciseInstruction').textContent = exercise.instruction;\n        \n        const progressBar = document.getElementById('practiceProgressBar');\n        const progressPercent = (this.currentExercise / this.practiceExercises.length) * 100;\n        progressBar.style.width = `${progressPercent}%`;\n        \n        document.getElementById('practiceStatus').textContent = `Exercise ${this.currentExercise + 1} of ${this.practiceExercises.length}`;\n        \n        // Reset practice button\n        const practiceButton = document.getElementById('practiceButton');\n        practiceButton.innerHTML = '<i class=\"fas fa-microphone\"></i> Start Practice';\n        practiceButton.onclick = () => this.startPractice();\n        \n        this.isPracticing = false;\n        \n        this.speak(exercise.instruction);\n    }\n    \n    startPractice() {\n        if (!this.recognition) {\n            this.speak('Speech recognition is not available in your browser.');\n            return;\n        }\n        \n        this.isPracticing = true;\n        const practiceButton = document.getElementById('practiceButton');\n        practiceButton.innerHTML = '<i class=\"fas fa-stop\"></i> Stop Practice';\n        practiceButton.onclick = () => this.stopPractice();\n        \n        document.getElementById('practiceStatus').textContent = 'Listening... Speak now';\n        \n        this.speak('I\\'m listening. Please speak your command now.');\n        \n        setTimeout(() => {\n            try {\n                this.recognition.start();\n            } catch (error) {\n                console.error('Speech recognition error:', error);\n                this.speak('Unable to start speech recognition. Please try again.');\n                this.stopPractice();\n            }\n        }, 2000);\n    }\n    \n    stopPractice() {\n        this.isPracticing = false;\n        if (this.recognition) {\n            this.recognition.stop();\n        }\n        \n        const practiceButton = document.getElementById('practiceButton');\n        practiceButton.innerHTML = '<i class=\"fas fa-microphone\"></i> Start Practice';\n        practiceButton.onclick = () => this.startPractice();\n        \n        document.getElementById('practiceStatus').textContent = 'Practice stopped';\n    }\n    \n    handlePracticeCommand(command) {\n        const exercise = this.practiceExercises[this.currentExercise];\n        \n        if (command.includes(exercise.expectedResponse)) {\n            this.speak(exercise.successMessage);\n            document.getElementById('practiceStatus').textContent = 'Success! Moving to next exercise...';\n            \n            setTimeout(() => {\n                this.currentExercise++;\n                this.updatePracticeExercise();\n            }, 2000);\n        } else {\n            this.speak('Good try! Let\\'s practice that command again. ' + exercise.instruction);\n            document.getElementById('practiceStatus').textContent = 'Try again - listen for the instruction';\n        }\n        \n        this.stopPractice();\n    }\n    \n    skipExercise() {\n        this.speak('Skipping exercise');\n        this.currentExercise++;\n        this.updatePracticeExercise();\n    }\n    \n    completePracticeSession() {\n        const progressBar = document.getElementById('practiceProgressBar');\n        progressBar.style.width = '100%';\n        \n        document.getElementById('practiceStatus').textContent = 'All exercises completed!';\n        document.getElementById('exerciseTitle').textContent = 'Practice Session Complete';\n        document.getElementById('exerciseInstruction').textContent = 'Congratulations! You\\'ve practiced all the essential BlindMate commands.';\n        \n        const practiceButton = document.getElementById('practiceButton');\n        practiceButton.innerHTML = '<i class=\"fas fa-check\"></i> Practice Complete';\n        practiceButton.disabled = true;\n        \n        this.speak('Excellent work! You\\'ve completed all practice exercises and are ready to use BlindMate confidently.');\n    }\n    \n    repeatCurrentStep() {\n        this.speakCurrentStep();\n    }\n    \n    toggleAudio() {\n        this.audioEnabled = !this.audioEnabled;\n        const audioToggle = document.getElementById('audioToggle');\n        \n        if (this.audioEnabled) {\n            audioToggle.innerHTML = '<i class=\"fas fa-volume-up\"></i> Audio On';\n            this.speak('Audio enabled');\n        } else {\n            audioToggle.innerHTML = '<i class=\"fas fa-volume-mute\"></i> Audio Off';\n            this.synthesis.cancel();\n        }\n    }\n    \n    speak(text, priority = false) {\n        if (!this.audioEnabled) return;\n        \n        if (priority) {\n            this.synthesis.cancel();\n        }\n        \n        const utterance = new SpeechSynthesisUtterance(text);\n        utterance.rate = 0.8;\n        utterance.pitch = 1;\n        utterance.volume = 0.9;\n        \n        // Use a clear, friendly voice if available\n        const voices = this.synthesis.getVoices();\n        const englishVoice = voices.find(voice => \n            voice.lang.startsWith('en') && voice.name.includes('Female')\n        ) || voices.find(voice => voice.lang.startsWith('en'));\n        \n        if (englishVoice) {\n            utterance.voice = englishVoice;\n        }\n        \n        this.synthesis.speak(utterance);\n    }\n    \n    completeTutorial() {\n        this.speak('Congratulations! You have completed the BlindMate tutorial. You are now ready to navigate with confidence. Would you like to launch BlindMate now?');\n        \n        // Store tutorial completion\n        localStorage.setItem('blindmate_tutorial_completed', 'true');\n        localStorage.setItem('blindmate_tutorial_date', new Date().toISOString());\n        \n        setTimeout(() => {\n            if (confirm('Launch BlindMate now?')) {\n                this.launchBlindMate();\n            }\n        }, 3000);\n    }\n    \n    launchBlindMate() {\n        this.speak('Launching BlindMate. Welcome to your navigation assistant!');\n        setTimeout(() => {\n            window.location.href = '/';\n        }, 2000);\n    }\n    \n    restartTutorial() {\n        if (confirm('Are you sure you want to restart the tutorial from the beginning?')) {\n            this.currentStep = 1;\n            this.currentExercise = 0;\n            this.practiceProgress = 0;\n            this.showStep(1);\n            this.speak('Tutorial restarted. Welcome back to the BlindMate tutorial.');\n        }\n    }\n    \n    exitTutorial() {\n        if (confirm('Are you sure you want to exit the tutorial?')) {\n            this.speak('Exiting tutorial. Goodbye!');\n            setTimeout(() => {\n                window.location.href = '/';\n            }, 1500);\n        }\n    }\n}\n\n// Demo functions for button interactions\nfunction speakExample(text) {\n    const tutorial = window.tutorialInstance;\n    tutorial.speak(`Example command: ${text}`);\n}\n\nfunction playDetectionDemo() {\n    const tutorial = window.tutorialInstance;\n    tutorial.speak('Enhanced detection demo with anti-overlap technology: Person ahead at 3 meters. Pausing for clarity. Car approaching from the right. Smart delay prevents voice overlap. Chair to your left with distance awareness.');\n}\n\nfunction playNavigationDemo() {\n    const tutorial = window.tutorialInstance;\n    tutorial.speak('Universal navigation demo: Should I start navigation to Times Square, New York? Route calculated using Google Maps. Distance: 2.5 kilometers. Battery-optimized GPS tracking enabled. Turn left in 50 meters onto Broadway. Smart rerouting available if you deviate. Automatic arrival detection when within 10 meters. You have arrived at Times Square.');\n}\n\nfunction playEmergencyDemo() {\n    const tutorial = window.tutorialInstance;\n    tutorial.speak('Emergency demo: Emergency mode activated. Broadcasting location. Sending alerts to emergency contacts. Loud audio beacon active. Emergency services have been notified of your location.');\n}\n\nfunction startPractice() {\n    const tutorial = window.tutorialInstance;\n    tutorial.startPractice();\n}\n\nfunction skipExercise() {\n    const tutorial = window.tutorialInstance;\n    tutorial.skipExercise();\n}\n\nfunction nextStep() {\n    const tutorial = window.tutorialInstance;\n    tutorial.nextStep();\n}\n\nfunction previousStep() {\n    const tutorial = window.tutorialInstance;\n    tutorial.previousStep();\n}\n\nfunction toggleAudio() {\n    const tutorial = window.tutorialInstance;\n    tutorial.toggleAudio();\n}\n\nfunction launchBlindMate() {\n    const tutorial = window.tutorialInstance;\n    tutorial.launchBlindMate();\n}\n\nfunction restartTutorial() {\n    const tutorial = window.tutorialInstance;\n    tutorial.restartTutorial();\n}\n\n// Initialize tutorial when page loads\ndocument.addEventListener('DOMContentLoaded', () => {\n    window.tutorialInstance = new OnboardingTutorial();\n});\n\n// Service worker registration for offline access\nif ('serviceWorker' in navigator) {\n    navigator.serviceWorker.register('/sw.js').catch(console.error);\n}","size_bytes":17231},"static/js/sw.js":{"content":"/**\n * Service Worker for BlindMate PWA\n * Provides offline capabilities and caching\n */\n\nconst CACHE_NAME = 'blindmate-v1';\nconst urlsToCache = [\n    '/',\n    '/navigation.js',\n    '/styles.css',\n    'https://cdn.replit.com/agent/bootstrap-agent-dark-theme.min.css',\n    'https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css',\n    'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.17.0/dist/tf.min.js',\n    'https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd@2.2.2/dist/coco-ssd.min.js'\n];\n\n// Install event - cache resources\nself.addEventListener('install', (event) => {\n    event.waitUntil(\n        caches.open(CACHE_NAME)\n            .then((cache) => {\n                console.log('Opened cache');\n                return cache.addAll(urlsToCache);\n            })\n    );\n});\n\n// Fetch event - serve cached content when offline\nself.addEventListener('fetch', (event) => {\n    event.respondWith(\n        caches.match(event.request)\n            .then((response) => {\n                // Return cached version or fetch from network\n                return response || fetch(event.request);\n            })\n    );\n});\n\n// Activate event - clean up old caches\nself.addEventListener('activate', (event) => {\n    event.waitUntil(\n        caches.keys().then((cacheNames) => {\n            return Promise.all(\n                cacheNames.map((cacheName) => {\n                    if (cacheName !== CACHE_NAME) {\n                        console.log('Deleting old cache:', cacheName);\n                        return caches.delete(cacheName);\n                    }\n                })\n            );\n        })\n    );\n});","size_bytes":1630}}}