I want you to build me a complete production-ready web app called BlindMate, targeted to help visually impaired users. Use modern tech stack and best practices. Here's exactly what I want:

🧩 Core Features:

👁️ Real-time Object Detection using Coco SSD (TensorFlow.js):

Use webcam feed from the user.

Load Coco SSD model in browser.

Detect objects like people, stairs, vehicles, chairs, walls, etc.

Draw bounding boxes and labels around detected objects using canvas.

Use Text-to-Speech (TTS) to alert the user with voice like:
"Obstacle ahead: chair, 1 meter away."

🧠 Gemini Voice Assistant Integration:

Use Gemini Pro model via Google Generative AI API.

Use Web Speech API to continuously listen for commands.

Commands like:

“Start detection”

“Take me to library”

“Enable navigation”

“Stop”

Gemini must respond with structured JSON:
{ "action": "start_detection" } or { "action": "navigate", "destination": "library" }

🗣️ Multilingual Voice Support:

Support English (en-IN), Hindi (hi-IN), and other common Indian languages.

Use Web Speech API to recognize voice in chosen language.

Let user select or say “Change language to Hindi”.

Gemini’s response must be translated and spoken aloud in the same language.

🎤 Voice-Based Permission Flow:

On load, assistant must ask:
“Should I start detection, Sir?”
→ If user says “Yes”, ask for webcam access and start detection.

Then ask:
“Would you like to enable location?”
→ If user says “Yes”, request geolocation and store coordinates.

🧭 Google Maps Navigation:

If Gemini detects action = “navigate”, use Google Maps API to:

Get directions from current location to the spoken place.

Speak turn-by-turn directions using TTS.

🎨 UI & Structure:

Clean light theme with large font and big buttons.

Responsive layout (mobile-first).

Single page app (HTML + JS + CSS).

Show live webcam preview + detection overlay.

Buttons for: Start/Stop Detection, Speak Command, Switch Language.

🧠 Tech Stack:

Frontend: Vanilla HTML/CSS/JS (no React needed)

AI: Coco SSD via TensorFlow.js

Voice: Web Speech API (TTS & Speech Recognition)

Backend: Flask or FastAPI (only to proxy Gemini API requests)

Gemini Pro API for all NLP and smart intent handling

🧪 Example Flow:

User opens site.

Assistant: “Should I start detection, Sir?”

User: “Yes”

Coco SSD starts detecting objects and speaking alerts.

Assistant: “Would you like to enable location?”

User: “Yes”

Location granted.

User: “Take me to the canteen”

Gemini returns: { action: "navigate", destination: "canteen" }

Google Maps opens route, and voice gives directions.

📦 Output Required:

Complete working code: index.html, app.js, styles.css

Python backend: app.py (Flask or FastAPI) for Gemini

TensorFlow.js Coco SSD setup

Gemini API integration sample

README with setup and how to run

Don’t use YOLO or any Python-based detection. Everything must run in browser using Coco SSD only.