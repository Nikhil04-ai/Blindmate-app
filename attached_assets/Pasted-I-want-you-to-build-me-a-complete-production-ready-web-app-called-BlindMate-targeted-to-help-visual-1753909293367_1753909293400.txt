I want you to build me a complete production-ready web app called BlindMate, targeted to help visually impaired users. Use modern tech stack and best practices. Here's exactly what I want:

ğŸ§© Core Features:

ğŸ‘ï¸ Real-time Object Detection using Coco SSD (TensorFlow.js):

Use webcam feed from the user.

Load Coco SSD model in browser.

Detect objects like people, stairs, vehicles, chairs, walls, etc.

Draw bounding boxes and labels around detected objects using canvas.

Use Text-to-Speech (TTS) to alert the user with voice like:
"Obstacle ahead: chair, 1 meter away."

ğŸ§  Gemini Voice Assistant Integration:

Use Gemini Pro model via Google Generative AI API.

Use Web Speech API to continuously listen for commands.

Commands like:

â€œStart detectionâ€

â€œTake me to libraryâ€

â€œEnable navigationâ€

â€œStopâ€

Gemini must respond with structured JSON:
{ "action": "start_detection" } or { "action": "navigate", "destination": "library" }

ğŸ—£ï¸ Multilingual Voice Support:

Support English (en-IN), Hindi (hi-IN), and other common Indian languages.

Use Web Speech API to recognize voice in chosen language.

Let user select or say â€œChange language to Hindiâ€.

Geminiâ€™s response must be translated and spoken aloud in the same language.

ğŸ¤ Voice-Based Permission Flow:

On load, assistant must ask:
â€œShould I start detection, Sir?â€
â†’ If user says â€œYesâ€, ask for webcam access and start detection.

Then ask:
â€œWould you like to enable location?â€
â†’ If user says â€œYesâ€, request geolocation and store coordinates.

ğŸ§­ Google Maps Navigation:

If Gemini detects action = â€œnavigateâ€, use Google Maps API to:

Get directions from current location to the spoken place.

Speak turn-by-turn directions using TTS.

ğŸ¨ UI & Structure:

Clean light theme with large font and big buttons.

Responsive layout (mobile-first).

Single page app (HTML + JS + CSS).

Show live webcam preview + detection overlay.

Buttons for: Start/Stop Detection, Speak Command, Switch Language.

ğŸ§  Tech Stack:

Frontend: Vanilla HTML/CSS/JS (no React needed)

AI: Coco SSD via TensorFlow.js

Voice: Web Speech API (TTS & Speech Recognition)

Backend: Flask or FastAPI (only to proxy Gemini API requests)

Gemini Pro API for all NLP and smart intent handling

ğŸ§ª Example Flow:

User opens site.

Assistant: â€œShould I start detection, Sir?â€

User: â€œYesâ€

Coco SSD starts detecting objects and speaking alerts.

Assistant: â€œWould you like to enable location?â€

User: â€œYesâ€

Location granted.

User: â€œTake me to the canteenâ€

Gemini returns: { action: "navigate", destination: "canteen" }

Google Maps opens route, and voice gives directions.

ğŸ“¦ Output Required:

Complete working code: index.html, app.js, styles.css

Python backend: app.py (Flask or FastAPI) for Gemini

TensorFlow.js Coco SSD setup

Gemini API integration sample

README with setup and how to run

Donâ€™t use YOLO or any Python-based detection. Everything must run in browser using Coco SSD only.